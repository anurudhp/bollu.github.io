<!DOCTYPE html><meta charset='UTF-8'><html><head><link rel='stylesheet' href='katex/katex.min.css'    integrity='sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X'    crossorigin='anonymous'><!-- The loading of KaTeX is deferred to speed up page rendering --><link rel='stylesheet' href='prism/prism.css'><title> A Universe of Sorts </title><style>@font-face {font-family: 'Blog Mono'; src: url('/static/iosevka-fixed-extended.ttf');}@font-face {font-family: 'Blog Text'; src: url('/static/Exo2-Regular.ttf');}html { font-size: 100%; }html,body { text-size-adjust: none; -webkit-text-size-adjust: none; -moz-text-size-adjust: none; -ms-text-size-adjust: none; } body { background-color: #FFFFFF; color: #000000;  font-family: 'Blog Text', sans-serif; font-size: 18px; line-height: 1.4em;  max-width: 100%; overflow-x: hidden; }
img { display:block; }.container { overflow-x: hidden; max-width:100%; }@media (max-width: 480px) { .container { margin-left: 5%; margin-right: 2%; } body { font-size: 40px; } }@media (max-width: 1024px) { .container { margin-left: 5%; margin-right: 2%; } body { font-size: 40px; } }@media (min-width: 1024px) { .container { margin-left: 30%; margin-right: 25%; } }.centered { margin-left: 30%; margin-right: 25%; }.image { }
a:hover { color: #1a73e8; text-decoration: underline;  }
a { color: #1a73e8; text-decoration: none; }
a:visited { color: #1a73e8; text-decoration: none; }
a:active { color: #1a73e8; text-decoration: none; }

blockquote { margin-left: 0px; margin-right: 0px; } pre, .latexblock, blockquote { border-left-color:#BBB;  border-left-style: solid;      border-left-width: 1px; }pre, blockquote { padding-left: 10px; }
pre { font-family: 'Blog Mono', monospace; font-size: 90%;  }pre {  overflow-x: auto; }.latexblock, blockquote, pre { margin-top: 10px; margin-bottom: 10px; padding-bottom: 5px; padding-top: 5px; background-color: #FFFFFF; }.latexblock { line-height: 1em }
.latexinline { white-space: nowrap }pre, kbd, samp, tt{ font-family:'Blog Mono',monospace; }ul, ol { list-style-position: inside; padding-left: 0; }</style></head><body><h2 class='centered'><a id=everything-you-know-about-word2vec-is-wrong href='#everything-you-know-about-word2vec-is-wrong'> ยง </a> Everything you know about word2vec is wrong</h2>
The classic explanation of <code>word2vec</code>, in skip-gram, with negative sampling,
in the paper and countless blog posts on the internet is as follows:
<pre><code><span class="token keyword">while</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
   <span class="token number">1.</span> vf <span class="token operator">=</span> vector of focus word
   <span class="token number">2.</span> vc <span class="token operator">=</span> vector of context word
   <span class="token number">3.</span> train such that <span class="token punctuation">(</span>vc <span class="token punctuation">.</span> vf <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
   <span class="token number">4.</span> <span class="token keyword">for</span><span class="token punctuation">(</span><span class="token number">0</span> <span class="token operator">&lt;=</span> i <span class="token operator">&lt;</span> negative samples<span class="token punctuation">)</span><span class="token punctuation">:</span>
           vneg <span class="token operator">=</span> vector of word <span class="token operator">*</span>not<span class="token operator">*</span> <span class="token keyword">in</span> context
           train such that <span class="token punctuation">(</span>vf <span class="token punctuation">.</span> vneg <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>
</code></pre>
Indeed, if I google "word2vec skipgram", the results I get are:
<ul><li> <a href=https://en.wikipedia.org/wiki/Word2vec#Training_algorithm>The wikipedia page which describes the algorithm on a high level</a></li><li> <a href=https://www.tensorflow.org/tutorials/representation/word2vec>The tensorflow page with the same explanation</a></li><li> <a href=https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b>The towards data science blog which describes the same algorithm</a></li></ul>
the list goes on. However, <b>every single one of these implementations is wrong</b>.
The original word2vec <code>C</code> implementation does <i>not</i> do what's explained above,
and is <i>drastically different</i>. Most serious users of word embeddings, who use
embeddings generated from <code>word2vec</code> do one of the following things:
<ol><li> They invoke the original C implementation directly.</li><li> They invoke the <code>gensim</code> implementation, which is <i>transliterated</i> from the
   C source to the extent that the variables names are the same.</li></ol>
Indeed, the <code>gensim</code> implementation is the 
<b>only one that I know of which is faithful to the C implementation</b>.
<h4 class='centered'><a id=the-c-implementation href='#the-c-implementation'> ยง </a> The C implementation</h4>
The C implementation in fact maintains <i>two vectors for each word</i>, one where
it appears as a focus word, and one where it appears as a context word.
(Is this sounding familiar? Indeed, it appears that GloVe actually took this
idea from <code>word2vec</code>, which has never mentioned this fact!)
The setup is incredibly well done in the C code:
<ul><li> An array called <code>syn0</code> holds the vector embedding of a word when it occurs
  as a <i>focus word</i>. This is <b>random initialized</b>.</li></ul>
<pre><code>https<span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span>tmikolov<span class="token operator">/</span>word2vec<span class="token operator">/</span>blob<span class="token operator">/</span><span class="token number">20</span>c129af10659f7c50e86e3be406df663beff438<span class="token operator">/</span>word2vec<span class="token punctuation">.</span>c#L369
  <span class="token keyword">for</span> <span class="token punctuation">(</span>a <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> a <span class="token operator">&lt;</span> vocab_size<span class="token punctuation">;</span> a<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token punctuation">(</span>b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> b <span class="token operator">&lt;</span> layer1_size<span class="token punctuation">;</span> b<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    next_random <span class="token operator">=</span> next_random <span class="token operator">*</span> <span class="token punctuation">(</span>unsigned long long<span class="token punctuation">)</span><span class="token number">25214903917</span> <span class="token operator">+</span> <span class="token number">11</span><span class="token punctuation">;</span>
    syn0<span class="token punctuation">[</span>a <span class="token operator">*</span> layer1_size <span class="token operator">+</span> b<span class="token punctuation">]</span> <span class="token operator">=</span>
       <span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">(</span>next_random <span class="token operator">&amp;</span> <span class="token number">0xFFFF</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>real<span class="token punctuation">)</span><span class="token number">65536</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">0.5</span><span class="token punctuation">)</span> <span class="token operator">/</span> layer1_size<span class="token punctuation">;</span>
  <span class="token punctuation">}</span>

</code></pre>
<ul><li> Another array called <code>syn1neg</code> holds the vector of a word when it occurs
  as a <i>context word</i>. This is <b>zero initialized</b>.</li></ul>
<pre><code>https<span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span>tmikolov<span class="token operator">/</span>word2vec<span class="token operator">/</span>blob<span class="token operator">/</span><span class="token number">20</span>c129af10659f7c50e86e3be406df663beff438<span class="token operator">/</span>word2vec<span class="token punctuation">.</span>c#L365
<span class="token keyword">for</span> <span class="token punctuation">(</span>a <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> a <span class="token operator">&lt;</span> vocab_size<span class="token punctuation">;</span> a<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token punctuation">(</span>b <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> b <span class="token operator">&lt;</span> layer1_size<span class="token punctuation">;</span> b<span class="token operator">++</span><span class="token punctuation">)</span>
  syn1neg<span class="token punctuation">[</span>a <span class="token operator">*</span> layer1_size <span class="token operator">+</span> b<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
</code></pre>
<ul><li> During training (skip-gram, negative sampling, though other cases are
  also similar), we first pick a focus word. This is held constant throughout
  the positive and negative sample training. The gradients of the focus vector
  are accumulated in a buffer, and are applied to the focus word
  <i>after it has been affected by both positive and negative samples</i>.</li></ul>
<pre><code><span class="token keyword">if</span> <span class="token punctuation">(</span>negative <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token punctuation">(</span>d <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> d <span class="token operator">&lt;</span> negative <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span> d<span class="token operator">++</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
  <span class="token comment">// if we are performing negative sampling, in the 1st iteration,</span>
  <span class="token comment">// pick a word from the context and set the dot product target to 1</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>d <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
    target <span class="token operator">=</span> word<span class="token punctuation">;</span>
    label <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
    <span class="token comment">// for all other iterations, pick a word randomly and set the dot</span>
    <span class="token comment">//product target to 0</span>
    next_random <span class="token operator">=</span> next_random <span class="token operator">*</span> <span class="token punctuation">(</span>unsigned long long<span class="token punctuation">)</span><span class="token number">25214903917</span> <span class="token operator">+</span> <span class="token number">11</span><span class="token punctuation">;</span>
    target <span class="token operator">=</span> table<span class="token punctuation">[</span><span class="token punctuation">(</span>next_random <span class="token operator">></span><span class="token operator">></span> <span class="token number">16</span><span class="token punctuation">)</span> <span class="token operator">%</span> table_size<span class="token punctuation">]</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>target <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span> target <span class="token operator">=</span> next_random <span class="token operator">%</span> <span class="token punctuation">(</span>vocab_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>target <span class="token operator">==</span> word<span class="token punctuation">)</span> <span class="token keyword">continue</span><span class="token punctuation">;</span>
    label <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span>
  l2 <span class="token operator">=</span> target <span class="token operator">*</span> layer1_size<span class="token punctuation">;</span>
  f <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span>

  <span class="token comment">// find dot product of original vector with negative sample vector</span>
  <span class="token comment">// store in f</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span>c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> layer1_size<span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span> f <span class="token operator">+</span><span class="token operator">=</span> syn0<span class="token punctuation">[</span>c <span class="token operator">+</span> l1<span class="token punctuation">]</span> <span class="token operator">*</span> syn1neg<span class="token punctuation">[</span>c <span class="token operator">+</span> l2<span class="token punctuation">]</span><span class="token punctuation">;</span>

  <span class="token comment">// set g = sigmoid(f) (roughly, the actual formula is slightly more complex)</span>
  <span class="token keyword">if</span> <span class="token punctuation">(</span>f <span class="token operator">></span> MAX_EXP<span class="token punctuation">)</span> g <span class="token operator">=</span> <span class="token punctuation">(</span>label <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> alpha<span class="token punctuation">;</span>
  <span class="token keyword">else</span> <span class="token keyword">if</span> <span class="token punctuation">(</span>f <span class="token operator">&lt;</span> <span class="token operator">-</span>MAX_EXP<span class="token punctuation">)</span> g <span class="token operator">=</span> <span class="token punctuation">(</span>label <span class="token operator">-</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">*</span> alpha<span class="token punctuation">;</span>
  <span class="token keyword">else</span> g <span class="token operator">=</span> <span class="token punctuation">(</span>label <span class="token operator">-</span> expTable<span class="token punctuation">[</span><span class="token punctuation">(</span>int<span class="token punctuation">)</span><span class="token punctuation">(</span><span class="token punctuation">(</span>f <span class="token operator">+</span> MAX_EXP<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>EXP_TABLE_SIZE <span class="token operator">/</span> MAX_EXP <span class="token operator">/</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> alpha<span class="token punctuation">;</span>

  <span class="token comment">// 1. update the vector syn1neg,</span>
  <span class="token comment">// 2. DO NOT UPDATE syn0</span>
  <span class="token comment">// 3. STORE THE syn0 gradient in a temporary buffer neu1e</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span>c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> layer1_size<span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span> neu1e<span class="token punctuation">[</span>c<span class="token punctuation">]</span> <span class="token operator">+</span><span class="token operator">=</span> g <span class="token operator">*</span> syn1neg<span class="token punctuation">[</span>c <span class="token operator">+</span> l2<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token keyword">for</span> <span class="token punctuation">(</span>c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> layer1_size<span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span> syn1neg<span class="token punctuation">[</span>c <span class="token operator">+</span> l2<span class="token punctuation">]</span> <span class="token operator">+</span><span class="token operator">=</span> g <span class="token operator">*</span> syn0<span class="token punctuation">[</span>c <span class="token operator">+</span> l1<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
<span class="token comment">// Finally, after all samples, update syn1 from neu1e</span>
https<span class="token punctuation">:</span><span class="token operator">/</span><span class="token operator">/</span>github<span class="token punctuation">.</span>com<span class="token operator">/</span>tmikolov<span class="token operator">/</span>word2vec<span class="token operator">/</span>blob<span class="token operator">/</span><span class="token number">20</span>c129af10659f7c50e86e3be406df663beff438<span class="token operator">/</span>word2vec<span class="token punctuation">.</span>c#L541
<span class="token comment">// Learn weights input -> hidden</span>
<span class="token keyword">for</span> <span class="token punctuation">(</span>c <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> c <span class="token operator">&lt;</span> layer1_size<span class="token punctuation">;</span> c<span class="token operator">++</span><span class="token punctuation">)</span> syn0<span class="token punctuation">[</span>c <span class="token operator">+</span> l1<span class="token punctuation">]</span> <span class="token operator">+</span><span class="token operator">=</span> neu1e<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">;</span>
</code></pre>
<h4 class='centered'><a id=why-random-and-zero-initialization href='#why-random-and-zero-initialization'> ยง </a> Why random and zero initialization?</h4>
Once again, since none of this actually explained in the original papers
<i>or on the web</i>, I can only hypothesize.
My hypothesis is that since the negative samples come from all over the text
and are not really weighed by frequency, you can wind up picking <i>any word</i>,
and more often than not, <i>a word whose vector has not been trained much at all</i>.
If this vector actually had a value, then it could move the actually important
focus word randomly.
The solution is to set all negative samples to zero, so that 
<i>only vectors that have occurred somewhat frequently</i> will affect the representation 
of another vector.
It's quite ingenious, really, and until this, I'd never really thought of
how important initialization strategies really are.
<h4 class='centered'><a id=why-im-writing-this href='#why-im-writing-this'> ยง </a> Why I'm writing this</h4>
I spent two months of my life trying to reproduce <code>word2vec</code>, following
the paper exactly, reading countless articles, and simply not succeeding.
I was unable to reach the same scores that <code>word2vec</code> did, and it was not
for lack of trying.
I could not have imagined that the paper would have literally fabricated an
algorithm that doesn't work, while the implementation does something completely
different.
Eventually, I decided to read the sources, and spent three whole days convinced
I was reading the code wrong since literally everything on the internet told me
otherwise.
I don't understand why the original paper and the internet contain zero
explanations of the <i>actual</i> mechanism behind <code>word2vec</code>, so I decided to put
it up myself.
This also explains GloVe's radical choice of having a separate vector
for the negative context --- they were just doing what <code>word2vec</code> does, but
they told people about it <code>:)</code>.
Is this academic dishonesty? I don't know the answer, and that's a heavy
question. But I'm frankly incredibly pissed, and this is probably the last
time I take a machine learning paper's explanation of the algorithm
seriously again --- from next time, I read the source <i>first</i>.
</body></html>
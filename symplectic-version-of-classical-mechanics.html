<!DOCTYPE html><meta charset='UTF-8'><html><head><link rel='stylesheet' href='katex/katex.min.css'    integrity='sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X'    crossorigin='anonymous'><!-- The loading of KaTeX is deferred to speed up page rendering --><script defer src='katex/katex.min.js'    integrity='sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4'    crossorigin='anonymous'></script><script>    function on_katex_load() {        const katex_opts = [            {left: '$', right: '$', display: false},            {left: '$$', right: '$$', display: true}        ];        renderMathInElement(document.body, katex_opts);        let elemsInline = document.getElementsByClassName('latexinline');        for (var i = 0; i < elemsInline.length; i++) {katex.render(elemsInline.item(i).textContent, elemsInline.item(i));}        let elemsBlock = document.getElementsByClassName('latexblock');        for (var i = 0; i < elemsInline.length; i++) {katex.render(elemsBlock.item(i).textContent, elemsBlock.item(i), {displayMode: true});}    }</script><!-- To automatically render math in text elements, include the auto-render extension: --><script defer src='katex/auto-render.min.js'    integrity='sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa'    crossorigin='anonymous'    onload='on_katex_load();'></script><title> A Universe of Sorts </title><style>@font-face {font-family: 'Blog Mono'; src: url('/static/iosevka-etoile-fixed.ttf');}@font-face {font-family: 'Blog Symbol'; src: url('/static/Symbola.ttf');}@font-face {font-family: 'Blog Text'; src: url('/static/Exo2-Regular.ttf');}html { font-size: 100%; }html,body { text-size-adjust: none; -webkit-text-size-adjust: none; -moz-text-size-adjust: none; -ms-text-size-adjust: none; } body { background-color: #FFFFFF; color: #000000;  font-family: 'Blog Text', sans-serif; font-size: 18px; line-height: 1.4em;  max-width: 100%; }
img { display:block; }.container { overflow-x: hidden }@media (max-width: 480px) { .container { margin-left: 5%; margin-right: 2%; } body { font-size: 40px; } }@media (max-width: 1024px) { .container { margin-left: 5%; margin-right: 2%; } body { font-size: 40px; } }@media (min-width: 1024px) { .container { margin-left: 30%; margin-right: 25%; } }.image { }
a:hover { color: #1a73e8; text-decoration: underline;  }
a { color: #1a73e8; text-decoration: none; }
a:visited { color: #1a73e8; text-decoration: none; }
a:active { color: #1a73e8; text-decoration: none; }

 .code, .latexblock, blockquote { border-left-color:#BBB;  border-left-style: solid;      border-left-width: 1px; }.code pre, blockquote { padding-left: 10px; }
 .code { font-family: 'Blog Mono', monospace; font-size: 90%;  }.latexblock, blockquote, .code, code { margin-top: 10px; margin-bottom: 10px; padding-bottom: 5px; padding-top: 5px; background-color: #FFFFFF; }.code, code { background-color: #FFFFFF; width: 100%; }.latexblock { line-height: 1em } .latexblock {  width: 100%; overflow-x: auto; white-space: nowrap; } .code pre { width: 100%; overflow-x: auto; margin: 0px; overflow-y: hidden; padding-top: 5px; padding-bottom: 5px; margin: 0px; }
.latexinline { white-space: nowrap }.code { white-space: nowrap }pre, code, kbd, samp, tt{ font-family:'Blog Mono',monospace; }ul, ol { list-style-position: inside; padding-left: 0; }</style></head><body><div class='container'><h2><a id=symplectic-version-of-classical-mechanics href='#symplectic-version-of-classical-mechanics'> § </a> <a href=#symplectic-version-of-classical-mechanics>Symplectic version of classical mechanics</a></h2>
<h4><a id=basics-symplectic-mechanics-as-inverting-omega href='#basics-symplectic-mechanics-as-inverting-omega'> § </a> Basics, symplectic mechanics as inverting <span class='latexinline'>\omega</span>:</h4>
I've never seen this kind of "inverting <span class='latexinline'>\omega</span>" perspective written down
anywhere.  Most of them start by using the inteior product <span class='latexinline'>i_X \omega</span> without
ever showing where the thing came from. This is my personal interpretation of
how the symplectic version of classical mecanics comes to be.
If we have a non-degenerate, closed 
two-form <span class='latexinline'>\omega: T_pM \times T_pM \rightarrow \mathbb R</span>.
Now, given a hamiltonian <span class='latexinline'>H: M \rightarrow \mathbb R</span>, we can construct a
vector field <span class='latexinline'>X_H: M \rightarrow TM</span> under the definition:
<div class='latexblock'>
\begin{align*}
&\text{partially apply $\omega$ to see $\omega$ as a mapping from $T_p$ to $T_p^*M$} \\
&\omega2: T_p M \rightarrow T_p*M \equiv \lambda (v: T_p M). \lambda (w: T_p M) . \omega(v, w) \\
&\omega2^{-1}: T_p^*M \rightarrow T_p M; dH: M \rightarrow T_p* M \\
&X_H \equiv \lambda (p: M) \rightarrow \omega2^{-1} (dH(p)) \\
&(p: M) \xrightarrow{dH} dH(p) : T_p*M \xrightarrow{\omega2^{-1}} \omega2^{-1}(dH(p)): T_pM \\
&X_H = \omega2^{-1} \circ dH
\end{align*}
</div>
This way, given a hamiltonian <span class='latexinline'>H: M \rightarrow \mathbb R</span>, we can construct
an associated vector field <span class='latexinline'>X_H</span>, in a pretty natural way.
We can also go the other way. Given the <span class='latexinline'>X</span>, we can build the <span class='latexinline'>dH</span>
under the equivalence:
<div class='latexblock'>
\begin{align*}
&\omega2^{-1} \circ dH  = X_H\\
&dH = \omega2(X_H) \\
&\int dH  = \int \omega2(X_H)  \\
&H = \int \omega2(X_H) 
\end{align*}
</div>
This needs some demands, like the one-form <span class='latexinline'>dH</span> being integrable. But this
works, and gives us a bijection between <span class='latexinline'>X_H</span> and <span class='latexinline'>H</span> as we wanted.
We can also analyze the definition we got from the previous manipulation:
<div class='latexblock'>
\begin{align*}
&\omega2(X_H) =  dH \\
&\lambda (w: T_p M) \omega(X_H, w) = dH \\
&\omega(X_H, \cdot) = dH \\
\end{align*}
</div>
We can take this as a <i>relationship</i> between <span class='latexinline'>X_H</span> and <span class='latexinline'>dH</span>. Exploiting
this, we can notice that <span class='latexinline'>dH(X_H) = 0</span>. That is, moving along <span class='latexinline'>X_H</span> does
not modify <span class='latexinline'>dH</span>:
<div class='latexblock'>
\begin{align*}
&\omega2(X_H) =  dH \\
&\lambda (w: T_p M) \omega(X_H, w) = dH \\
&dH(X_H) = \omega(X_H, X_H) = 0 ~ \text{$\omega$ is anti-symmetric}
\end{align*}
</div>
<h4><a id=preservation-of-omega href='#preservation-of-omega'> § </a> Preservation of <span class='latexinline'>\omega</span></h4>
We wish to show that <span class='latexinline'>X_H^*(\omega) = \omega</span>. That is, pushing forward
<span class='latexinline'>\omega</span> along the vector field <span class='latexinline'>X_H</span> preserves <span class='latexinline'>\omega</span>.
TODO.
<h4><a id=moment-map href='#moment-map'> § </a> Moment Map</h4>
Now that we have a method of going from a vector field <span class='latexinline'>X_H</span> to a Hamiltonian
<span class='latexinline'>H</span>, we can go crazier with this. We can <i>generate vector fields</i> using
Lie group actions on the manifold, and then look for hamiltonians corresponding
to this lie group. This lets us perform "inverse Noether", where for a given
choice of symmetry, we can find the Hamiltonian that possesses this symmetry.
We can create a map from the Lie algebra <span class='latexinline'>\mathfrak{g} \in \mathfrak{G}</span> to
a vector field <span class='latexinline'>X_{\mathfrak g}</span>, performing:
<div class='latexblock'>
\begin{align*}
&t : \mathbb R \mapsto e^{t\mathfrak g} : G \\
&t : \mathbb R \mapsto \phi(e^{t\mathfrak g}) : M \\
&X_{\mathfrak g} \equiv \frac{d}{dt}(\phi(e^{t\mathfrak g}))|_{t = 0}: TM
\end{align*}
</div>
We can then attempt to recover a hamiltonian <span class='latexinline'>H_{\mathfrak g}</span> from
<span class='latexinline'>X_{\mathfrak g}</span>. If we get a hamiltonian from this process, then it
will have the right symmetries.
<h3><a id=theorems-for-free href='#theorems-for-free'> § </a> <a href=#theorems-for-free>Theorems for free</a></h3>
These are personal notes I made of a custom notation for denoting the relations
from the theorems for free paper. I developed the notation since I wanted
to keep track of what types are floating around and what the relations are doing.
We interpret types as sets.  If elements belong to the relation, ie, if <code>(a, a') ∈ R ⊂ AxA</code>,
we will denote this as <code>a[A -R- A']a'</code>. We will now write down some inference
rules:
<ol><li> We define <code>ReflB</code> as <code>a[Bool ReflB Bool]a</code></li><li> We define <code>ReflI</code> as <code>i[Int ReflI Int]i</code></li><li> The product of two relations <code>[A R B]</code>, <code>[X S Y]</code> is called as <code>RxS</code>,
   and is defined as: <code>(a,x)[AxX RxS BxY](b,y)</code> iff: <code>∀ abxy, a[A R B]b ∧ x[X S Y]y</code>.</li><li> The list space of a <code>[A R B]</code> is called <code>[A* [A R B] B*]</code>, 
   and is defined as: <code>la[A* [A R B] B*]lb</code> iff:
   <code>∀ la lb, |la| = |lb| ∧ (∀ i, la[i][A R B]lb[i])</code></li><li> The function space of two relations<code>[A R B]</code>, <code>[X S Y]</code> is called <code>[A->X R->S B->Y]</code>,
   and is defined as: <code>f[A->X R->S B->Y]g</code> iff: <code>∀ a b, a[A R B]b => f(a)[X S Y]g(b)</code>.</li><li> The type family space of two relations is a function that takes
   a relation <code>[A R B]</code> and produces a new relation: 
   <code>g[FA | [A R B] | FB]h</code>. The relation takes as parameter a relation <code>[A R B]</code>
    for each choice.</li><li> The space of relations of <code>∀X.F(X)</code> is a relation defined by:
   <code>g[A->FA | ∀X.F(X) [FA [A R B] FB]| B->FB]h</code>
    <code>∀ A B R, (g A)[FA | [A R B] |  FB](h B)</code>.</li></ol>
<h4><a id=parametricity-theorem href='#parametricity-theorem'> § </a> Parametricity theorem</h4>
The parametricity thm states that for all terms <code>(r: R)</code>, we can deduce
<code>r[R rel(R) R]r</code> where <code>rel(R)</code> is the relation that fits the type, and is
derived from the above rules.
<h4><a id=parametricity-for-lists-when-the-relation-is-a-function href='#parametricity-for-lists-when-the-relation-is-a-function'> § </a> Parametricity for lists when the relation is a function:</h4>
The list space of a <code>[A R B]</code> is called <code>[A* [A R B] B*]</code>, 
and is defined as: <code>la[A* [A R B] B*]lb</code> iff:
<ul><li> <code>∀ la lb, |la| = |lb| ∧ (∀ i, la[i][A R B]lb[i])</code></li></ul>
Now, let us take a special case where <code>[A R B]</code> is a function <code>δ: A -> B</code>. That is:
<ul><li> <code>a[A R B]b</code> iff <code>δ(a) = b</code>.</li></ul>
If this is the case, then we can simplify the math to be:
<ul><li> <code>la[A* [A R B] B*]lb <=> ∀ la lb, |la| = |lb| ∧ (∀ i, la[i][A R B]lb[i])</code></li><li> <code>la[A* [A R B] B*]lb <=> ∀ la lb, |la| = |lb| ∧ (∀ i, δ(la[i]) = lb[i]</code></li><li> <code>la[A* [A R B] B*]lb <=> ∀ la lb, map δ la = lb</code></li></ul>
<h4><a id=parametricity-to-prove-rearrangements href='#parametricity-to-prove-rearrangements'> § </a> Parametricity to prove rearrangements</h4>
<ul><li> <code>r[∀ X. X* -> X*]r</code></li><li> <code>(r A)[A*->A*  | [A*->A* [A R B] B*->B*] |  B*->B*](r B)</code></li><li> <code>as[A* [A R B]  B*]bs => (r A as)[A* [A R B] B*](r B bs)</code></li><li> Pick <code>[A R B]</code> to be a <i>function</i> <code>δ: A -> B</code>. Ie, <code>a[A R B]b</code> iff <code>δ(a) = b</code>.</li><li> This lets us convert all occrences of <code>α[A R B]ω</code> into <code>ω = δ(α)</code>.</li><li> Hence, <code>as[A* [A R B]  B*]bs</code> becomes <code>map δ as = bs</code>.</li><li> Hence, <code>(r A as)[A* [A R B] B*](r B bs)</code> becomes <code>map δ (r A as) = (r B bs)</code></li><li> In toto, this let us replace <code>bs</code> with <code>map δ as</code>. We derive:</li><li> <code>map δ (r A as) = (r B bs)</code></li><li> <code>map δ (r A as) = (r B (map δ as)</code></li><li> <code>map δ . (r A) = (r B) . map δ</code></li><li> Replace <code>bs[i]</code> with <code>δ(as[i])</code>to get result:
   <code>δ(r A as[i]) = r B δ(as[i])</code>, which was indeed what we were looking for.</li></ul>
<h4><a id=references href='#references'> § </a> References</h4>
<ul><li> <a href=https://ecee.colorado.edu/ecen5533/fall11/reading/free.pdf>Theorems for free by Phil Wadler</a></li></ul>
<h3><a id=how-to-reason-with-half-open-intervals href='#how-to-reason-with-half-open-intervals'> § </a> <a href=#how-to-reason-with-half-open-intervals>How to reason with half-open intervals</a></h3>
I've always found code that uses half-open intervals far harder to write
than using closed intervals. For example, when performing string processing,
I prefer to write <code>closed</code> over <code>halfopen</code> since I find it easier
to think about:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
However, I realised that by changing how I think about this to:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
It somehow made it way easier to grok.
<ul><li> I had problems with <code><</code> since I would
  mentally shift from <code>i < begin + len</code> to <code>i <= begin + len - 1</code>. Making
  this move would then make <i>all other reasoning</i> harder, since I had
  keep switching between the <code><</code> and <code><=</code> point of view.</li></ul>
<ul><li> On the other hand, when using <code>i != begin + len</code>, there was a single location
  to focus on: the point <code>begin + len</code>, and what happens when <code>i</code> reaches it.</li></ul>
Of course, this is old had to anyone who performs loop optimisatison: LLVM
internally converts most comparisons into the <code>a != b</code> form, because it's
easier to analyse. It took me this long it's easier for me to <i>think</i>
in this viewpoint as well.
<h3><a id=how-does-one-build-a-fusion-bomb href='#how-does-one-build-a-fusion-bomb'> § </a> <a href=#how-does-one-build-a-fusion-bomb>How does one build a fusion bomb?</a></h3>
I haven't found anything on the internet that describes how to build
a fusion bomb; it's almost as if this information has been supressed
by governments. However, I'm curious --- would a physics grad student
studying nuclear physics or explosives have the theoretical know-how to
precisely build one, given the raw materials? Or is there some
"secret sauce" that's necessary?
I read on wikipedia that most countries classify the details:
<blockquote> Detailed knowledge of fission and fusion weapons is classified to some degree in virtually every industrialized nation. In the United States, such knowledge can by default be classified as "Restricted Data", even if it is created by persons who are not government employees or associated with weapons programs, in a legal doctrine known as "born secret".</blockquote>
<h3><a id=christoffel-symbols-geometrically href='#christoffel-symbols-geometrically'> § </a> <a href=#christoffel-symbols-geometrically>Christoffel symbols, geometrically</a></h3>
Suppose we have a manifold <span class='latexinline'>M</span>. of dimension <span class='latexinline'>d</span> that has been embedded isometrically
into <span class='latexinline'>\mathbb R^n</span>. So we have a function <span class='latexinline'>e: \mathbb R^d \rightarrow \mathbb R^n</span>
which is the embedding. We will identify <span class='latexinline'>M</span> to be the subspace <span class='latexinline'>Im(e)</span>.
Recall that <span class='latexinline'>\partial_{x_i} e : \mathbb R^d \rightarrow \mathbb R^n</span>
is defined as:
<div class='latexblock'>
\begin{align*}
&\partial_{x_i}e : \mathbb R^d \rightarrow \mathbb R^n \\
&[\partial {x_i}e](p) \equiv
 \lim_{\delta x \rightarrow 0} \frac{e(p + (0:0, 1:0\dots, i:\delta_x, \dots, n:0)) - e(p)}{\delta x}
\end{align*}
</div>
Note that it is a function of type <span class='latexinline'>\mathbb R^d \rightarrow \mathbb R^n</span>.
<ul><li> The tangent space at point <span class='latexinline'>p \in Image(e)</span> is going to be spanned by
  the basis <span class='latexinline'>\{ \partial_{x_i}e \vert_p : \mathbb R^n \}</span>.</li><li> The metric tensor of <span class='latexinline'>M</span>,
 <span class='latexinline'>g_{ij} \equiv \langle \frac{\partial e}{\partial x_i} \vert \frac{\partial e}{\partial x_j} \rangle</span>.
 That is, the metric tensor "agrees" with the dot product of the
 ambient space <span class='latexinline'>\mathbb R^n</span>.</li><li> A vector field <span class='latexinline'>V</span> on the manifold <span class='latexinline'>M</span> is by definition a combination of
  the tangent vector fields. <span class='latexinline'>V(p_0) \equiv v^j(p_0) \partial_{x_j} e(p_0)</span></li></ul>
We can calculate the derivaive of this vector field as follows:
<div class='latexblock'>
\begin{align*}
&\frac{V(p)}{\partial x^i} \\
&= \partial_{x_i} \left[ v_j(p) \partial_{x_j} e \right]
&= v^j \cdot \partial_{x_i} \partial_{x_j} e + \partial_{x_j}e \cdot \partial_{x_i} v^j
\end{align*}
</div>
We choose to rewrite the second degree term in terms of the tangent
space, and some component that is normal to us that we have no
control over.
<div class='latexblock'>
(\partial_{x_i} \partial_{x_j} e )(p) \equiv \Gamma^k_{ij} \partial_{x_k} e + \vec{n}
</div>
This gives us the Christoffel symbols as "variation of second derivative <i>along</i>
the manifold.
<h4><a id=relationship-to-the-levi-cevita-connection href='#relationship-to-the-levi-cevita-connection'> § </a> Relationship to the Levi-Cevita connection</h4>
The covariant derivative defined by the Levi-Cevita connection is the derivative
that contains the projection of the full derivative in <span class='latexinline'>\mathbb R^n</span> onto
the tangent space <span class='latexinline'>T_p M</span>. This is defined by the equations:
<div class='latexblock'>
\begin{align*}
 &\nabla_{e_i} V \equiv \partial_{x_i} V - \vec{n}  \\
 &= \Pi_{\vec{n}^\bot} \left [v^j \cdot \partial_{x_i} \partial_{x_j} e + \partial_{x_j}e \cdot \partial_{x_i} v^j \right] \\
 &= \Pi_{\vec{n}^\bot} \left[ v^j \cdot (\Gamma^k_{ij} \partial_{x_k} e + \vec{n})+ \partial_{x_j}e \cdot \partial_{x_i} v^j \right] \\
 &= v^j \cdot (\Gamma^k_{ij} \partial_{x_k} e + \vec 0) + \partial_{x_j}e \cdot \partial_{x_i} v^j \\
 &= v^j \cdot (\Gamma^k_{ij} \partial_{x_k} e + \vec 0) + \partial_{x_k}e \cdot \partial_{x_i} v^k \\
 &= v^j \cdot \Gamma^k_{ij} \partial_{x_k} e  + \partial_{x_k}e \cdot \partial_{x_i} v^k \\
 &= \partial_{x_k} e \left( v^j \cdot \Gamma^k_{ij}  + \partial_{x_i} v^k \right) \\
\end{align*}
</div>
<h4><a id=references href='#references'> § </a> References</h4>
<ul><li> <a href=https://en.wikipedia.org/wiki/Covariant_derivative#Informal_definition_using_an_embedding_into_Euclidean_space>The wikipedia page on Covariant derivative</a></li></ul>
<h3><a id=a-natural-vector-space-without-an-explicit-basis href='#a-natural-vector-space-without-an-explicit-basis'> § </a> <a href=#a-natural-vector-space-without-an-explicit-basis>A natural vector space without an explicit basis</a></h3>
On learning about infinite dimensional vector spaces, one learns that
we need to use the axiom of choice to assert that every such vector space
has a basis; indeed, it's equivalent to the AoC to assert this. However,
I had not known any "natural" examples of such a vector space till I studied
the proof of the barvinok algorithm. I produce the example here.
Consider a space such as <span class='latexinline'>S \equiv \mathbb R^3</span>. Now, consider the vector
space spanned by the indicator functions of polyhedra in <span class='latexinline'>S</span>. that's a mouthful,
so let's break it down.
A polyhedra is defined as a set of points that is defined by linear
inequalities: <span class='latexinline'>P \equiv \{ x \in S : a_i \cdot x \leq b_i, i \in [1\dots n] \}</span>,
for all <span class='latexinline'>a_i \in S</span>, <span class='latexinline'>b \in \mathbb R</span>.
The indicator functions are of the form:
<div class='latexblock'>
[poly]: S \rightarrow \mathbb R;
[poly](x) \equiv
\begin{cases} 1 & x \in poly \\
0 & \text{otherwise} \end{cases}
</div>
we can define a vector space of these functions over <span class='latexinline'>\mathbb R</span>, using
the "scaling" action as the action of <span class='latexinline'>\mathbb R</span> on these functions:
The vector space <span class='latexinline'>V</span> is <b>defined</b> as the span of the indicator functions
of all polyhedra. It's clearly a vector space, and a hopefully intuitive
one. However, note that the set we generated this from (indicators of polyhedra)
don't form a basis since they have many linear dependencies between them.
For example, one can write the equation:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h3><a id=cache-oblivious-b-trees href='#cache-oblivious-b-trees'> § </a> <a href=#cache-oblivious-b-trees>Cache oblivious B trees</a></h3>
Central idea: assume a memory model where computation is free, only cost
is pulling data from cache into memory. Cache has total size <span class='latexinline'>M</span>, can hold
blocks of size <span class='latexinline'>B</span>. So it can hold <span class='latexinline'>M/B</span> blocks of main memory. Memory memory
has infinite size. Cost is number of transfers.
We assume that the algorithm <i>does not know M or B</i>. We assume that the cache
replacement strategy is optimal (kick out block that is going to be used
farthest in the future). This is an OK assumption to make since an LRU cache
using <i>twice</i> the memory of a "oracular" cache performs equally well (citation?)
These data structures are cool since they essentially "Adapt" to varying cache
hierarchies and even multiple level cache hierarchies.
We study how to build cache-oblivious B-trees.
<h4><a id=building-optimal-cache-oblivious-b-trees-to-solve-search href='#building-optimal-cache-oblivious-b-trees-to-solve-search'> § </a> Building optimal cache-oblivious B trees to solve search</h4>
<ul><li> We use a balanced BST. We want to find an order to store nodes in memory
  such that when we search for an element, we minimize number of blocks
  we need to pull in.</li><li> All standard orders such as level order, pre-order, post-order fail.</li><li> Corrrect order is "VEB (Van Em De Boas) order": carve a tree at the middle
  level of its edges. Layout a "triangle" or smaller collection
  of nodes linearly. Then Recursively layout the trees, linearly in memory.</li><li> Supposedly if the number of nodes is <span class='latexinline'>N</span>, we wil have roughly <span class='latexinline'>\sqrt(N)</span>
  nodes on the top, and then <span class='latexinline'>\sqrt(N)</span> <i>triangles</i> at the bottom.</li></ul>
<h4><a id=analysis-claim-we-need-to-pull-ologb-n-blocks-for-any-b-for-any-search-query href='#analysis-claim-we-need-to-pull-ologb-n-blocks-for-any-b-for-any-search-query'> § </a> Analysis Claim: we need to pull <span class='latexinline'>O(\log_B N)</span> blocks for any <span class='latexinline'>B</span> for any search query</h4>
<span class='latexinline'>N</span> is the number of nodes in the BST. Note that in the analysis, <i>we know what B is</i>,
even though the <i>algorithm does not</i>.
<ul><li> We look at a particuar level of recursion. We will call it a "level of detail"
  straddling B.</li><li> We will have large triangles of size <span class='latexinline'>\geq B</span>, inside which there are smaller
  triangles of size <span class='latexinline'>\leq B</span> (reminds me of sierpinski).</li><li> We know that the algorithm recursively lays it out, and triangle stores
  everything "inside" it <i>in a contiguous region</i>. So we stop at the
  requisite size where we know that the tree's triangles themselves
  contain triangles which fit into the block size.</li><li> A little triangle of size less than B can live in at most two memory blocks
  by straddling a block boundary: by eg. having <span class='latexinline'>(B-1)</span> bits in one block
  and a single bit in another block.</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> The question is that on a root-to-leaf bpath, how many such triangles do
  we need to visit. Since we repeatedly divide the nodes in half 
  <i>with respect to height</i> until the little triangle has number of nodes less
  than <span class='latexinline'>B</span>, the height is going to be <span class='latexinline'>O(\log B)</span> since it's still a binary tree.</li><li> total height in <span class='latexinline'>O(\log N)</span>.</li><li> so height of "chunked tree" where we view each triangle as a single node
  is <span class='latexinline'>\log N / \log B = \log_B n</span>.</li><li> <b>insight</b>: ou data structure construction in some sense permits us to
  "binary search on <span class='latexinline'>B</span>" since we divide the data structure into levels
  based on <span class='latexinline'>B</span>. if <span class='latexinline'>B = N</span>, then the full data structure fits into memory
  and we're good.</li></ul>
<h4><a id=black-box-ordered-file-maintainince href='#black-box-ordered-file-maintainince'> § </a> Black box: ordered file maintainince</h4>
We need a black box: ordered file maintainance (linked list for arrays)
<ul><li> Store <span class='latexinline'>n</span> elements in specified order in an array of linear size <span class='latexinline'>O(N)</span>.
  Array permits gaps.</li><li> updates: delete element, insert elements between 2 elements.</li><li> cannot do this in linear time, but we can move elements in an interval of
  size <span class='latexinline'>\log^2(N)</span> amortized.</li><li> We need <span class='latexinline'>O(1)</span> scans for the data structure.</li></ul>
<h4><a id=next-d-bst-inserts-and-deletes-layout href='#next-d-bst-inserts-and-deletes-layout'> § </a> Next: <i>dynamic</i> BST (inserts and deletes): layout</h4>
we take a VEB static tree on top of an ordered file. Tree is a segtree
that has max of nodes. Leaves are the members of the ordered file.
<h4><a id=updates href='#updates'> § </a> Updates</h4>
<ul><li> search for node.</li><li> update ordered file.</li><li> propogate updates into the tree. This will have to be done in post-order
  because we need the leaves to be fixed before we can update the parent
  <code>max</code>.</li></ul>
<h4><a id=updates-analysis href='#updates-analysis'> § </a> Updates: analysis.</h4>
<ul><li> look at level of detail that straddles <span class='latexinline'>B</span>.</li><li> Let us look at the bottom 2 levels.</li><li> Note that when we perform post-order inside a triangle that has 3 triangles
  of size <span class='latexinline'>\leq B</span>, we need to alternate between parent triangle and child triangle.
  Since the parent triangle is of size <span class='latexinline'>\leq B</span> and can therefore take
  at most <span class='latexinline'>2B</span> blocks of memory, similarly the child can take at most <span class='latexinline'>2B</span>
  blocks of memory.</li><li> So if our cache can hold <span class='latexinline'>4</span> blocks of memory, we're done.
  We won't need to kick anything out when performing the post-order
  traversal.</li><li> For levels that are above the bottom 2 levels, we're still OK. there
  are not many triangles! / not many nodes! (<code>1:16:00</code> in the video)</li></ul>
<h4><a id=references href='#references'> § </a> References</h4>
<ul><li> <a href=https://courses.csail.mit.edu/6.851/fall17/lectures/L07.html?notes=5>Erik demaine, advanced data structures, lecture 7: Memory hiearchy: models, cache oblivious B trees</a></li></ul>
<h3><a id=krohn-rhodes-decomposition href='#krohn-rhodes-decomposition'> § </a> <a href=#krohn-rhodes-decomposition>Krohn-Rhodes decomposition</a></h3>
We denote partial functions with <span class='latexinline'>X \rightharpoonup Y</span> and total functions
with <span class='latexinline'>X \rightarrow Y</span>.
A set <span class='latexinline'>X</span> equipped with a binary operator
<span class='latexinline'>\star: X \times X \rightarrow X</span> which is closed and associative
is a semigroup.
<h4><a id=partial-function-semigroup href='#partial-function-semigroup'> § </a> Partial function semigroup</h4>
For a ground set <span class='latexinline'>X</span>, the set of partial functions <span class='latexinline'>Pf(X) \equiv \{ f: X \rightharpoonup X \}</span>
along with function composition forms a semigroup. This is in fact stronger
than a semigroup. There exists:
<ul><li> An identify function <span class='latexinline'>e_x: X \rightarrow X; e_X(x) = x</span></li><li> A zero function <span class='latexinline'>\theta_x: X \rightharpoonup X; \theta_x(x) = undef</span>, where by
  <span class='latexinline'>undef</span> we mean that it is <i>undefined</i>.</li></ul>
<h4><a id=transformation-semigroupts href='#transformation-semigroupts'> § </a> Transformation semigroup(TS)</h4>
Let <span class='latexinline'>Q</span> be a set. Let <span class='latexinline'>S \subseteq Pf(Q)</span> be a sub-semigroup of <span class='latexinline'>Pf(Q)</span>.
Then the semigroup <span class='latexinline'>X \equiv (Q, S)</span> is called as the
 <i>transformation semigroup</i>(X) of states <span class='latexinline'>Q</span>.
<ul><li> The elements of <span class='latexinline'>Q</span> are called states of <span class='latexinline'>X</span></li><li> while  the elements of <span class='latexinline'>S</span> are called  <i>actions</i> of <span class='latexinline'>X</span>.</li><li> The set <span class='latexinline'>Q</span> itself is called as the <i>underlying set</i> of <span class='latexinline'>X</span>.</li><li> For a fixed transformation semigroup <span class='latexinline'>X</span>, we will write <span class='latexinline'>Q_X</span> and <span class='latexinline'>S_X</span>
  to refer to its states and actions.</li></ul>
We call <span class='latexinline'>X \equiv (Q, S)</span> as a <i>transformation monoid</i> if <span class='latexinline'>S</span> contains <span class='latexinline'>1_Q(q) = q</span>.
<h4><a id=subtlety-of-being-at-transformation-monoid href='#subtlety-of-being-at-transformation-monoid'> § </a> Subtlety of being at transformation monoid.</h4>
There is some subttlety here. Just because <span class='latexinline'>S</span> is a monoid does not mean that
it that is a <i>transformation monoid</i>. It must have the identity element of
<span class='latexinline'>Pf(Q)</span> to be called a transformation monoid. For example, consider the
set <span class='latexinline'>Q \equiv \\{ a, b \\}</span> and the transformation semigroup <span class='latexinline'>S \equiv \\{ f \equiv \alpha \mapsto b \\}</span>.
Now the set <span class='latexinline'>S</span> is indeed a monoid with identity element as <span class='latexinline'>f: Q \rightarrow Q</span>.
however, <span class='latexinline'>f \neq 1_Q</span> , andh ence, <span class='latexinline'>S</span> is a not a <i>transformation monoid</i>.
<h4><a id=examples-of-transformation-semigroups href='#examples-of-transformation-semigroups'> § </a> Examples of transformation semigroups</h4>
<ol><li> <span class='latexinline'>(X, \{ \theta(x) = undef \})</span>. The semigroup with the empty transformation.</li><li> <span class='latexinline'>(X, \emptyset)</span>, the semigroup with <i>no</i> transformations.</li></ol>
<h4><a id=semigroup-action href='#semigroup-action'> § </a> Semigroup action</h4>
We sometimes wish to represent a semigroup using an action/transformation semigroup
on a ground set <span class='latexinline'>X</span>. So, given some semigroup <span class='latexinline'>(T, \times)</span> that needs to be represented,
if we can find a morphism <span class='latexinline'>r: T \rightarrow Pf(X)</span> (<span class='latexinline'>r</span> for representation)
such that:
<ul><li> <span class='latexinline'> r(t_1 + t_2) = r(t_1) \circ r(t_2)</span>. [<span class='latexinline'>r</span> is a semigroup morphism].</li><li> <span class='latexinline'>t_1 \neq t_2 \implies \exists x \in X</span> such that <span class='latexinline'>r(t_1)(x) \neq r(t_2)(x)</span>.
  [Faithfulness].</li></ul>
Put more simply, <span class='latexinline'>t_1 \neq t_2 \implies r(t_1) \neq r(t_2)</span> where we define
function equality extensionally: <span class='latexinline'>f = g \equiv \forall x, f(x) = g(x)</span>.
<h4><a id=embedding-actions-into-the-transformation-semigroup href='#embedding-actions-into-the-transformation-semigroup'> § </a> Embedding actions into the transformation semigroup</h4>
We often wish to represent some semigroup <span class='latexinline'>S</span> as the transformation semigroup
of some set of states <span class='latexinline'>Q</span>. We can achieve this by proving a morphism:
<ul><li> <span class='latexinline'>r: S \rightarrow Pf(Q)</span> that is faithful.</li></ul>
Then, we can treat elements of <span class='latexinline'>S</span> as elements of <span class='latexinline'>Pf(Q)</span>.
<h4><a id=completion-of-a-transformation-semigroup href='#completion-of-a-transformation-semigroup'> § </a> Completion of a transformation semigroup</h4>
Given a transformation semigroup <span class='latexinline'>X \equiv (Q, S)</span> we can <i>complete</i> it
by adding a new sink state <span class='latexinline'>\bot</span>, and then converting all partial
functions in <span class='latexinline'>S</span> to total functions that transition to <span class='latexinline'>\bot</span>. We have that
<span class='latexinline'>\bot \cdot s = s \cdot \bot ~ \forall s \in S</span>.
We denote the completion as <span class='latexinline'>X^c \equiv (Q^c, S^c)</span>.
<h4><a id=coverings href='#coverings'> § </a> Coverings</h4>
Let <span class='latexinline'>X \equiv (Q_X, S_X)</span> and <span class='latexinline'>Y \equiv (Q_Y, S_Y)</span> be transformation
semigroups. Let <span class='latexinline'>\phi \subseteq Q_Y \times Q_X</span> be a relation. Let <span class='latexinline'>s_x \in S_X</span>
and <span class='latexinline'>s_y \in S_Y</span>. Then, if the following diagram commutes:
<div class='latexblock'>
\begin{array}{ccc}
a & \rightarrow & b \\
\downarrow & & \downarrow \\
x & \rightarrow & y \\
\end{array}
</div>
If <span class='latexinline'>s_x(\phi(q_y)) = \phi(t_y(q_y))</span>, then we say that <span class='latexinline'>t_y</span> covers <span class='latexinline'>s_x</span> relative to <span class='latexinline'>\phi</span>.
We imagine the <span class='latexinline'>t_y</span> lying above <span class='latexinline'>s_x</span>, being projected down by <span class='latexinline'>\phi</span>.
If a fixed <span class='latexinline'>\phi</span>, for all <span class='latexinline'>s_x \in S_X</span> there exists a <span class='latexinline'>t_y \in S_Y</span> such that
<span class='latexinline'>t</span> covers <span class='latexinline'>s</span> relative to <span class='latexinline'>\phi</span>, then we say that <span class='latexinline'>\phi:</span> is a 
<i>relation of automata</i>.
<ul><li> If <span class='latexinline'>\phi: Q_Y \rightarrow Q_X</span> is surjective,
  then we say that <span class='latexinline'>\phi</span> is a <b>relational covering</b> and write:</li></ul>
<div class='latexblock'>
X \triangleleft_{\phi} Y
</div>
<ul><li> If <span class='latexinline'>\phi \subeteq Q_Y \times Q_X </span> is <i>both</i> surjective and partial,
  then we say that <span class='latexinline'>\phi</span> is a <b>covering</b> and write:</li></ul>
<div class='latexblock'>
X \prec_\phi Y
</div>
If <span class='latexinline'>X \prec_\phi Y</span>, we say that <span class='latexinline'>Y</span> dominates <span class='latexinline'>X</span>, or <span class='latexinline'>Y</span> covers <span class='latexinline'>X</span>, or
<span class='latexinline'>X</span> divides <span class='latexinline'>Y</span>.
<h4><a id=checking-coverings-and-generating-subsets href='#checking-coverings-and-generating-subsets'> § </a> Checking coverings and generating subsets</h4>
We note that for a given covering <span class='latexinline'>\phi</span>, if <span class='latexinline'>s_x</span> is covered by <span class='latexinline'>t_y</span>
and <span class='latexinline'>p_x</span> is covered by <span class='latexinline'>q_y</span>, then <span class='latexinline'>s_x \circ t_x</span> is covered by <span class='latexinline'>t_y \circ q_y</span>.
Thus, to check if <span class='latexinline'>X</span> is covered by <span class='latexinline'>Y</span>, we simply need to check if 
<b>some generating subset of <span class='latexinline'>X</span> is covered by <span class='latexinline'>Y</span></b>.
<h4><a id=checking-coverings-of-representations href='#checking-coverings-of-representations'> § </a> Checking coverings of representations</h4>
Let us assume we have a representation of a transformation semigroup
given with a semigroup <span class='latexinline'>\Sigma</span>, a transformation semigroup
<span class='latexinline'>X \equiv (Q_X, S_X)</span>, and a representation <span class='latexinline'>r: \Sigma \rightarrow S_X</span> that is
faithful.
Now, to check that <span class='latexinline'>X</span> is covered by another <span class='latexinline'>Y</span>, it suffices to check that
there exists a <span class='latexinline'>t_y \in Y</span> for each <span class='latexinline'>\sigma \in X</span> such that <span class='latexinline'>r(\sigma)</span> is
covered by this <span class='latexinline'>t_y</span>.
<h4><a id=companion-relation href='#companion-relation'> § </a> Companion relation</h4>
Given a relation <span class='latexinline'>\phi: Y \rightarrow X</span>, then we define:
<div class='latexblock'>
\Sigma \equiv \{ (s, t) : t \in T_Y \text{ covers } s \in S_X \}
</div>
Recall compositions of elements are covered by a composition
of their coverings. Hence, if <span class='latexinline'>(s, t), (s', t') \in \Sigma</span>, then
<span class='latexinline'>(ss', tt') \in \Sigma</span>. thus, <span class='latexinline'>\Sigma</span> is a subsemigroup of <span class='latexinline'>S_X \times S_Y</span>.
We can regard <span class='latexinline'>\Sigma</span> as the graph of a relation <span class='latexinline'>\phi' \subseteq Q_Y \times Q_X</span>.
This will be called as <b>companion relation</b> of <span class='latexinline'>\phi</span>.
<h4><a id=wreath-products href='#wreath-products'> § </a> Wreath products</h4>
Let <span class='latexinline'>X \equiv (Q_X, S_X)</span> and <span class='latexinline'>Y \equiv (Q_Y, S_Y)</span>. We're going to define a large
product <span class='latexinline'>X \wr Y</span>.
We begn with the set <span class='latexinline'>W \equiv S_X^Q_Y \times S_Y</span>, where
<span class='latexinline'>S_X^Q_Y \equiv \{ f : Q_Y \rightarrow S_X \}</span>.
The wreath product then becomes:
<div class='latexblock'>
X \wr Y \equiv (Q_X \times Q_Y, W)
</div>
with the action of <span class='latexinline'>W</span> on an element of <span class='latexinline'>Q_X \times Q_Y</span> being defined as:
<div class='latexblock'>
(f : Q_Y \rightarrow S_X, s_y : S_Y) (q_x : Q_X, q_Y : Q_Y) \equiv ( f(q_y)(q_x) , s_y (q_y))
</div>
it's a "follow the types" sort of definition, where we edit the right component
as <span class='latexinline'>r_y \mapsto t_y(r_y)</span> since that's all we can do. In the case of
the left component, we have a <span class='latexinline'>q_x</span>, and we need to produce another element
in <span class='latexinline'>Q_X</span>, so we "must use <span class='latexinline'>f</span>". The only way to use <span class='latexinline'>f</span> is to feed it
a <span class='latexinline'>t_y</span>. This forces us into the above definition.
<h4><a id=composition-of-wreath-products href='#composition-of-wreath-products'> § </a> Composition of wreath products</h4>
To show that its closed under composition, let's consider <span class='latexinline'>(f, s_y), (g, t_y) \in W</span>
with <span class='latexinline'>f, g: Q_Yg \rightarrow S_X</span>, and <span class='latexinline'>s_y, t_y \in S_Y</span>. The result is
going to be:
<div class='latexblock'>
(f, s_y)  (g, t_y) =  (\lambda q_y. f(q_y) \circ g(q_y), t_y \circ u_y)
</div>
<h4><a id=equivalences-of-subsets-of-states href='#equivalences-of-subsets-of-states'> § </a> Equivalences of subsets of states</h4>
Let <span class='latexinline'>X = (Q, S)</span> be a transition system. Given subsets <span class='latexinline'>(a, b, \subseteq Q)</span>,
we shall write <span class='latexinline'>b \leq a</span> if either <span class='latexinline'>b \subseteq a</span> or there exists some <span class='latexinline'>s \in S</span>
such that <span class='latexinline'>b \subseteq sa</span>, where <span class='latexinline'>s(a) \equiv \{ s(a_i) : a_i \in a\}</span>. We can
define an equivalence relation <span class='latexinline'>a \sim b \iff a \leq b \land b \leq a</span>.
<b>Note</b>: <span class='latexinline'> b \leq a \implies |b| \leq |a|</span>, since 
<span class='latexinline'>b \leq a</span> means that <span class='latexinline'>b \subseteq s(a)</span>. Note that <span class='latexinline'>s</span> is actually a
function <span class='latexinline'>s: Q \rightarrow Q</span>, and a function mapped over a set can only
ever decrease the number of elements in a set, since a function can only
xglomp elements together; it can never break an element apart into two.
Hence, <span class='latexinline'>b \subseteq sa \subseteq a</span>, and thus <span class='latexinline'>|b| \leq |a|</span>.
Similiarly, <span class='latexinline'>a \leq b \implies |a| \leq |b|</span>. Therefore, <span class='latexinline'>b \sim a</span> means
that <span class='latexinline'>|b| = |a|</span>.
<b>Theorem</b>: for all <span class='latexinline'>a, b \in Q_X</span> such that
<span class='latexinline'>a ~ b</span> such that <span class='latexinline'>b \subseteq s(a)</span>, we show that <span class='latexinline'>b = s(a)</span>, and there exists
a <span class='latexinline'>t \in S_X</span> such that <span class='latexinline'>a = t(b)</span>.
<b>Proof</b>: Since <span class='latexinline'>b \subseteq s(a) \subseteq a</span> and <span class='latexinline'>|b| = |a|</span>, <span class='latexinline'>b = s(a)</span>.
Therefore <span class='latexinline'>s</span> is a permutation. Hence, <span class='latexinline'>s</span> is invertible and there exists
an inverse permutation <span class='latexinline'>t</span> such that <span class='latexinline'>a = t(b)</span>. We now need to show that
<span class='latexinline'>t \in S_X</span>. To do this, first note that if the order of the permutation
<span class='latexinline'>s</span> is <span class='latexinline'>n</span>, then <span class='latexinline'>t = s^{n-1}</span>, since <span class='latexinline'>t \circ s = s^{n-1} \circ s = 1_S</span>.
Since the semigroup <span class='latexinline'>S</span> is closed under composition <span class='latexinline'>t = s^{n-1}</span> is in <span class='latexinline'>S</span>,
since it is <span class='latexinline'>s</span> composed with itself  <span class='latexinline'>(n-1)</span> times.
<h4><a id=subset-families-of-interest href='#subset-families-of-interest'> § </a> Subset families of interest</h4>
We will be interest in a family of subsets of <span class='latexinline'>Q_X</span> called <span class='latexinline'>A</span>, of the form:
<ul><li> all sets of the form <span class='latexinline'>s(Q)</span> for all <span class='latexinline'>s \in S_X</span></li><li> the set <span class='latexinline'>Q</span></li><li> the empty set <span class='latexinline'>\emptyset</span></li><li> all the singleton sets <span class='latexinline'>\{ q \}</span> for all <span class='latexinline'>q \in Q</span>.</li></ul>
In the above set, we have <span class='latexinline'>\leq</span> and <span class='latexinline'>\sim</span> as defined above.
We note that the set <span class='latexinline'>A</span> is <b>closed under the action of all <span class='latexinline'>s \in S_X</span></b>.
For example, the empty set is taken to the empty set. All singleton
sets are taken to other singleton sets. For the full set <span class='latexinline'>Q</span>, we add
the sets <span class='latexinline'>s(Q)</span> for all <span class='latexinline'>s \in S_X</span>.
<h4><a id=height-function href='#height-function'> § </a> Height function</h4>
A height function for a transition system <span class='latexinline'>X \equiv (Q_X, S_X)</span> is a function
<span class='latexinline'>h: A \rightarrow \mathbb Z</span> such that:
<ol><li> <span class='latexinline'>h(\emptyset) = -1</span>.</li><li> <span class='latexinline'>h(\{ q \}) = 0 \forall q \in Q</span>.</li><li> <span class='latexinline'>a \sim b \implies h(a) = h(b)</span> for all <span class='latexinline'>a, b \in A</span>.</li><li> <span class='latexinline'>b < a \implies h(b) < h(a)</span> for all <span class='latexinline'>a, b \in A</span>.</li></ol>
The notation <span class='latexinline'>b < a \equiv (b \leq a) \land \lnot (a \leq b)</span>.
(3) + (4) imply that two elements of the same height are either equivalent
or incomparable.
<h4><a id=pavings-and-bricks href='#pavings-and-bricks'> § </a> Pavings and bricks</h4>
for <span class='latexinline'>a \in A</span> such that <span class='latexinline'>|a| > 1</span>, we denote by <span class='latexinline'>B_a</span> the set of all <span class='latexinline'>b \in A</span>
what are maximal subsets of <span class='latexinline'>a</span>. That is, if <span class='latexinline'>b \in B_a</span> then <span class='latexinline'>b \subsetneq a</span>,
and <span class='latexinline'>\not \exists c, b \subsetneq c \subsetneq a</span>. Equivalently, if there
exists a <span class='latexinline'>c</span> such that <span class='latexinline'>b \subseteq c \subseteq a</span>, then <span class='latexinline'>b = c</span> or <span class='latexinline'>b = a</span>.
Note that we can assert that <span class='latexinline'>a = \cup_{b \in B_a} b</span>. This is because <span class='latexinline'>B_a</span>
contains all the singletons of <span class='latexinline'>Q_X</span>. so we can begin by writing <span class='latexinline'>a</span> as
a union of singletons, and then merging elements of <span class='latexinline'>B_a</span> into larger elements
of <span class='latexinline'>B</span>, terminating when we cannot merge any more elements of <span class='latexinline'>B_a</span>.
<ul><li> The set <span class='latexinline'>B_a</span> is called as the <b>paving of <span class='latexinline'>a</span></b>.</li><li> The elements of <span class='latexinline'>B_a</span> are called as the <b>bricks of <span class='latexinline'>a</span></b>.</li></ul>
<h4><a id=group-of-permutations-for-a-in-a href='#group-of-permutations-for-a-in-a'> § </a> Group of permutations for <span class='latexinline'>a \in A</span></h4>
Let us assume that there exists a <span class='latexinline'>s \in S</span> such that <span class='latexinline'>s(a) = a</span>. Let <span class='latexinline'>A_a</span>
be the set of all elements in <span class='latexinline'>A</span> contained in <span class='latexinline'>a</span>:
<span class='latexinline'>A_a = \{ A_i : A_i \in A, A_i \subseteq a \}</span>.
Recall that the set <span class='latexinline'>A</span> was closed under the action of all <span class='latexinline'>s</span>, and hence,
since <span class='latexinline'>s</span> is a permutation of <span class='latexinline'>a</span>, this naturally extends into a
permutation of <span class='latexinline'>A_a</span>: <span class='latexinline'>s A_a = A_a</span>. Now note that this induces a permutation
of the set <span class='latexinline'>B_a</span>. This creates a transition system:
<div class='latexblock'>
\begin{align*}
&G_a \equiv \{ s \in S : s a = a \} \\
&H_a \equiv (B_a, G_a) \\
\end{align*}
</div>
We have already shown how if <span class='latexinline'>s \in S</span> defines a permutation of some set <span class='latexinline'>X</span>
by its action, then its inverse also exists in <span class='latexinline'>S</span>. So, this means that
<span class='latexinline'>G_a</span> is in fact a transition <i>group</i> that acts on <span class='latexinline'>B_a</span>.
It might turn out that <span class='latexinline'>G_a = \emptyset</span>. However, if <span class='latexinline'>G_a \neq \emptyset</span>,
then as stated above, <span class='latexinline'>G_a</span> is a group.
We will call such a transition group a <b>generalized transition group</b>, since
either <span class='latexinline'>G_a = \emptyset</span> or <span class='latexinline'>G_a</span> is a group.
Now, the generalized transition group <span class='latexinline'>H_a</span> is called as the
<b>holonomy transition system</b> of <span class='latexinline'>a</span>, and the group <span class='latexinline'>G_a</span> is called as
the <b>holonomy group</b> of <span class='latexinline'>a</span>.
We have that <span class='latexinline'>G_a \prec S</span> since <span class='latexinline'>G_a</span> is a quotient of the sub-semigroup
<span class='latexinline'>\{ s | s \in S, as = a \}</span>. (TODO: so what? why does this mean that it's <span class='latexinline'>\prec</span>?)
<b>Theorem:</b> if <span class='latexinline'>a \sim b</span>, then <span class='latexinline'>H_a \simeq H_b</span>
  (similar subsets have isomorphic holonomy transition systems).
<b>Proof:</b> Let us assume that <span class='latexinline'>a \neq b</span>. since <span class='latexinline'>a \sim b</span>, we have elements
of the form <span class='latexinline'>s, s^{-1} \in S</span> such that <span class='latexinline'>b = s(a)</span>, <span class='latexinline'>a = s^{-1}(b)</span>.
Recall that for <span class='latexinline'>b_a \in B_a</span> is such that for a member <span class='latexinline'>g \in G_a</span>,
<span class='latexinline'>g(b_a) = b_a</span>. <span class='latexinline'>B_b</span> must have the element <span class='latexinline'>s(b_a)</span>. [TODO!]
<h4><a id=holonomy-decomposition href='#holonomy-decomposition'> § </a> Holonomy decomposition</h4>
Let <span class='latexinline'>X \equiv (Q, S)</span> be a transition system and let <span class='latexinline'>h</span> be a height
function for <span class='latexinline'>X</span>, such that <span class='latexinline'>h(Q) > 0</span>. For a fixed <span class='latexinline'>i</span>,
let <span class='latexinline'>a_1, a_2, \dots a_k</span> be the representatives of equivalence classes of
elements of <span class='latexinline'>A</span> of height equal to <span class='latexinline'>i</span>. We define:
<div class='latexblock'>
H_i^\lor \equiv H_{a_1} \lor H_{a_2} \dots \lor H_{a_n}
</div>
<h4><a id=inductive-hypothesis-for-coverings href='#inductive-hypothesis-for-coverings'> § </a> Inductive hypothesis for coverings</h4>
We will say a relational covering <span class='latexinline'>X \triangleleft_{\phi} Y</span> is <b>of rank <span class='latexinline'>i</span></b>
with respect to a given height function <span class='latexinline'>h</span> if <span class='latexinline'>\phi</span> relates states in <span class='latexinline'>Y</span>
to subsets of states in <span class='latexinline'>x</span> that are members of <span class='latexinline'>A</span> and have rank at most i.
Formally, for each <span class='latexinline'>p \in Q_Y</span>, we have that <span class='latexinline'>\phi(p) \in A</span> and<span class='latexinline'>h(\phi(p)) \leq i</span>.
We prove that if <span class='latexinline'>X \triangleleft_{\phi} Y</span> is a relational covering of rank <span class='latexinline'>i</span>,
then <span class='latexinline'>X \triangleleft_{\phi} \overbar{H_i^\lor} \wr Y</span> is a relational covering
of rank <span class='latexinline'>i - 1</span>.
The proof is a proof by induction.
<h4><a id=base-case href='#base-case'> § </a> Base case:</h4>
Start with the relational covering with <span class='latexinline'>Q_Y = \{ 0 \}, S_Y = \{ id \}</span>,
and the cover <span class='latexinline'>\phi(0) = Q_X</span>. Clearly, this has rank <span class='latexinline'>n</span> since the height
of <span class='latexinline'>Q_X</span> is <span class='latexinline'>n</span>, and <span class='latexinline'>\phi</span> is inded a covering, since the only transition
that <span class='latexinline'>Y</span> can make (stay at the same state) is simulated by any transition
in <span class='latexinline'>S_X</span> [TODO: is this really the argument?]
For induction, assume <span class='latexinline'>X \triangleleft_{\phi} Y</span> is a relational covering of rank <span class='latexinline'>i</span>
with respect to some height function <span class='latexinline'>h</span>. <span class='latexinline'>X\equiv (Q_X, S_X)</span> and
<span class='latexinline'>Y \equiv (Q_Y, S_Y)</span>. We define
<ul><li> <span class='latexinline'>QY_i \equiv \{ q_y : q_y \in Q_Y, h(\phi(q_y)) = i \}</span></li><li> <span class='latexinline'>QY_< \equiv \{ q_y : q_y \in Q_Y, h(\phi(q_y)) < i \}</span></li></ul>
We know that <span class='latexinline'>A</span> contains elements of height exactly <span class='latexinline'>i</span>. Let <span class='latexinline'>a_1, a_2, \dots a_k</span>
be representatives of sets of of height <span class='latexinline'>i</span> in <span class='latexinline'>A</span>. Thus, for each <span class='latexinline'>qy_i \in QY_i</span>,
we have that:
<ul><li> <span class='latexinline'>\phi(qy_i) = a_j</span> for a <b>unique</b> <span class='latexinline'>1 \leq j \leq k</span>.</li><li> We select elements <span class='latexinline'>u, \overline{u} \in S</span> such that <span class='latexinline'>u(\phi(qy_i)) = a_j</span>
  and <span class='latexinline'>\overline{u}(a_j) = \phi(qy_i)</span>.</li></ul>
We will show how to establish a relational covering:
<ul><li> <span class='latexinline'>X \triangleleft_{\phi} \wr \overbar{H_i^\lor} Y</span> using a relation:</li><li> <span class='latexinline'>\phi \subseteq [(B_{a_1} \cup B_{a_2} \cup \dots B_{a_k})\times Q_Y ] \times Q_X</span></li></ul>
<h4><a id=references href='#references'> § </a> References</h4>
<ul><li> Automata, Languages and Computation by Elinberg.</li><li> <a href=http://www-verimag.imag.fr/~maler/Papers/kr-new.pdf>On the Krohn-Rhodes decomposition theorem by Oded Maler</a></li><li> <a href=http://www.egri-nagy.hu/pdf/holonomy_general.pdf>Ideas of the Holonomy Decomposition of Finite Transformation Semigroups</a></li><li> <a href=http://www-groups.mcs.st-andrews.ac.uk/~alanc/pub/c_semigroups/c_semigroups_a4.pdf>Nine chapters on the semigroup art</a></li><li> <a href=http://www.biomicsproject.eu/file-repository/category/CompHolonomy.pdf>Computational holonomy decompositions of transformation semigroups</a></li><li> <a href=http://graspermachine.sourceforge.net/>Algebraic Hierarchical Decomposition of finite automata: webpage with links to implementations</a></li><li> <a href=https://github.com/gap-packages/sgpdec><code>sgpdec</code> library on github</a></li><li> <a href=https://compsemi.wordpress.com/>Computational semigroup theory blog</a></li><li> <a href=https://arxiv.org/pdf/1306.1138.pdf>Compact notation for semigroup/automata</a></li></ul>
<h3><a id=proving-block-matmul-using-program-analysis href='#proving-block-matmul-using-program-analysis'> § </a> <a href=#proving-block-matmul-using-program-analysis>Proving block matmul using program analysis</a></h3>
It's a somewhat well-known fact that given matrix multiplication: <span class='latexinline'>O = AB</span>
where <span class='latexinline'>O \in \mathbb R^{2n \times 2m}</span> (<span class='latexinline'>O</span> for output),
<span class='latexinline'>A \in \mathbb R^{2n \times r}, B \in \mathbb R^{r \times 2m}</span> are matrices.
We can also write this as follows:
<div class='latexblock'>
\begin{bmatrix} o_{11} & o_{12} \\ o_{21} & o_{22} \end{bmatrix} =
\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix}
\begin{bmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \end{bmatrix}
=
\begin{bmatrix}
a_{11} b_{11} + a_{12} b_{21} & a_{11} b_{12} + a_{12} b_{22} \\
a_{21} b_{11}+ a_{22} b_{21} & a_{21} b_{12} + a_{22} b_{22}
\end{bmatrix}
</div>
When written as code, the original matrix multiplication is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
and the block-based matrix multiplication is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
we wish to show that both of these programs have the <i>same semantics</i>.
We will do this by appealing to ideas from program analysis.
<h4><a id=the-key-idea href='#the-key-idea'> § </a> The key idea</h4>
We will consider the statement:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
as occuring at an abstract "point in time" <span class='latexinline'>(i, j, k)</span> in the <code>matmul</code> function.
I also occurs at an abstract "point in time" <span class='latexinline'>(BI, BJ, i', j', k')</span> in
the <code>matmulBlock</code> function.
We will then show that the loops <code>for(i...) for(j...) for(k...)</code> are fully
parallel, and hence we can reorder the loops any way we want.
Then, we will show that the ordering imposed by <span class='latexinline'>(BI, BJ, i', j', k')</span>
is a reordering of the original <span class='latexinline'>(i, j, k)</span> ordering. We do this by
showing that there is a bijection:
<div class='latexblock'>
(i=i_0, j=j_0, k=k_0) \rightarrow (BI=i_0/N, BJ=j_0/N, i=i_0\%N, j=j_0\%N, k=k_0)
</div>
Thus, this bijection executes all loops, and does so without affecting the
program semantics.
<h4><a id=schedules href='#schedules'> § </a> Schedules</h4>
We'll zoom out a little, to consider some simple programs and understan
how to represent parallelism.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Notice that this program is equivalent to the program with the <span class='latexinline'>i</span> loop
reversed:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
What's actually <i>stopping</i> us from reversing the loop <code>for(j...)</code>? it's
the fact that the value of, say, <code>out[i=0][j=1]</code> <i>depends</i> on
<code>out[i=0][j=0]</code>. We can see that in general, <code>out[i=i_0][j=j_0]</code> <i>depends</i>
on <code>out[i=i_0][j=j_0-1]</code>. We can represent this by considering a
<i>dependence set</i>:
<div class='latexblock'>
\{ \texttt{write}:(i_0, j_0-1) \rightarrow \texttt{write}:(i_0, j_0) \}
</div>
in general, we can reorder statements as long as we do not change
the <i>directions</i> of the arrows in the dependence set.
<h4><a id=dependence-structure-of-matmul href='#dependence-structure-of-matmul'> § </a> Dependence structure of <code>matmul</code>.</h4>
<h4><a id=fully-parallel-reordering href='#fully-parallel-reordering'> § </a> Fully parallel, reordering</h4>
<h4><a id=references href='#references'> § </a> References</h4>
<ul><li> Optimizing Compilers for Modern Architectures: A Dependence-based Approach</li><li> <a href=http://polyhedral.info/>Polyhedral compilation</a></li></ul>
<h3><a id=why-i-like-algebra-over-analysis href='#why-i-like-algebra-over-analysis'> § </a> <a href=#why-i-like-algebra-over-analysis>Why I like algebra over analysis</a></h3>
Midnight discussions with my room-mate
<a href=https://researchweb.iiit.ac.in/~arjun.p/>Arjun P</a>.
This tries to explore what it is about algebra that I find appealing.
I think the fundamental difference to me comes down to flavour ---
analysis and combinatorial objects feel very "algorithm", while Algebra feels
"data structure".
To expand on the analogy, a proof technique is like an algorithm, while an
algebraic object is like a data structure. The existence of an algebraic object
allows us to "meditate" on the proof technique as a separate object that does
not move through time. This allows us to "get to know" the algebraic object,
independent of how it's used. So, at least for me, I have a richness of
feeling when it comes to algebra that just doesn't shine through with analysis.
The one exception maybe reading something like "by compactness", which has
been hammered into me by exercises from Munkres :)
Meditating on a proof technique is much harder, since the proof technique
is necessarily intertwined with the problem, unlike a data structure which
to some degree has an independent existence.
This reminds me of the quote: "“Art is how we decorate space;
Music is how we decorate time.”. I'm not sure how to draw out the
tenuous connection I feel, but it's there.
Arjun comes from a background of combinatorics, and my understanding of his
perspective is that each proof is a technique unto itself. Or, perhaps
instantiating the technique for each proof is difficult enough that abstracting
it out is not useful enough in the first place.
A good example of a proof technique that got studied on its own right in
combinatorics is the probabilistic method. A more reasonable example is that of
the Pigeonhole principle, which still requires insight to instantiate in
practise.
Not that this does not occur in algebra either, but there is something in
algebra about how just meditating on the definitions. For example,
Whitney trick that got pulled out of the proof of the Whitney embedding
theorem.
To draw an analogy for the haskellers, it's the same joy of being able to write
down the type of a haskell function and know exactly what it does, enough that
a program can automatically derive the function (djinn). The fact that we know
the object well enough that just writing the type down allows us to infer the
<i>program</i>, makes it beautiful. There's something very elegant about the
<i>minimality</i> that algebra demands. Indeed, this calls back to another quote:
"perfection is achieved not when there is nothing more to add, but when there
is nothing left to take away".
I'm really glad that this 2 AM discussion allowed me to finally pin down
why I like algebra.
<h3><a id=using-for-cleaner-function-type-typedefs href='#using-for-cleaner-function-type-typedefs'> § </a> <a href=#using-for-cleaner-function-type-typedefs><code>using</code> for cleaner function type typedefs</a></h3>
I've always struggled with remembering the syntax for function type typedefs:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
we can now use <code>using</code> for a far more pleasant syntax:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
which is also intuitive. You write down the "type"
on the right hand side, and give it a name on the left.
This is not strictly the same, since the <code>typedef</code>
<code>typedefs</code> <code>FNTYNAME</code> to a <i>function pointer type</i>, while
the C++ version typedefs the <i>function type</i>. I prefer
the latter at any rate, since I dislike the fact
that the usual typedef tends to hide the fact that a
function pointer is some pointer-like-thing.
<h3><a id=a-walkway-of-lanterns href='#a-walkway-of-lanterns'> § </a> <a href=#a-walkway-of-lanterns>A walkway of lanterns</a></h3>
<h4><a id=semidirect-products href='#semidirect-products'> § </a> Semidirect products</h4>
<ul><li> <span class='latexinline'>(\alpha \equiv \{ a, b, \dots\}, +, 0)</span></li><li> <span class='latexinline'>(\omega \equiv \{ X, Y, \dots\}, \times, 1)</span></li><li> <span class='latexinline'>\cdot ~: ~\omega \rightarrow Automorphisms(\alpha)</span></li></ul>
<ul><li> <a href=https://www.cse.iitk.ac.in/users/ppk/research/publication/Conference/2016-09-22-How-to-twist-pointers.pdf>How to twist pointers without breaking them</a></li></ul>
<ul><li> rotations: <span class='latexinline'>\mathbb Z 5</span></li><li> reflection: <span class='latexinline'>\mathbb Z 2</span></li></ul>
<ul><li> <span class='latexinline'>D_5 = \mathbb Z5 \rtimes \mathbb Z2</span></li></ul>
<div class='latexblock'>
\begin{align*}
\begin{bmatrix}
1 & 0 \\
a & X
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\
b & Y
\end{bmatrix}
=  \begin{bmatrix}
1 & 0 \\
a + X \cdot b & XY
\end{bmatrix}
\end{align*}
</div>
<ul><li> <span class='latexinline'>(Y \mapsto b) \xrightarrow{act} (X \mapsto a)</span></li></ul>
<ul><li> <span class='latexinline'>XY \mapsto a + X \cdot b</span></li></ul>
<h4><a id=a-walkway-of-lanterns href='#a-walkway-of-lanterns'> § </a> A walkway of lanterns</h4>
<ul><li> Imagine <span class='latexinline'>\mathbb Z</span> as a long walkway. you start at 0. You are but a poor lamp lighter.</li><li> Where are the lamps? At each <span class='latexinline'>i \in \mathbb Z</span>, you have a lamp that is either on, or off. So you have <span class='latexinline'>\mathbb Z2</span>.</li></ul>
<ul><li> <span class='latexinline'>L \equiv \mathbb Z \rightarrow \mathbb Z2</span> is our space of lanterns. You can act on this space by either moving using <span class='latexinline'>\mathbb Z</span>, or toggling a lamp using <span class='latexinline'>\mathbb Z2</span>. <span class='latexinline'>\mathbb Z2^{\mathbb Z} \rtimes \mathbb Z</span></li></ul>
<ul><li> <span class='latexinline'>g = (lights:\langle-1, 0, 1\rangle,  loc:10)</span></li><li> <span class='latexinline'>move_3: (lights: \langle \rangle, loc: 3)</span></li><li> <span class='latexinline'>move_3 \cdot g =  (lights:\langle-1, 0, 1\rangle,  loc:13)</span></li><li> <span class='latexinline'>togglex = (lights:\langle 0, 2 \rangle, loc: 0)</span></li><li> <span class='latexinline'>togglex \cdot g = (lights: \langle -1, 0, 1, 13, 15 \rangle, loc:13)</span></li><li> <span class='latexinline'>toggley = (lights: \langle -13, -12 \rangle, loc:0)</span></li><li> <span class='latexinline'>toggley\cdot g= (lights:\langle -1 \rangle, loc:13)</span></li></ul>
<h4><a id=krohn-rhodes-aka-how-to-model-freudian-psychoanalysis-using-lagrangians-over-semigroups href='#krohn-rhodes-aka-how-to-model-freudian-psychoanalysis-using-lagrangians-over-semigroups'> § </a> Krohn-rhodes, AKA how to model Freudian psychoanalysis using Lagrangians over semigroups.</h4>
<h3><a id=natural-transformations href='#natural-transformations'> § </a> <a href=#natural-transformations>Natural transformations</a></h3>
<img width=400  src="./static/natural-transformation.png">
I don't find people who draw "all three parts" of the natural transformation:
the catories <span class='latexinline'>C</span>, <span class='latexinline'>FC</span>, and <span class='latexinline'>GC</span>, and then show the relationship between
them, so I made this for my own reference.
<h3><a id=the-hilarious-commentary-by-dinosaure-in-ocaml-git href='#the-hilarious-commentary-by-dinosaure-in-ocaml-git'> § </a> <a href=#the-hilarious-commentary-by-dinosaure-in-ocaml-git>The hilarious commentary by dinosaure in OCaml git</a></h3>
the <a href=https://github.com/mirage/ocaml-git/>Ocaml-git</a> project is a
re-implementation of <code>git</code> in <code>OCaml</code>. It's well-written, and I was
walking through the codebase, when I found absolutely amazing, hilarious,
and deep comments from <code>dinosaure</code>. I really enjoyed reading through the
codebase, and the existence of these comments made it more humane to read.
I don't know who <code>dinosaure</code> is, but I'm really glad they wrote the comments
they did, it really made my day.
<h4><a id=the-one-that-takes-a-stab-at-haskell-for-fun href='#the-one-that-takes-a-stab-at-haskell-for-fun'> § </a> The one that takes a stab at Haskell for fun</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=the-academic-one-that-broken-links-to-a-paper href='#the-academic-one-that-broken-links-to-a-paper'> § </a> The academic one that broken-links to a paper</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=the-one-about-the-frustrations-of-bug-hunting href='#the-one-about-the-frustrations-of-bug-hunting'> § </a> The one about the frustrations of bug-hunting</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=the-one-about-a-potential-heisenbug href='#the-one-about-a-potential-heisenbug'> § </a> The one about a potential heisenbug</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=the-requisite-comment-in-french-for-an-ocaml-project href='#the-requisite-comment-in-french-for-an-ocaml-project'> § </a> The requisite comment in french for an OCaml project</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=the-deep-one href='#the-deep-one'> § </a> The deep one</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h3><a id=how-to-link-against-mlir-with-cmake href='#how-to-link-against-mlir-with-cmake'> § </a> <a href=#how-to-link-against-mlir-with-cmake>How to link against MLIR with CMake</a></h3>
Since <code>MLIR</code> hasn't setup the nice tooling that LLVM has around <code>CMake</code>
as far as I can tell, one needs to actually <i>know</i> <code>CMake</code> to link against
<code>MLIR</code>. However, as is well known, <code>CMake</code> incantations are handed down
by preists who spend the better part of their lives studying the tome
that is the CMake manual. I, an unlucky soul had to go on this adventure,
and I hope to spare you the trouble.
I wished to link against a static library build of MLIR. The secret
lies in the <code>find_library</code> call:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
I cribbed the actual things to link against from the path
<a href=https://github.com/llvm/llvm-project/blob/master/mlir/examples/toy/Ch2/CMakeLists.txt><code>mlir/examples/Toy/Ch2/CMakeLists.txt</code></a>
which helpfully lists MLIR things it needs to link against.
The full <code>CMakeLists</code> is here:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h3><a id=energy-as-triangulaizing-state-space href='#energy-as-triangulaizing-state-space'> § </a> <a href=#energy-as-triangulaizing-state-space>Energy as triangulaizing state space</a></h3>
This comes from The wild book by John Rhodes, which I anticipate I'll be posting more of in the coming weeks.
<h4><a id=experiments href='#experiments'> § </a> Experiments</h4>
Let an experiment be a tuple of the phase space <span class='latexinline'>X</span>, action space <span class='latexinline'>A</span>,
and an action of the actions onto the phase space
<span class='latexinline'>\curverightarrow: A \times X \rightarrow X</span>. We will write
<span class='latexinline'>x' = a \curverightarrow x</span> to denote the new state of the system
<span class='latexinline'>x</span>. So the experiment <span class='latexinline'>E</span> is the data
<span class='latexinline'>E \equiv (X, A, \curverightarrow : A \times X \rightarrow X)</span>.
<h4><a id=coordinate-systems href='#coordinate-systems'> § </a> Coordinate systems.</h4>
The existence of the action <span class='latexinline'>\curverightarrow</span> allows us to
write the evolution of the system recursively:
<span class='latexinline'>x_{t+1} = a \rightarrow x_t</span>.
However, to understand the final state <span class='latexinline'>x_{t+1}</span>, we need to essentially
"run the recursion", which does not permit us to 
<i>understand the experiment</i>.
What we really need is the ability to "unroll" the loop. To quote:
<blockquote> Informally, understanding an experiment <span class='latexinline'>E</span> means introducing coordinates into phase space of <span class='latexinline'>E</span> which are in triangular form under the action of the inputs of <span class='latexinline'>E</span>.</blockquote>
<h4><a id=conservation-laws-as-triangular-form href='#conservation-laws-as-triangular-form'> § </a> Conservation laws as triangular form</h4>
We identify certain interesting invariants of a system by two criteria:
<ol><li> The parameter <span class='latexinline'>Q(t)</span> determines some obviously important aspects of
   the system. That is, there is a deterministic function <span class='latexinline'>M(Q(t))</span> which
   maps <span class='latexinline'>Q(t)</span> to "measure" some internal state of the system.</li><li> If the values of such a  parameter <span class='latexinline'>Q</span> is known at time <span class='latexinline'>t_0</span> (denoted <span class='latexinline'>Q(t_0)</span>)
    and it is also known what inputs are presented to the
    system from time <span class='latexinline'>t</span> to time <span class='latexinline'>t + \epsilon</span>
    (denoted <span class='latexinline'>I[t_0, t_0 + \epsilon]</span>), then the new value of <span class='latexinline'>Q</span> is a
    deterministic function of <span class='latexinline'>Q(t_0)</span> and <span class='latexinline'>I[t_0, t_0+ \epsilon]</span>.</li></ol>
Such parameters allow us to understand a system, since they are deterministic
parameters of the evolution of the system, while also provding a way to
measure some internal state of the system using <span class='latexinline'>M</span>.
For example, consider a system <span class='latexinline'>x</span> with an energy function <span class='latexinline'>e(x)</span>. If we
perform an action <span class='latexinline'>a</span> on the system <span class='latexinline'>x</span>, then we can predict the action
<span class='latexinline'>e(x' = a \curvearrowright x)</span> given just <span class='latexinline'>e(x)</span> and <span class='latexinline'>a</span> --- here,
<span class='latexinline'>(x' = a \curverightarrow x)</span> is the action of the system <span class='latexinline'>a</span> on <span class='latexinline'>x</span>.
<blockquote> In general, conservation principles give a first coordinate of a triangularization. In the main a large part of physics can be viewed as discovering and introducing functions <span class='latexinline'>e</span> of the states <span class='latexinline'>q</span> of the system such that under action <span class='latexinline'>a</span>, <span class='latexinline'>e(a \curverightarrow q)</span> depends only on <span class='latexinline'>e(q)</span> and <span class='latexinline'>a</span>, and <b>not</b> on <span class='latexinline'>q</span>.</blockquote>
<h4><a id=theory-semidirect-and-wreath-products href='#theory-semidirect-and-wreath-products'> § </a> Theory: semidirect and wreath products</h4>
<ul><li> For semidirect products, I refer you to
    <a href=#the-cutest-way-to-write-semidirect-products>the cutest way to write semidirect products</a>
    <a href=#line-of-investigation-to-build-physical-intuition-for-semidirect-products>Line of investigation to build physical intuition for semidirect products</a>.</li></ul>
<h4><a id=symmetries-as-triangular-form href='#symmetries-as-triangular-form'> § </a> Symmetries as triangular form</h4>
<blockquote> We first heuristically indicate the construction involved in going from the group of symmetries to the triangularization, and then precisely write it out in all pedantic detail.</blockquote>
Let an experiment be <span class='latexinline'>E \equiv (X, A, \curverightarrow)</span>. Then we define <span class='latexinline'>\Pi</span>
is a <i>symmetry</i> of <span class='latexinline'>E</span> iff:
<ol><li> <span class='latexinline'>\Pi: X \rightarrow X</span> is a permutation of <span class='latexinline'>X</span>.</li><li> <span class='latexinline'>\Pi</span> commutes with the action of each <span class='latexinline'>a</span>:
       <span class='latexinline'> \Pi(a \curverightarrow x) = a \curverightarrow \Pi(x) </span>.</li></ol>
We say that the theory <span class='latexinline'>E</span> is <i>transitive</i> (in the action sense) if for
all <span class='latexinline'>x_1, x_2 \in X, x_1 \neq x_2</span>, there exists <span class='latexinline'>a_1, a_2, \dots a_n</span>
such that <span class='latexinline'> x_2 = a_n \curverightarrow \dots (a_1 \curverightarrow x_1) </span>.
Facts of the symmetries of a system:
<ol><li> We know that the symmetries of a theory <span class='latexinline'>E</span> form a group.</li><li> If <span class='latexinline'>E</span> is transitive, then each symmetry <span class='latexinline'>\Pi</span> is a regular permutation
   --- If there exists an <span class='latexinline'>x</span> such that <span class='latexinline'>\Pi(x_f) = x_f</span> (a fixed point), then
   this implies that <span class='latexinline'>\Pi(x) = x</span> for <i>all</i> <span class='latexinline'>x</span>.</li><li> Let the action split <span class='latexinline'>X</span> into disjoint orbits <span class='latexinline'>O_1, O_2, \dots O_k</span> from whom
   we choose representatives <span class='latexinline'>x_1 \in O_1, x_2 \in O_2, \dots x_k \in O_k</span>.
   Then, if <span class='latexinline'>E</span> is transitive, there is <i>exactly one</i> action that sends a
   particular <span class='latexinline'>x_i</span> to a particular <span class='latexinline'>x_j</span>. So, on fixing <i>one component</i>
   of an action, we fix <i>all components</i>.</li></ol>
To show that this gives rise to a triangulation, we first construct
a semigroup of the actions of the experiment:
<span class='latexinline'>S(E) \equiv \{ a_1 \dots a_n : n \geq 1 \text{~and~} a_i \in A \}</span>.
Now, let <span class='latexinline'>G = Sym(E)</span>, the full symmetry group of <span class='latexinline'>E</span>. One can apparently
express the symmetry group in terms of:
<div class='latexblock'>(X, S) \leq (G, G)  \wr (\{ O_1, O_2, \dots O_k\}, T)</div>
<h3><a id=the-cutest-way-to-write-semidirect-products href='#the-cutest-way-to-write-semidirect-products'> § </a> <a href=#the-cutest-way-to-write-semidirect-products>The cutest way to write semidirect products</a></h3>
Given two monoids <span class='latexinline'>(M, +, 0_M)</span> and <span class='latexinline'>(N, \times, 1_N)</span>, and a
homomorphism <span class='latexinline'>\phi: N \rightarrow End(M)</span>, where <span class='latexinline'>End(M)</span>
is the endomorphism group of <span class='latexinline'>M</span>. We will notate <span class='latexinline'>\phi(n)(m)</span> as <span class='latexinline'>n \cdot m \in M</span>.
Now the semidirect product <span class='latexinline'>M \ltimes_\phi N</span> is the set <span class='latexinline'>M \times N</span> equipped
with the multiplication rule:
<ul><li> <span class='latexinline'>(m, n) (m', n') = (m + n \cdot m', nn')</span></li></ul>
This can also be written down as:
<div class='latexblock'>
\begin{bmatrix}
1 & 0 \\ m & n
\end{bmatrix}
\begin{bmatrix}
1 & 0 \\ m' & n'
\end{bmatrix} =
\begin{bmatrix}
1 & 0 \\ m + n \cdot m' & n \times n'
\end{bmatrix}
</div>
This way of writing down semidirect products as matrices makes many things
immediately clear:
<ul><li> The semidirect product is some kind of "shear" transform, since that's
  what a shear transformation looks like, matrix-wise.</li><li> The resulting monoid <span class='latexinline'>M \ltimes_{\phi} N</span> has identity <span class='latexinline'>(0_M, 1_N)</span>,
  since for the matrix to be identity, we need the 2nd row to be <span class='latexinline'>(0, 1)</span>.</li><li> The inverse operation if <span class='latexinline'>(M, N)</span> were groups would have to be such that</li></ul>
<div class='latexblock'>
\begin{bmatrix} 1 & 0 \\ m + n \cdot m' & n \times n' \end{bmatrix} =
\begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
</div>
Hence:
<ul><li> <span class='latexinline'>nn' = 1</span> implies that <span class='latexinline'>n' = 1/n</span>.</li><li> <span class='latexinline'>m + n m' = 0</span> implies that <span class='latexinline'>m' = -m/n</span>.</li></ul>
which is indeed the right expression for the inverse.
<h3><a id=my-favourite-aplisms href='#my-favourite-aplisms'> § </a> <a href=#my-favourite-aplisms>My Favourite APLisms</a></h3>
<h4><a id=identity-matrix href='#identity-matrix'> § </a> identity matrix</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
This relies heavily on <code>⍴</code> replicating its arguments.
<h4><a id=histogram href='#histogram'> § </a> histogram</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
The use of <code>n[x] +←1</code> will stably write <code>+1</code> as many times as there are repeated
indexes in <code>xs</code>.
<h4><a id=string-matching-parity-as-fold- href='#string-matching-parity-as-fold-'> § </a> String matching / parity as fold <code>≠</code>:</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h3><a id=proof-of-chinese-remainder-theorem-on-rings href='#proof-of-chinese-remainder-theorem-on-rings'> § </a> <a href=#proof-of-chinese-remainder-theorem-on-rings>Proof of chinese remainder theorem on rings</a></h3>
<h4><a id=general-operations-on-ideals href='#general-operations-on-ideals'> § </a> General operations on ideals</h4>
We have at our hands a commutative ring <span class='latexinline'>R</span>, and we wish to study the ideal
structure on the ring. In particular, we can combine ideals in the following
ways:
<ol><li> <span class='latexinline'>I + J \equiv \{ i + j : \forall i \in I, j \in J \}</span></li><li> <span class='latexinline'>I \cap J \equiv \{ x : \forall x \in I \land x \in J \}</span></li><li> <span class='latexinline'>I\oplus J \equiv \{ (i, j) : \forall i \in I \land j \in J \}</span></li><li> <span class='latexinline'>IJ \equiv \{ ij : \forall i \in I \land j \in J \}</span></li></ol>
We have the containment:
<div class='latexblock'>
IJ \subseteq I \cap J \subseteq I, J \subseteq I + J \subseteq R
</div>
<h4><a id=ij-is-a-ideal-ij-subseteq-i-cap-j href='#ij-is-a-ideal-ij-subseteq-i-cap-j'> § </a> <span class='latexinline'>IJ</span> is a ideal, <span class='latexinline'>IJ \subseteq I \cap J</span></h4>
it's not immediate from the definition that <span class='latexinline'>IJ</span> is an ideal. The idea is
that given a sum <span class='latexinline'>\sum_k i_k j_k \in IJ</span>, we can write each <span class='latexinline'>i_k j_k = i'_k</span>,
since the ideal <span class='latexinline'>I</span> is closed under multiplication with <span class='latexinline'>R</span>. This gives
us <span class='latexinline'>\sum i'_k = i'' \in I</span>. Similarly, we can interpret <span class='latexinline'>\sum_k i_k j_k = \sum_k j'_k = j''k \in J</span>.
Hence, we get the containment <span class='latexinline'>IJ \subseteq I \cap J</span>.
<h4><a id=i-cap-j-subseteq-i-i-cap-j-subseteq-j href='#i-cap-j-subseteq-i-i-cap-j-subseteq-j'> § </a> <span class='latexinline'>I \cap J subseteq I</span>, <span class='latexinline'>I \cap J \subseteq J</span></h4>
Immediate from the inclusion function.
<h4><a id=i-j-subseteq-i-j href='#i-j-subseteq-i-j'> § </a> <span class='latexinline'>I, J \subseteq I + J</span></h4>
Immediate from inclusion
<h4><a id=crt-from-an-exact-sequence href='#crt-from-an-exact-sequence'> § </a> CRT from an exact sequence</h4>
There exists an exact sequence:
<div class='latexblock'>
\begin{align*}
0 \rightarrow I \cap J \xrightarrow{f} I \oplus J \xrightarrow{g} I + J \rightarrow 0 \\
&f(r) = (r, r) \\
&g((i, j)) = i + j
\end{align*}
</div>
We are forced into this formula by considerations of dimension. We know:
<div class='latexblock'>
\begin{align*}
&dim(I \oplus J) = dim(I) + dim(J) \\
&dim(I + J) = dim(I) + dim(J) - dim(I \cap J) \text{[inclusion-exclusion]} \\
&dim(I + J) = dim(I \oplus J) - dim(I \cap J) \\
&dim(I + J) - dim(I \oplus J) + dim(I \cap J) = 0\\
&V - E + F = 2
\end{align*}
</div>
By analogy to euler characteristic which arises from homology, we need to have
<span class='latexinline'>I \oplus J</span> in the middle of our exact sequence. So we must have:
<div class='latexblock'>
0 \rightarrow ? \rightarrow I \oplus J \rightarrow ?\rightarrow 0
</div>
Now we need to decide on the relative ordering between <span class='latexinline'>I \cap J</span> and <span class='latexinline'>I + J</span>.
<ul><li> There is  no <i>universal</i> way to send <span class='latexinline'>I oplus J \rightarrow I \cap J</span>. It's
  an unnatural operation to restrict the direct sum into the intersection.</li><li> There is a <i>universal</i> way to send <span class='latexinline'>I \oplus J \rightarrow I + J</span>: sum
  the two components. This can be seen as currying the addition operation.</li></ul>
Thus, the exact sequence <i>must</i> have <span class='latexinline'>I + J</span> in the image of <span class='latexinline'>I \oplus J</span>. This
forces us to arrive at:
<div class='latexblock'>
0 \rightarrow I \cap J \rightarrow I \oplus J \rightarrow I + J \rightarrow 0
</div>
The product ideal <span class='latexinline'>IJ</span> plays no role, since it's not possible to define a 
product of modules <i>in general</i> (just as it is not possible to define
a product of vector spaces). Thus, the exact sequence better involve
module related operations.  We can now recover CRT:
<div class='latexblock'>
\begin{align*}
0 \rightarrow I \cap J \xrightarrow{f} I \oplus J \xrightarrow{g} I + J \rightarrow 0 \\
0 \rightarrow R \xrightarrow{f} R \olpus R \xrightarrow{g} R \rightarrow 0 \\
0 \rightarrow R / (I \cap J) \rightarrow R/I \oplus R /J \rightarrow R/(I + J) \rightarrow 0
\end{align*}
</div>
<h4><a id=references href='#references'> § </a> References</h4>
<ul><li> <a href=https://www.youtube.com/watch?v=YxyxP894MLk>I learnt the material from this course on commutative algebra from IIT bombay</a>.</li></ul>
<h3><a id=monic-and-epic-arrows href='#monic-and-epic-arrows'> § </a> <a href=#monic-and-epic-arrows>monic and epic arrows</a></h3>
This is trivial, I'm surprised it took me <i>this long</i> to internalize this fact.
When we convert a poset <span class='latexinline'>(X, \leq)</span> into a category, we stipulate that
<span class='latexinline'>x \rightarrow y \iff x \leq y</span>.
If we now consider the category <span class='latexinline'>Set</span> of sets and functions between sets,
and arrow <span class='latexinline'>A \xrightarrow{f} B</span> is a function from <span class='latexinline'>A</span> to <span class='latexinline'>B</span>. If <span class='latexinline'>f</span> is
monic, then we know that <span class='latexinline'>|A| = |Im(f)| \leq |B|</span>. That is, a monic arrow
behaves a lot like a poset arrow!
Similarly, an epic arrow behaves a lot like the arrow in the inverse poset.
I wonder if quite a lot of category theoretic diagrams are clarified by thinking
of monic and epic directly in terms of controlling sizes.
<h3><a id=the-geometry-of-lagrange-multipliers href='#the-geometry-of-lagrange-multipliers'> § </a> <a href=#the-geometry-of-lagrange-multipliers>The geometry of Lagrange multipliers</a></h3>
If we want to minise a function <span class='latexinline'>f(x)</span> subject to the constraints <span class='latexinline'>g(x) = c</span>,
one uses the method of lagrange multipliers. The idea is to consider a new
function <span class='latexinline'>L(x, \lambda) = f(x) + \lambda (c - g(x))</span>. Now, if one has a local maxima
<span class='latexinline'>(x^\star, y^\star)</span>, then the conditions:
<ol><li> <span class='latexinline'>\frac{\partial L}{\partial x} = 0</span>: <span class='latexinline'>f'(x^\star) - \lambda g'(x^\star) = 0</span>.</li><li> <span class='latexinline'>\frac{\partial L}{\partial \lambda} = 0</span>: <span class='latexinline'>g(x^\star) = c</span>.</li></ol>
Equation (2) is sensible: we want our optima to satisfy the constraint that
we had originally imposed. What is Equation (1) trying to say?
Geometrically, it's asking us to keep <span class='latexinline'>f'(x^\star)</span> parallel to <span class='latexinline'>g'(x^\star)</span>.
Why is this a good ask?
Let us say that we are at an <span class='latexinline'>(x_0)</span> which is a feasible point (<span class='latexinline'>g(x_0) = c</span>).
We are interested in wiggling
<span class='latexinline'>(x_0) \xrightarrow{wiggle} (x_0 + \vec\epsilon) \equiv x_1</span>.
<ul><li> <span class='latexinline'>x_1</span> is still feasible: <span class='latexinline'>g(x_1) = c = g(x_0)</span>.</li><li> <span class='latexinline'>x_1</span> is an improvement: <span class='latexinline'>f(x_1) > f(x_0)</span>.</li></ul>
<ul><li> If we want <span class='latexinline'>g(x_1)</span> to not change, then we need <span class='latexinline'>g'(x_0) \cdot \vec \epsilon = 0</span>.</li><li> If we want <span class='latexinline'>f(x_1)</span> to be larger, we need <span class='latexinline'>f'(x_0) \cdot \vec \epsilon > 0</span>.</li></ul>
If <span class='latexinline'>f'(x_0)</span> and <span class='latexinline'>g'(x_0)</span> are parallel, then attempting to improve <span class='latexinline'>f(x_0 + \vec \epsilon)</span>
by change <span class='latexinline'>g(x_0 + \vec \epsilon)</span>, and thereby violate the constraint
<span class='latexinline'>g(x_0 + \episilon) = c</span>.
<h3><a id=efficient-tree-transformations-on-gpus href='#efficient-tree-transformations-on-gpus'> § </a> <a href=#efficient-tree-transformations-on-gpus>Efficient tree transformations on GPUs</a></h3>
All material lifted straight from <a href=https://scholarworks.iu.edu/dspace/handle/2022/24749>Aaron Hsu's PhD thesis</a>. I'll be converting
APL notation to C++-like notation.
<h4><a id=tree-repsentation-as-multi-dimensional-ragged-nested-arrays href='#tree-repsentation-as-multi-dimensional-ragged-nested-arrays'> § </a> Tree repsentation as multi-dimensional ragged nested arrays</h4>
We're interested in this tree:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
I'll be writing APL commands in front of a <code>$</code> to mimic bash, and I'll
write some arrays as multi-line. To run them, collapse them into a single
line. The <code>ast</code> object is represented in memory as:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Here's how read the array representation. Look at the top level of the tree.
we have a root node with three children:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
With the first <code>∘</code> being the root node, and the three adjacent cells
being the children.
Next, we look at how <code>x</code> is represented. This is predictably recursive. Let's
see the subtree under <code>x</code>:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Similarly for <code>y</code>:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
And so on, leading to the final representation:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Note that for this representation to work, we need to be able to:
<ul><li> nest arrays inside arrays.</li><li> have subarrays of different sizes (ragged arrays)</li><li> of different <i>nesting depths</i> --- so it's really not even an array?</li></ul>
I don't understand the memory layout of this, to be honest. I feel like to
represent this in memory would still rely on pointer-chasing, since we need
to box all the arrays. This is possibly optimised by APL to not be too bad.
<h4><a id=the-depth-vector-representation href='#the-depth-vector-representation'> § </a> The depth vector representation</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
If we visit this tree and record depths in pre-order <code>(node left right)</code>, we
arrive at the list:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
formatted as:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
This linearlized is the list:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
To convert the <code>ast</code> object into a depth vector representation, we can
use the following call:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Let's break this down:
TODO
<h4><a id=inverted-tables href='#inverted-tables'> § </a> Inverted tables</h4>
We represent data associated with our nodes as follows:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
This is the same thing as a
<a href=https://en.wikipedia.org/wiki/AoS_and_SoA#Structure_of_Arrays>structure of arrays (SOA) representation</a>,
where each array of information (eg, the depth at <code>data[1]</code>, the <code>T</code>
information at <code>data[2]</code>) are each <i>arrays</i> which can be accessed well on SIMD
instructions.
<h4><a id=ast-representation href='#ast-representation'> § </a> AST representation</h4>
TODO
<h4><a id=path-matrices href='#path-matrices'> § </a> Path matrices</h4>
We want information of how to go up and down the tree in ideally constant time.
We store this information in what is known as a <i>path matrix</i>.
For our recurring example, the path matrix is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
To efficiently compute this, we first replace every value in
our tree with its preorder traversal visit time. This changes
the tree to:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
The values we store in the tree are the integers. The old labels
are represented for clarity.
The path matrix for this tree is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=rendering-the-depth-information-in-2d href='#rendering-the-depth-information-in-2d'> § </a> Rendering the depth information in 2D</h4>
We use the incantation:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Let's break this down (the symbol <code> </code> means a lamp, for commenting/illumination)
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
The use of <code>¨</code> needs some explanation. <code>¨</code> is a higher order function which
takes a function and makes it a mapped version of the original function.
So, <code>,¨</code> is a function which attemps to map the concatenation operator.
Now, given two arrays <code>(1 2 3)</code>
and <code>(4 5 6)</code>, <code>(1 2 3) ,¨ 4 5 6</code> attemps to run <code>,</code> on each pair
<code>1 and 4</code>, <code>2 and 5</code>, <code>3 and 6</code>. This gives us tuples <code>((1 4) (2 5) (3 6))</code>.
So, for our purposes, <code>zip ← ,¨</code>.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> <code>⌈</code> is the maximum operator and <code>/</code> is the fold operator, so
  <code>⌈/d</code> finds the maximum in <code>d</code>. Recall that <code>(≢d)</code> find the no. of
   elements in <code>d</code>. <code>⍴</code> reshapes an array to the desired size. We pass it
   a <code>1x1</code> array containing only <code>-</code>, which gets reshaped into a
   <code>(⌈/d) x (≢d)</code> sizes array of <code>-</code> symbols.</li></ul>
TODO: explain @ and its use
<h4><a id=creating-the-path-matrix href='#creating-the-path-matrix'> § </a> Creating the path matrix</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
The incantation can be broken down into:
<ul><li> <code>(((⌈/d+1)(≢d))⍴0)</code> is used to create a <code>max(d+1)x|d|</code> dimension array of zeros.
   Here, the rows define depths, and the columns correspond to tree nodes
   which for us are their preorder indexes.</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> <code>((d ,¨ ⍳≢d))</code> creates an array of pairs <code>(depth, preindex)</code>. We will use
  this to fill index <code>(d, pi)</code> with the value <code>pi</code>.</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> <code>ixgrid ← ((⍳≢d)@writeixs) grid</code> rewrites at index <code>writeixs[i]</code> the value (<code>(i≢d)[i]</code>).</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> Finally, <code>⌈</code> is the maximum operator, and <code>\</code> is the <a href=>prefix scan</a> operator,
  so <code>⌈\ixgrid</code> creates a prefix scan of the above grid to give us our
  final path matrix:</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=using-the-path-matrix-distance-of-a-node-from-every-other-node href='#using-the-path-matrix-distance-of-a-node-from-every-other-node'> § </a> Using the path matrix:  distance of a node from every other node.</h4>
Note that the maximum distance between two nodes is to climb
all the way to the top node, and then climb down:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
If we know the lowest common ancestor of two nodes,
then the distance of one node to another is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
So, we can compute the depth as:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
[TODO: picture]
[TODO: finish writing this]
<h4><a id=parent-vector-representation href='#parent-vector-representation'> § </a> Parent vector representation</h4>
A parent vector is a vector of length <code>n</code> where <code>Parent[i]</code> denotes an
index into <code>Parent</code>. Hence, the following condition will return 1
if V is a parent vector.
For example, for our given example, here is the parent vector:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
The condition a parent vector must satisfy is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> <code>V ∊ (⍳≢V)</code> will be a list of whether each element in v belongs (<code>∊</code>) to the list
  <code>(⍳≢V) = [1..len(V)]</code></li><li> Recall that <code>/</code> is for reduction, and <code>∧/</code> is a boolean <code>AND</code> reduction.
  Hence, we compute whether each element of the vector <code>V</code> is in the range <code>[1..len(V)]</code>.</li><li> We add the constraint that root notes that don't have a parent simply
  point to themselves. This allows us to free ourselves from requiring
  some kind of <code>nullptr</code> check.</li></ul>
The root node (parent of all elements) can be found using the fixpoint operator (<code>⍨</code>):
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=converting-from-depth-vector-to-parent-vector-take-1 href='#converting-from-depth-vector-to-parent-vector-take-1'> § </a> Converting from depth vector to parent vector, Take 1</h4>
As usual, let's consider our example:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Note that the depth vector already encodes parent-child information.
<ul><li> The parent of node <code>i</code> is a node <code>j</code> such that <code>d[j] = d[i] - 1</code> and
  <code>j</code> is the closest index to the left of <code>i</code> such that this happens.</li></ul>
For example, to compute the parent of <code>t:11</code>, notice that it's at depth <code>2</code>.
So we should find all the nodes from <code>d[0..11]</code> which have depths equal to
<code>2</code>, and then pick the rightmost one. This translates to the expression:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
While this is intuitive, this does not scale: It does not permit us to find
the parent of all the nodes <i>at once</i> --- ie, it is not parallelisable
over choices of <code>t</code>.
<h4><a id=converting-from-depth-vector-to-parent-vector-take-2-or-scan-idiom href='#converting-from-depth-vector-to-parent-vector-take-2-or-scan-idiom'> § </a> Converting from depth vector to parent vector, Take 2 (Or scan idiom)</h4>
Imagine we have a list of <code>0</code>s and <code>1</code>s, and we want to find the <i>index</i> of
the rightmost <code>1</code> value. For example, given:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
we want the answer to be <code>f a = 8</code>. We saw an implementation in terms of
<code>f←{(⌽⍸⍵)[0]}</code> in Take 1.
(recall that <code>⍵</code> is the symbol for the right-hand-side argument of a function).
We're going to perform the same operation slightly differently. Let's consider
the series of transformations:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Why the hell does this work? Well, here's the proof:
<ul><li> On running <code>⌽a</code>, we reverse the <code>a</code>. The last 1 of <code>a</code> at index <span class='latexinline'>i</span>
  becomes the first <span class='latexinline'>1</span> of <code>⌽a</code> at index <span class='latexinline'>i' \equiv n-i</span>.</li><li> On running  <code>∨\ ⌽a</code>, numbers including and after the first 1
  become <code>1</code>. That is, all indexes <span class='latexinline'>j \geq i'</span> have 1 in them.</li><li> On running <code>+/ (∨\ ⌽a)</code>, we sum up all 1s. This will give us <span class='latexinline'>n-i'+1</span> 1s.
  That is, <span class='latexinline'>n-i'+1 = n-(n-i)+1 =i+1</span>.</li><li> We subtract a <span class='latexinline'>1</span> to correctly find the <span class='latexinline'>i</span> from <span class='latexinline'>i+1</span>.</li></ul>
This technique will work for <b>every row of a matrix</b>. This is paramount,
since we can now repeat this for the depth vector we were previously
interested in for each row, and thereby compute the parent index!
<h4><a id=converting-from-depth-vector-to-parent-vector-take-3-full-matrix href='#converting-from-depth-vector-to-parent-vector-take-3-full-matrix'> § </a> Converting from depth vector to parent vector, Take 3 (full matrix)</h4>
We want to extend the previous method we hit upon to compute the parents
of all nodes in parallel. To perform this, we need to run the moral
equivalent of the following:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
for <i>every single choice of t</i>. To perform this, we can build a 2D matrix
of <code>d[⍳t]=d[t]-1</code> where <code>t</code> ranges over <code>[0..len(d)-1]</code> (ie, it ranges
over all the nodes in the graph).
We begin by using:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> Note that <code>gt[i][j] = 1</code> iff <code>d[j] < d[i]</code>. So, for a given row (<code>i = fixed</code>), the <code>1s</code>
  nodes that are at lower depth (ie, potential parents).</li></ul>
<ul><li> If we mask this to only have those indeces where <code>j <= i</code>, then the
  last one in each row will be such that <code>d[last 1] = d[i] - 1</code>. Why? Because
  the node that is closest to us with a depth less than us <i>must</i> be our parent,
  in the preorder traversal.</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Combining the three techniques, we can arrive at:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
For comparison, the actual value is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
We have an off-by-one error for the <code>0</code> node! That's easily fixed, we simply
perform a maximum with <code>0</code> to move <code>¯1 -> 0</code>:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
So, that's our function:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Note that the time complexity for this is dominated by having to calculate
the outer products, which even given infinite parallelism, take <span class='latexinline'>O(n)</span> time.
We will slowly chip away at this, to be far better.
<h4><a id=converting-from-depth-vector-to-parent-vector-take-4-log-critial-depth href='#converting-from-depth-vector-to-parent-vector-take-4-log-critial-depth'> § </a> Converting from depth vector to parent vector, Take 4 (log critial depth)</h4>
We will use the Key(<code>⌸</code>) operator which allows us to create key value pairs.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
In fact, it allows us to apply an arbitrary function to combine keys and values.
We will use a function that simply returns all the values for each key.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Our first try doesn't quite work: it winds up trying to create a numeric matrix,
which means that we can't have different rows of different sizes. So, the
information that <i>only</i> index <code>0</code> is such that <code>d[0] = 0</code> is lost. What we
can to is to wrap the keys to arrive at:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Consider the groups <code>b[2] = (2 4 6 8 11 14)</code> and <code>b[3] = (5 9 10 12 13)</code>. All of <code>3</code>'s parents
are present in <code>2</code>. Every element in <code>3</code> fits at some location in <code>2</code>. Here is what
the fit would look like:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
We use the Interval Index(<code>⍸</code>) operator to solve the problem of finding the
parent / where we should sqeeze a node from <code>b[3]</code> into <code>b[2]</code>
(This is formally known as the
<a href=https://en.wikipedia.org/wiki/Predecessor_problem>predecessor problem</a>)
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Now, we can use the technology of predecessor to find parents
of depth 3 nodes among the depth 2 nodes:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
We need to know one-more APL-ism: the <code>2-scan</code>. When we write
a usual scan operation, we have:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
We begin by assuming the parent of <code>i</code> is <code>i</code> by using <code>p←⍳≢d</code>.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Now comes the biggie:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> <code>⍺</code> is the list of parent nodes.</li><li> <code>⍵</code> is the list of current child nodes.</li><li> We first find the indexes of our parent nodes by using
  the <code>pix ← parent ⍸ child</code> idiom.</li><li> Then, we find the actual parents by indexing into
  the parent list: <code>pix[parentixs]</code>.</li><li> We write these into the parents of the child using:
  <code>p[children] ← parent[parent ⍸ child]</code></li></ul>
This finally culminates in:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Which can be further golfed to:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
The total time complexity of this method assuming infinite parallelism is as follows:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> <code>(p←⍳≢d)</code> can be filled in <code>O(1)</code> time.</li><li> <code>(d2nodes←{⊂⍵}⌸d)</code> is searching for keys in a small integer domain, so this is <code>O(#nodes)</code> using
  radix sort as far as I know. However, the thesis mentions that this can be done in
  <code>O(log(|#nodes|))</code>. I'm not sure how, I need to learn this.</li><li> For each call of <code>findp</code>, the call <code>(pix ← ⍺⍸⍵)</code> can be implemented using binary search
  leading to a logarthmic complexity in the size of <code>⍺</code> (since we are looking up
  for predecessors of <code>⍵</code> in <code>⍺</code>).</li><li> The time complexity of the fold <code>2findp/d2nodes</code> can be done entirely in parallel
  since all the writes into the <code>p</code> vector are independent: we only write the
  parent of the current node we are looking at.</li></ul>
<h4><a id=34-computing-nearest-parent-by-predicate href='#34-computing-nearest-parent-by-predicate'> § </a> 3.4: Computing nearest Parent by predicate</h4>
I'm going to simplify the original presentation by quite a bit.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
We want to find nodes marked as <code>X</code> that are the closest parents to a
given node. The <code>X</code> vector is a boolean vector that has a <code>1</code> at
the index of each <code>X</code> node: <code>(b, e, f)</code>. So, the indexes <code>(1, 4, 5)</code>
are <code>1</code> in the <code>X</code> vector.
The output we want is the vector:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
The incantation is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
TODO. At any rate, since this does not require any writes and purely reads,
and nor does it need any synchronization, this is fairly straightforward
to implement on the GPU.
<h4><a id=35-lifting-subtrees-to-the-root href='#35-lifting-subtrees-to-the-root'> § </a> 3.5: Lifting subtrees to the root</h4>
Once we have marked our <code>X</code> nodes, we now wish to lift entire subtrees of <code>X</code>
up to the root.
<ul><li>  This pass displays how to lift subtrees and add new nodes to replace the subtree's original nodes.</li><li> Luckily, there are no <i>sibling</i> relationships that need to be maintained since
  we are uprooting an entire subtree.</li><li> There are no <i>ordering constraints</i> on how the subtrees should be arranged at
  the top.</li><li> Hence, we can simply add new nodes to the <i>end</i> of the tree (in terms of the preorder traversal).
  Adding to the middle of the tree will be discussed later.</li></ul>
There is some good advice in the thesis:
<blockquote> When using APL primitives this way, it may be good to map their names and definitions to the domain of trees. For example, the primitive <code>⍸Predicate</code> is read as "the nodes where <code>Predicate</code> holds" and not as "the indexes where <code>Predicate</code> is 1".</blockquote>
For example, given the tree:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
we want the transformed tree to be:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
We first look for nodes that need to be lifted.  There are:
<ul><li> Non-root nodes (ie, nodes whose parents are not themselves: <code>p≠(⍳≢p)</code>)</li><li> Which have the property <code>X</code>.</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=36-wrapping-expressions href='#36-wrapping-expressions'> § </a> 3.6: Wrapping Expressions</h4>
<h4><a id=37-lifting-guard-test-exprsessions href='#37-lifting-guard-test-exprsessions'> § </a> 3.7: Lifting Guard Test Exprsessions</h4>
<h4><a id=38-couting-rank-of-index-operators href='#38-couting-rank-of-index-operators'> § </a> 3.8: Couting rank of index operators</h4>
<h4><a id=39-flattening-expressions href='#39-flattening-expressions'> § </a> 3.9: Flattening Expressions</h4>
<h4><a id=310-associating-frame-slots-and-variables href='#310-associating-frame-slots-and-variables'> § </a> 3.10: Associating Frame slots and variables</h4>
<h4><a id=311-placing-frames-into-a-lexical-stack href='#311-placing-frames-into-a-lexical-stack'> § </a> 3.11: Placing frames into a lexical stack</h4>
<h4><a id=312-recording-exported-names href='#312-recording-exported-names'> § </a> 3.12: Recording Exported names</h4>
<h4><a id=313-lexical-resolution href='#313-lexical-resolution'> § </a> 3.13: Lexical Resolution</h4>
<h4><a id=521-traversal-idioms href='#521-traversal-idioms'> § </a> 5.2.1 Traversal Idioms</h4>
<h4><a id=522-edge-mutation-idioms href='#522-edge-mutation-idioms'> § </a> 5.2.2 Edge Mutation Idioms</h4>
<h4><a id=523-node-mutation-idioms href='#523-node-mutation-idioms'> § </a> 5.2.3 Node Mutation Idioms</h4>
<h3><a id=things-i-wish-i-knew-when-i-was-learning-apl href='#things-i-wish-i-knew-when-i-was-learning-apl'> § </a> <a href=#things-i-wish-i-knew-when-i-was-learning-apl>Things I wish I knew when I was learning APL</a></h3>
<ul><li> For pasting multi-line code,
  <a href=https://github.com/Dyalog/ride/issues/323>there is a bug in the bug tracker for RIDE</a>.
  For multi-line dfns, one can use <code>∇</code>. For multi-line values, I don't know yet.</li></ul>
<ul><li> Operators in APL terminology (such as <code>¨</code>) are higher order functions.
  Thus, an operator allows one to modify known functions.</li></ul>
<ul><li> Use <code>]disp</code> and <code>]display</code> to understand the structure of APL arrays.</li></ul>
<ul><li> Set <code>]box on -style=max</code> to <i>always enable</i> drawing arrays with <code>]display</code>.
  This is supremely useful as a newbie to understand array structure.</li></ul>
<ul><li> Set <code>]box on -trains=parens</code> to render trains as trees. Super
  helpful when attempting to grok <code>train</code> code.</li></ul>
<ul><li> Set <code>]boxing on</code> to enable boxing for trains, arguments, everything.</li></ul>
<h3><a id=every-ideal-that-is-maximal-wrt-being-disjoint-from-a-multiplicative-subset-is-prime href='#every-ideal-that-is-maximal-wrt-being-disjoint-from-a-multiplicative-subset-is-prime'> § </a> <a href=#every-ideal-that-is-maximal-wrt-being-disjoint-from-a-multiplicative-subset-is-prime>Every ideal that is maximal wrt. being disjoint from a multiplicative subset is prime</a></h3>
I ran across this when reading another question on math.se, so I
<a href=https://math.stackexchange.com/questions/3570129/proof-verification-request-complement-of-multiplicative-set-is-ideal-iff-the-id>posted this proof for verification</a> just to be sure I wasn't missing
something.
We wish to characterise prime ideals as precisely those that are disjoint from
a multiplicative subset <span class='latexinline'>S \subseteq R</span>. That is:
<ul><li> An ideal <span class='latexinline'>P</span> is prime iff <span class='latexinline'>P = R \setminus S</span>, where <span class='latexinline'>S</span> is a multiplicative subset
  that cannot be made larger (ie, is maximal wrt to the <span class='latexinline'>\subseteq</span> ordering).</li></ul>
I'll be using the definition of prime as:
<ul><li> An ideal <span class='latexinline'>P</span> is prime if for all <span class='latexinline'>x, y \in R</span>,
  <span class='latexinline'>xy \in P \implies x \in P \lor y \in P</span>.</li></ul>
<h4><a id=prime-ideal-implies-complement-is-maximal-multiplicative-subset href='#prime-ideal-implies-complement-is-maximal-multiplicative-subset'> § </a> Prime ideal implies complement is maximal multiplicative subset:</h4>
Let <span class='latexinline'>S = \equiv R \setminus P</span> be the complement of the prime ideal <span class='latexinline'>P \subsetneq R</span>
in question.
<ul><li> Since <span class='latexinline'>P \neq R</span>, <span class='latexinline'>1 \not \in </span>P. (if <span class='latexinline'>1 \in P</span>, then every element <span class='latexinline'>x . 1 \in P</span>
  since <span class='latexinline'>P</span> is an ideal, and must be closed under multiplication with the
  entire ring). Hence, <span class='latexinline'>1 \in S</span>.</li><li> For any <span class='latexinline'>x, y \in S</span>, we need <span class='latexinline'>xy \in S</span> for <span class='latexinline'>S</span> to be mulitplicative.</li><li> For contradiction, let us say that <span class='latexinline'>x, y \in S</span> such that <span class='latexinline'>xy \not \in S</span>.
  Translating to <span class='latexinline'>P</span>, this means that <span class='latexinline'>x, y \not \in P</span> such that <span class='latexinline'>xy \in P</span>.
  This contradictions the definition of <span class='latexinline'>P</span> being prime.</li></ul>
<h4><a id=ideal-whose-complement-is-maximal-multiplicative-subset-implies-ideal-is-prime href='#ideal-whose-complement-is-maximal-multiplicative-subset-implies-ideal-is-prime'> § </a> Ideal whose complement is maximal multiplicative subset implies ideal is prime.</h4>
<ul><li> Let <span class='latexinline'>I</span> be an ideal of the ring <span class='latexinline'>R</span> such that its complement <span class='latexinline'>S \equiv R / I</span>
  is a maximal multiplicative subset.</li><li> Let <span class='latexinline'>i_1 i_2 \in I</span>. For <span class='latexinline'>I</span> to be prime,
  we need to show that <span class='latexinline'>i_1 \in I</span> or <span class='latexinline'>i_2 \in I</span>.</li><li> For contradiction, let <span class='latexinline'>i_1, i_2 \not \in I</span>.
  Thus, <span class='latexinline'>i_1, i_2 \in S</span>. Since <span class='latexinline'>S</span> is multiplicative, <span class='latexinline'>i_1 i_2 \in S</span>. That is,
  <span class='latexinline'>i_1 i_2 \not \in I</span> (since <span class='latexinline'>I</span> is disjoint from <span class='latexinline'>S</span>).</li><li> But this violates our assumption that <span class='latexinline'>i_1 i_2 \in I</span>. Hence, contradiction.</li></ul>
<h3><a id=getting-started-with-apl href='#getting-started-with-apl'> § </a> <a href=#getting-started-with-apl>Getting started with APL</a></h3>
<ul><li> Install <a href=https://www.dyalog.com/download-zone.htm>Dyalog APL</a>.</li><li> Setup <a href=https://github.com/Dyalog/ride>RIDE</a>, the IDE for dyalog APL.
  This IDE comes with auto complete, good key bindings, a top bar chock-full of
  information of all the APL symbols. It's really well designed and a pleasure
  to use.</li><li> Follow the <a href=https://tutorial.dyalog.com/>Dyalog tutorial</a>, solving it
  chapter by chapter.</li><li> Bookmark <a href=https://aplcart.info/>APLCart</a>, a collection of APL idioms, and
  refer to it when in need.</li></ul>
<h3><a id=spacechem-was-the-best-compiler-i-ever-used href='#spacechem-was-the-best-compiler-i-ever-used'> § </a> <a href=#spacechem-was-the-best-compiler-i-ever-used>SpaceChem was the best compiler I ever used</a></h3>
It's kind of sad that this is the case, but on thinking about this, I realised
that the SpaceChem game was essentially a compiler, and it was such a pleasure
to learn how to use and debug --- the visual nature of it made it amazing to
find out.
<h3><a id=mnemonic-for-kruskal-and-prim href='#mnemonic-for-kruskal-and-prim'> § </a> <a href=#mnemonic-for-kruskal-and-prim>Mnemonic for Kruskal and Prim</a></h3>
I often forget which is which, so I came up with this:
<ul><li> Prim is very prim and proper, and therefore doesn't spread herself out. She
  picks out the minimum spanning tree one vertex at a time.</li></ul>
<h3><a id=legendre-transform href='#legendre-transform'> § </a> <a href=#legendre-transform>Legendre transform</a></h3>
<img  src="./static/legendre.png">
<h3><a id=cartesian-trees href='#cartesian-trees'> § </a> <a href=#cartesian-trees>Cartesian Trees</a></h3>
Cartesian trees construct a tree <span class='latexinline'>T = C(A)</span> given an array <span class='latexinline'>A</span>, such that
range minimum query (RMQ) on the array <span class='latexinline'>A</span> is equivalent to the lowest common ancestor (LCA)
of the nodes of the tree <span class='latexinline'>T</span>.
<img width=200 height=200 src="./static/cartesian-tree.svg">
Note that the tree looks like a <i>min-heap</i>.
To see the connection to LCA, if we want to find the range minimum in the range containing the
elements <code>[12, 10, 20, 15, 18]</code> of the array, the minimum is <code>10</code>, which is
indeed the lowest common ancestor of the nodes of <code>12</code> and <code>18</code> in the tree.
<h4><a id=building-a-cartesian-tree-in-linear-time href='#building-a-cartesian-tree-in-linear-time'> § </a> Building a Cartesian tree in linear time:</h4>
<h4><a id=converting-lca-to-rmq href='#converting-lca-to-rmq'> § </a> Converting LCA to RMQ</h4>
We can go the other way, and convert an LCA problem into a RMQ problem. We
perform an inorder traversal of the nodes, scribbling down the
depth of the node (<a href=https://youtu.be/0rCFkuQS968?t=934>Link to lecture at 15:30</a>).
We ask for the <i>argmin</i> version of RMQ, that gives us the <i>index</i> of
the node with the lowest depth. This gives us the index of where the node lives.
<h4><a id=universe-reduction-in-rmq href='#universe-reduction-in-rmq'> § </a> Universe reduction in RMQ</h4>
We can have an arbitrary ordered universe, on which we want to perform RMQ.
We can convert this to LCA by using a cartesian tree, and then convert to
a "clean" RMQ (by using the LCA -> RMQ using depth conversion). This now
will give us way faster operations (since we now have integers).
<h4><a id=-1-rmq href='#-1-rmq'> § </a> <code>+-1</code> RMQ:</h4>
We want the differences between nodes to have a difference of only <code>+-1</code>. We
had a much wider gap. Here, we perform an Euler tour (walk the tree DFS search order),
and sribble down every vertex we visit.
To find the LCA, we perform the RMQ on the locations of the <i>first</i> occurence
of the node. (I think we don't actually need the first occurence, any
occurence will do).
<h4><a id=references href='#references'> § </a> References</h4>
<ul><li> Material shamelessly written down from
  <a href=https://www.youtube.com/watch?v=0rCFkuQS968>Eric Demaine's excellent (MIT 6.851 Advanced Data Structures): Lecture 15</a></li><li> Image of the tree <a href=https://upload.wikimedia.org/wikipedia/commons/d/d5/Cartesian_tree.svg>taken from WikiMedia</a></li></ul>
<h3><a id=dfs-numbers-as-a-monotone-map href='#dfs-numbers-as-a-monotone-map'> § </a> <a href=#dfs-numbers-as-a-monotone-map>DFS numbers as a monotone map</a></h3>
Really, we want a partial order that is defined with the tree as the
Hasse diagram. However, performing operations on this is hard. Hence,
the DFS numbering is a good monotone map from this partial order
to the naturals, which creates a total order.
I want to think about this deeper, I feel that this might be a good way
to think about the <code>low</code> numbers that show up in
<a href=https://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm>tarjan's algorithm for strongly connected components</a>
This also begs the question: can we use other partial orders, that chunk
some information, but don't lose <i>all</i> the information as going to a total
order (the naturals) does?
<h3><a id=self-attention-not-really href='#self-attention-not-really'> § </a> <a href=#self-attention-not-really>Self attention? not really</a></h3>
The code is taken from <a href=https://nlp.seas.harvard.edu/2018/04/03/attention.html>The annotated transformer</a>
which explains the "attention is all you need paper".
On skimming the code, one sees the delightful line of code:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
where the line:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
seems to imply that we are, indeed, performing a self attention with the same
value <code>x</code> as the query, key, and value.
However, reading the code of the self-attention (or the paper) leads
one to realise:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
where we notice:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
where we see that <code>query, key, value</code> are being linearly transformed
before being used. Hence, an input of <span class='latexinline'>(x, x, x)</span> is transformed
to <span class='latexinline'>(q', k', v') = (Qx, Kx, Vx)</span> where <span class='latexinline'>Q, K, V</span> are arbitrary matrices.
Next, when we pass these into attention, the output we get is:
<div class='latexblock'>
\text{softmax}(q', k'^T) v = (Q x) (K x)^T (V x) = Q x x^T K^T V x
</div>
the code below is the same thing, spelled out:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
So It's not <i>really</i> self attention: it's more like: modulated attention
to self <code>:)</code>
<h3><a id=coarse-structures href='#coarse-structures'> § </a> <a href=#coarse-structures>Coarse structures</a></h3>
A coarse structure on the set <span class='latexinline'>X</span> is a collection of relations on <span class='latexinline'>X</span>:
<span class='latexinline'>E \subseteq 2^{X \times X}</span> (called as <i>controlled sets</i> / <i>entourages</i>)
such that:
<ul><li> <span class='latexinline'>(\delta \equiv \{ (x, x) : x \in X \}) \in  E</span>.</li><li> Closed under subsets: <span class='latexinline'>\forall e \in E, f \subset e \implies f \in E</span>.</li><li> Closed under transpose: if <span class='latexinline'>e \in E</span> then <span class='latexinline'>(e^T \equiv \{ (y, x) : (x, y) \in e \}) \in E</span>.</li><li> Closed under finite unions.</li><li> Closed under composition: <span class='latexinline'>\forall e, f \in E, e \circ f \in E</span>, where <span class='latexinline'>\circ</span>
  is composition of relations.</li></ul>
The sets that are controlled are "small" sets.
The bounded coarse structure on a metric space <span class='latexinline'>(X, d)</span> is the set of all relations
such that there exists a uniform bound such that all related elements are within
that bounded distance.
<div class='latexblock'>
(e \subset X \times X) \in E \iff \exists \delta \in \mathbb R, \forall (x, y) \in E, d(x, y) < \delta
</div>
We can check that the functions:
<ul><li> <span class='latexinline'>f: \mathbb Z \rightarrow \mathbb R, f(x) \equiv x</span> and</li><li> <span class='latexinline'>g: \mathbb R \rightarrow \mathbb Z, g(x) \equiv \lfloor x \rfloor</span></li></ul>
are coarse inverses to each other.
I am interested in this because if topology is related to semidecidability,
then coarse structures (which are their dual) are related to..?
<h4><a id=references href='#references'> § </a> References</h4>
<ul><li> <a href=http://www.ams.org/notices/200606/whatis-roe.pdf>What is a.. coarse structure by AMS</a></li></ul>
<h3><a id=matroids-for-greedy-algorithms href='#matroids-for-greedy-algorithms'> § </a> <a href=#matroids-for-greedy-algorithms>Matroids for greedy algorithms</a></h3>
<h4><a id=definitions-of-matroids href='#definitions-of-matroids'> § </a> Definitions of matroids</h4>
A matrioid <span class='latexinline'>M</span> is a set <span class='latexinline'>X</span> equipped with an independence set <span class='latexinline'>I \subseteq 2^X</span>.
<ul><li> The empty set is independent: <span class='latexinline'>\emptyset \in I</span>.</li><li> The independence set is downward-closed/closed under subsets:  <span class='latexinline'> \forall i \in I, \forall i' \subseteq i, i' \in I</span>.</li><li> For any independent sets <span class='latexinline'>A, B \in I</span>, if <span class='latexinline'>\vert A \vert</span> is larger than
  <span class='latexinline'>\vert B \vert</span>, then we must be able to add an element from
  <span class='latexinline'>a \in A</span> into <span class='latexinline'>B' \equiv B \cup {a}</span> such that <span class='latexinline'>B'</span> is both independent and larger than <span class='latexinline'>B</span>:
  <span class='latexinline'>B' \in I \land \vert B' \vert > \vert B \vert</span>. (<b>The exchange property</b>)</li></ul>
<h4><a id=example-1-linearly-independent-sets href='#example-1-linearly-independent-sets'> § </a> Example 1: Linearly independent sets</h4>
Let <span class='latexinline'>V</span> be a vector space. The independent sets <span class='latexinline'>I</span> are of the form:
<div class='latexblock'> I \equiv \{ S \subseteq V ~:~ \text{vectors in $S$ are lineary independent} \} </div>
This is an independence system because the empty set is linearly independent,
and subsets of a linearly independent collection of vectors will be linearly
independent.
The exchange property is satisfied because of linear algebraic reasons.
<h4><a id=example-2-the-graphiccyclic-matroid-matroid-of-forests href='#example-2-the-graphiccyclic-matroid-matroid-of-forests'> § </a> Example 2: The graphic/cyclic Matroid: Matroid of Forests</h4>
Let <span class='latexinline'>G = (V, E)</span> be a graph. Then collections of edges of the form:
<div class='latexblock'> I \equiv \{ F \subseteq E : \text{$F$ contains no cycles} \} </div>
is an independence system because the empty forest is a forest, and
a subset of edges of a forest continues to be a forest.
To check the exchange property, TODO
<h4><a id=example-3-the-partition-matroid href='#example-3-the-partition-matroid'> § </a> Example 3: The partition matroid</h4>
Consider the partition matroid <span class='latexinline'>M \equiv (E, I)</span>, where we have a
partitioning of <span class='latexinline'>E</span> known as <span class='latexinline'>E_i</span>, and numbers <span class='latexinline'>k_i</span> the
independence set consists of subsets <span class='latexinline'>F</span> which have at most <span class='latexinline'>k_i</span>
elements in common with each <span class='latexinline'>E_i</span>.
<div class='latexblock'>
I \equiv \{ F \subseteq E  : \forall i = 1, \dots N, \vert F \cap E_i \vert \leq k_i \}
</div>
The independence axioms are intuitively satisfied, since our constraints on picking
edges are of the form <span class='latexinline'>\vert F \cap E_i \vert \leq k_i</span>, which will continue to
hold as <span class='latexinline'>F</span> becomes smaller.
For the exchange axiom, let <span class='latexinline'>\vert Y \vert > \vert X \vert</span>. Then, we can assert that for some index
<span class='latexinline'>i</span>, it must be the case that <span class='latexinline'>\vert Y \cap E_i \vert > \vert X \cap E_i \vert</span>. Hence,
we can add an element in <span class='latexinline'>E_i \cap (Y / X)</span> into <span class='latexinline'>X</span> whilst still maintaining independence.
<h4><a id=bases-and-circuits href='#bases-and-circuits'> § </a> Bases and Circuits</h4>
<ul><li> <b>Bases</b> are the maximal independent sets of <span class='latexinline'>I</span> (ordered by inclusion). On adding an element into a basis element, they
  will become dependent. They are called bases by analogy with linear algebra.</li></ul>
<ul><li> <b>Circuits</b> are minimal dependent sets of <span class='latexinline'>I</span>. This comes from analogy with trees: if we remove an element
  from any circuit (loop) in a graph, what we are left with is a tree.</li></ul>
A matroid can be completely categorized by knowing either the bases or the circuits of that matroid.
<h4><a id=unique-circuit-property href='#unique-circuit-property'> § </a> Unique Circuit property</h4>
<ul><li> <b>Theorem</b>: Let <span class='latexinline'>M \equiv (E, I)</span> be a matroid, and let <span class='latexinline'>S \in I, e \in E</span> such that <span class='latexinline'>S \cup \{e \} \not \in I</span>.</li></ul>
Then, there exists a <b>unique circuit</b> <span class='latexinline'>C \subseteq S \cup \{ e \}</span>.
That is, when we go from independent to dependent by adding an element, we will
have a <b>single, unique circuit</b>. For example, when we add an edge into a
forest to create a cycle, this cycle will be unique!
<h4><a id=proof href='#proof'> § </a> Proof</h4>
Let <span class='latexinline'>C_1, C_2</span> be circuits
created when <span class='latexinline'>e</span> was added into <span class='latexinline'>S</span>, where <span class='latexinline'>C_1</span> is the <b>largest</b> circuit of <span class='latexinline'>S</span>,
and <span class='latexinline'>C_2</span> is the <b>smallest</b> circuit of <span class='latexinline'>S</span>.
Notice that <span class='latexinline'>C_1, C_2</span> <b>must</b> contain <span class='latexinline'>e</span> ---
if they did not, then <span class='latexinline'>C_1, C_2</span> would be circuits in
<span class='latexinline'>S</span>, contradicting the assumption that <span class='latexinline'>S</span> is independent.
Recall that <span class='latexinline'>C_1, C_2</span> are both circuits, which means that removing even a
single element from them will cause them to become independent sets.
Let us contemplate <span class='latexinline'>C \equiv C_1 \cup C_2</span>. Either <span class='latexinline'>C = C_1</span> in which
case we are done.
Otherwise, <span class='latexinline'>\vert C \vert > \vert C_1 \vert</span>, <span class='latexinline'>\vert C \vert > \vert C_2 \vert</span>.
Otherwise, consider <span class='latexinline'>C' \equiv C \ \{ e \} = (C_1 \cup C_2) \ \{e\} = (C_1 \ \{e\}) \cup (C_2 \ \{ e \})</span>.
<ul><li> <span class='latexinline'>C' \subseteq S</span>, since <span class='latexinline'>\C_1 \ \{e\}, C_2 \ \{e\} \subseteq S</span>.</li><li> <span class='latexinline'>S</span> is an independent set, all of whose subsets are independent by
  definition.  So <span class='latexinline'>C'</span> is an independent set.</li><li> <span class='latexinline'>\vert C' \vert \geq \vert C_1 \vert</span>, <span class='latexinline'>\vert C' \vert \geq \vert C_2 \vert</span>.</li></ul>
Now, we consider <span class='latexinline'>C</span>. Clearly, this is a dependent set,
since <span class='latexinline'>C_1 \subsetneq C</span>, and <span class='latexinline'>C_1</span> is a dependent set.
Since, <span class='latexinline'>C = C' \cup \{e \}</span>, this means that <span class='latexinline'>C'</span> is a maximally independent set.
Since <span class='latexinline'>C'</span> does not contain <span class='latexinline'>e</span>, <span class='latexinline'>C' = S</span>.
<h4><a id=rank-functions href='#rank-functions'> § </a> Rank functions</h4>
A rank function of a matroid <span class='latexinline'>M \equiv \langle X, I \rangle</span>
is a function:
<div class='latexblock'>
r: 2^X \rightarrow \mathbb N : r(S) = \max \{ \vert T \vert : T \subseteq S \land T \in I \}
</div>
That is, for any subset <span class='latexinline'>S \subseteq X</span>, <span class='latexinline'>r(S)</span> is the cardinality of the
largest independent subset of <span class='latexinline'>S</span>.
<ul><li> In the matroid of linearly independent sets of vectors, the rank of
  a set of vectors is the dimension of their spanning set.</li></ul>
In this matroid, the
TODO: picture
<h4><a id=bipartite-matching-as-matroid-intersection href='#bipartite-matching-as-matroid-intersection'> § </a> Bipartite matching as matroid intersection</h4>
Matchings in a bipartite graph <span class='latexinline'>G = (V, E)</span> with partition <span class='latexinline'>(A, B)</span> arise
as the intersection of the independent sets of two matroids.
We will denote by <span class='latexinline'>\delta: V \rightarrow 2^E</span> the function which takes
a vertex to the set of edges incident on that vertex.
Let <span class='latexinline'>M_A</span> be a <i>partition matroid</i>: <span class='latexinline'>M_A \equiv (E, I_A)</span> where <span class='latexinline'>I_A</span> is:
<div class='latexblock'>
I_A \equiv \{ F \subseteq E : \vert F \cap \delta(a) \vert \leq 1 \forall a \in A \}
</div>
That is, in <span class='latexinline'>I_A</span>, every independent set has for each vertex of <span class='latexinline'>A</span>, at most
one edge incident. We need to check that this is an independent set.
The empty set of no edges is independent. If some collection of edges are
such that they have at most one edge incident, then removing edges can
only <i>decrease</i> incidence. Hence, it's also downward closed.
TODO: add picture
Similarly, we define <span class='latexinline'>M_B \equiv (E, I_B)</span>:
<div class='latexblock'>
I_B \equiv \{ F \subseteq E : \vert F \cap \delta(b) \vert \leq 1 \forall b \in B \}
</div>
Now, notice that any collection of edges <span class='latexinline'>F \in I_A \cap I_B</span> is a legal
matching, since the edges cover all vertices of <span class='latexinline'>A</span> and <span class='latexinline'>B</span> at most once.
The largest element of <span class='latexinline'>I_A \cap I_B</span> is the <i>maximum matching</i> that we
are looking for.
<h4><a id=largest-common-independent-set href='#largest-common-independent-set'> § </a> Largest common independent set</h4>
Given two matroids <span class='latexinline'>M_1 \equiv (E, I_1)</span>, <span class='latexinline'>M_2 \equiv (E, I_2)</span>, with rank
functions <span class='latexinline'>r_1</span> and <span class='latexinline'>r_2</span>. Let <span class='latexinline'>S \in I_1 cap I_2</span> and let <span class='latexinline'>F \subseteq E</span>.
<ul><li>  <span class='latexinline'>\vert S \vert = \vert S \cap F \vert + \vert S \cap (E / F) \vert</span>.</li></ul>
<h4><a id=references href='#references'> § </a> References:</h4>
<ul><li> <a href=http://math.mit.edu/~goemans/18433S11/matroid-notes.pdf>Michel Goeman's standalone notes on matroids</a></li><li> <a href=http://math.mit.edu/~goemans/18433S11/matroid-intersect-notes.pdf>Michel Goeman's standalone notes on matroid intersection</a></li><li> <a href=https://math.mit.edu/~goemans/18438F09/lec11.pdf>Lecture 11 of Michel Goeman's lecture on Advanced Combinatorial Optimisation</a></li></ul>
<h3><a id=grokking-zariski href='#grokking-zariski'> § </a> <a href=#grokking-zariski>Grokking Zariski</a></h3>
There's a lot written on the Zariski topology on the internet, but most
of them lack explicit examples and pictures. This is my attempt to
communicate what the Zariski topology looks like, from the perspectives
that tickle my fancy (a wealth of concrete examples,
topology-as-semi-decidability, and pictures).
<h4><a id=the-zariski-topology href='#the-zariski-topology'> § </a> The Zariski topology</h4>
Recall that the Zariski topology is defined by talking about what its
closed sets are. The common zero sets of a family of polynomials are
the closed sets of the Zariski topology. Formally, the topology <span class='latexinline'>(\mathbb R^n, \tau)</span>
has as closed sets:
<div class='latexblock'>
\{ x \in \mathbb R^n : \forall f_i \in \mathbb R[X_1, X_2, \dots X_n],  f_i(x) = 0  \}
</div>
Open sets (the complement of closed sets) are of them form:
<div class='latexblock'>
\{ x \in \mathbb R^n : \exists f_i \in \mathbb R[X_1, X_2, \dots X_n],  f_i(x) \neq 0  \} \in \tau
</div>
The empty set is generated as <span class='latexinline'>\{ x \in \mathbb R^n : 0 \neq 0 \}</span> and the
full set is generated as <span class='latexinline'>\{ x \in \mathbb R^n : 1 \neq 0 \}</span>.
<h4><a id=semi-decidability href='#semi-decidability'> § </a> Semi-decidability</h4>
Recall that in this view of topology, for a space <span class='latexinline'>(X, \tau)</span>, for every
open set <span class='latexinline'>O \in \tau</span>, we associate a turing machine <span class='latexinline'>T_O</span> which semi-decides
inclusion. That is, if a point is in <span class='latexinline'>O</span> then it halts with the output <code>IN-SET</code>.
if the point is not in <span class='latexinline'>O</span>, <span class='latexinline'>T_O</span> infinite loops. Formally:
<div class='latexblock'>
\begin{align*}
x &&\in O \iff \text{$T_O$ halts on input $x$} \\
x &&\not \in O \iff \text{$T_O$ does not halts on input $o$}
\end{align*}
</div>
Alternatively, for a closed set <span class='latexinline'>C \in \tau</span>, we associate a a turing machine
<span class='latexinline'>T_C</span> which semi-decides exclusion. That is, if a point is <i>not</i> in <span class='latexinline'>C</span>
it halts with the output "NOT-IN-SET". If the point is in <span class='latexinline'>C</span>, the turing
machine <span class='latexinline'>T_C</span> infinite loops.
Now in the case of polynomials, we can write a function that semidecides
exclusion from closed sets:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Since we can only evaluate a polynomial up to some finite precision, we
start with zero precsion and then gradually get more precise. If some
<span class='latexinline'>x0</span> is <i>not</i> a root of <span class='latexinline'>poly</span>, then at some point, we will have
that <span class='latexinline'>poly(x0) /neq 0</span>. If <span class='latexinline'>x0</span> is indeed a root, then we will never halt
this process; We will keep getting <code>poly(x0[:precision]) = 0</code> for all
levels of precision.
<h4><a id=specr href='#specr'> § </a> <code>Spec(R)</code></h4>
<ul><li> To setup a topology for the prime spectrum of a ring, we define the topological
  space as <span class='latexinline'>Spec(R)</span>, the set of all prime ideals of <span class='latexinline'>R</span>.</li></ul>
<ul><li> The closed sets of the topology are <span class='latexinline'>\{ V(I) : I \text{is an ideal of } R \}</span>,
  where the function <span class='latexinline'>V: \text{Ideals of } R \rightarrow 2^{\text{Prime ideals of } R}</span>
  each ideal to the set of prime ideals that contain it. Formally,
  <span class='latexinline'>V(I) = \{ p \in Spec(R) : I \subseteq p \}</span>. </li></ul>
<ul><li> We can think of this differently, by seeing that we can rewrite the condition
  as  <span class='latexinline'>V(I) = \{ P \in Spec(R) : I \xrightarrow{P} 0 \}</span>: On rewriting using the prime ideal <span class='latexinline'>P</span>, we send the ideal <span class='latexinline'>I</span> to <span class='latexinline'>0</span>.</li></ul>
<ul><li> Thus, the closed sets of <span class='latexinline'>Spec(R)</span> are precisely the 'zeroes of polynomials' / 'zeroes of ideals'.</li></ul>
<ul><li> To make the analogy precise, note that in the polynomial case, we imposed a 
  topology on <span class='latexinline'>\mathb R</span> by saying that the closed sets were <span class='latexinline'>V(p_i) = \{ x \in \mathbb R : p(x) = 0 \}</span>
  for some polynomial <span class='latexinline'>p \in \mathbb R[x]</span>. 
  </li><li> Here, we are saying that the closed sets are <span class='latexinline'>V(I) = \{ x \in Spec(R) : I(x) = 0 \}</span>
  for some ideal <span class='latexinline'>I \in R</span>. so we are looking at  ideals as functions
  from the prime ideal to the reduction of the ideal. That is, <span class='latexinline'>I(P) = I/P</span>. </li></ul>
<h4><a id=specmathbb-z-from-this-perspective href='#specmathbb-z-from-this-perspective'> § </a> <span class='latexinline'>Spec(\mathbb Z)</span> from this perspective</h4>
Since <span class='latexinline'>\mathbb Z</span> is a PID, we can think of numbers instead of ideals. The above
picture asks us to think of a number as a function from a prime to a reduced
number. <span class='latexinline'>n(p) = n \% p</span>. We then have that the closed sets are those primes
which can zero out some number. That is:
<div class='latexblock'>
\begin{align*}
V(I) = \{ x \in Spec(\mathbb Z) : I(x) = 0 \}
V((n)) = \{ (p) \in Spec(\mathbb Z) : (n)/(p) = 0 \}
V((n)) = \{ (p) \in Spec(\mathbb Z) : (n) \mod (p) = 0 \}
\end{align*}
</div>
So in our minds eye, we need to imagine a space of prime ideals (points), which
are testing with all ideals (polynomials). Given a set of prime ideals (a tentative locus, say a circle),
the set of prime ideals is closed if it occurs as the zero of some collection of ideas 
(if the locus occurs as the zero of some collection of polynomials). 
<h4><a id=nilpotents-of-a-scheme href='#nilpotents-of-a-scheme'> § </a> Nilpotents of a scheme</h4>
<ul><li> Consider the functions <span class='latexinline'>f(x) = (x - 1)</span> and <span class='latexinline'>g(x) = (x - 1)^2</span>. as functions on <span class='latexinline'>\mathbb R</span>.
  They are indistinguishable based on Zariski, since their zero sets are
  the same (<span class='latexinline'>\{ 1 \}</span>).</li></ul>
<ul><li> If we now move to the scheme setting, we get two different schemes: <span class='latexinline'>R_g \equiv \mathbb R[X] / (x - 1) \simeq \mathbb R</span>,
  and <span class='latexinline'>R_g \equiv R[X] / (x - 1)^2</span>. </li></ul>
<ul><li> In the ring <span class='latexinline'>R_g</span>, we have an element <span class='latexinline'>(x-1)</span>
  such that <span class='latexinline'>(x-1)^2 = 0</span>, which is a nilpotent. This "picks up" on the repeated
  root.</li></ul> - So the scheme is stronger than Zariski, as it can tell the difference
   between these two situations. So we have a kind of "infinitesimal thickening"
   in the case of <span class='latexinline'>R_g</span>.
   <a href=https://math.stackexchange.com/questions/3751728/geometric-meaning-of-the-nilradical>For more, check the <code>math.se</code> question: 'Geometric meaning of the nilradical'</a>
<ul><li> Another example is to consider (i) the pair of polynomials <span class='latexinline'>y = 0</span>
  (the x-axis) and <span class='latexinline'>y = \sqrt(x)</span>.
  The intersection is the zero set of the ideal
  <span class='latexinline'>(y, y^2 - x) = (y, 0 - x) = (y, x)</span>.</li></ul>
<ul><li> Then consider (ii) the pair of <span class='latexinline'>y = 0</span> (the x-axis) and <span class='latexinline'>y = x^2</span>.
  Here, we have "more intersection" along
  the x-axis than in the previous case, as the parabola is "aligned" to the x-axis.
  The intersection is the zero set of the ideal
  <span class='latexinline'>(y, y - x^2) = (y, 0 - x^2) = (y, x^2)</span>.</li></ul>
<ul><li> So (i) is governed by <span class='latexinline'>\mathbb R[X, Y] / (y, x)</span>, while (ii) is governed by <span class='latexinline'>\mathbb R[X, Y] /(y, x^2)</span>
  which has a nilpotent <span class='latexinline'>x^2</span>. This tells us that in (ii), there is an
  "infinitesimal thickening" along the <span class='latexinline'>x</span>-axis
  of the intersection.</li></ul>
<h3><a id=my-preferred-version-of-quicksort href='#my-preferred-version-of-quicksort'> § </a> <a href=#my-preferred-version-of-quicksort>My preferred version of quicksort</a></h3>
Wikipedia lists the implementation of quicksort as:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Here, the indeces <code>[lo..i-1]</code> have values less than the pivot, while
<code>[i..j]</code> are great or equal to the pivot.
<h4><a id=the-version-i-prefer href='#the-version-i-prefer'> § </a> The version I prefer</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
This implementation to me makes very clear to me what information is "known":
<ul><li> The segments that is strictly less than the partition.</li><li> The segment that is strictly great or equal the partition.</li></ul>
It also makes clear what is being "probed"/"tentative":
<ul><li> anything we are accessing as <code>+-1</code> is not known yet, we are feeling out
  the boundaries of our partitions.</li></ul>
The termination condition is clear: when one partition starts reaching into
the other partitions resources, its done.
Due to using closed intervals everywhere, it's very easy to see precisely
what data starts and ends where.
What version of quicksort do you prefer? Drop me an email!
<h3><a id=geometric-proof-of-cauchy-schwarz-inequality href='#geometric-proof-of-cauchy-schwarz-inequality'> § </a> <a href=#geometric-proof-of-cauchy-schwarz-inequality>Geometric proof of Cauchy Schwarz inequality</a></h3>
<img src="./static/cauchy-schwarz.svg">
<ul><li> All credit goes to <code>p0a</code> on <code>##math</code> on freenode for teaching me this proof!</li></ul>
Here's one fun application of Cauchy-Schwarz. We can apply it to two vectors
<span class='latexinline'>x=(\sqrt a, \sqrt b)</span> and <span class='latexinline'>y=(\sqrt b, \sqrt a)</span> to derive the AM-GM
inequality:
<h3><a id=dataflow-analysis-using-grobner-basis href='#dataflow-analysis-using-grobner-basis'> § </a> <a href=#dataflow-analysis-using-grobner-basis>Dataflow analysis using Grobner basis</a></h3>
This was a quick experiment in using Grobner basis to model situations. We
can represent our dataflow analysis constraints in terms of polynomial
rewrites over <span class='latexinline'>F_2</span>.
Given the program:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
whose semantics I hope are fairly straightforward --- the dictionary represents
instruction locations. Instructions proceed sequentially. branch moves
control flow around. Note that <code>br</code> can branch to multiple locations,
since we are not control-flow sensitive.
The idea is that since in a dataflow analysis, we need information at
each variable at each program point, we can create a ring of polynomials
over <span class='latexinline'>F_2</span> for each variable at each program point. So in this case,
we wold need:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
We then add elements into the ideal that represents our constraints.
For example, to perform dataflow analysis, we need to add constraints
about how if a variable <code>z</code> is alive, all variables that are used
to compute <code>z</code> at <code>100</code> are alive. This sets up equations that may
have cycles (in the case of loops).
These are usually resolved using the
<a href=https://en.wikipedia.org/wiki/Data-flow_analysis#An_iterative_algorithm>Kildall algorithm</a>.
However, we can also ask SAGE to kindly solve the Grobner basis. I hypothesize
that the "easy" dataflow problems out to be <a href=https://hal.inria.fr/inria-00074446/document>toric ideals</a>
which admit much faster solutions.
<h3><a id=fenwick-trees-and-orbits href='#fenwick-trees-and-orbits'> § </a> <a href=#Fenwick-trees-and-orbits>Fenwick trees and orbits</a></h3>
I learnt of a nice, formal way to prove the correctness of Fenwick
trees in terms of orbits that I wish to reproduce here.
One can use a Fenwick tree to perform cumulative sums
<span class='latexinline'>Sum(n) \equiv \sum_i^n A[i]</span>, and updates <span class='latexinline'>Upd(i, v) \equiv A[i] += v</span>. Naively,
cumulative sums can take <span class='latexinline'>O(n)</span> time and updates take <span class='latexinline'>O(1)</span> time.
A Fenwick tree can perform <i>both</i> in <span class='latexinline'>\log(n)</span>. In general, we can perform
any monoid-based catenation and update in <span class='latexinline'>\log(n)</span>.
<h4><a id=organization href='#organization'> § </a> organization</h4>
We allow indexes <span class='latexinline'>[1, 2, \dots n]</span>. The node with factorization <span class='latexinline'>i \equiv 2^k \times l</span>,
<span class='latexinline'>2 \not \vert l</span> (that is, <span class='latexinline'>k</span> is the highest power of <span class='latexinline'>2</span> in <span class='latexinline'>i</span>)
is responsible for the interval <span class='latexinline'>[i-2^k+1, i] = (i-2^k, i]</span>.
I'm going to state all the manipulations in terms of prime factorizations,
since I find it far more intuitive than bit-fiddling. In general, I want
to find a new framework to discover and analyze bit-fiddling heavy algorithms.
Some examples of the range of responsibility of an index are:
<ul><li> <span class='latexinline'>1 = 2^0 \times 1 = (0, 1]</span> (Subtract <span class='latexinline'>2^0 = 1</span>)</li><li> <span class='latexinline'>2 = 2\times 1 = (0, 2]</span> (Subtract <span class='latexinline'>2^1 = 2</span>)</li><li> <span class='latexinline'>3 = 3 = (2, 3]</span></li><li> <span class='latexinline'>4 = 2^2 = (0, 4]</span></li><li> <span class='latexinline'>5 = 5 = (4, 5]</span></li><li> <span class='latexinline'>6 = 2\times 3 = (4, 6]</span></li><li> <span class='latexinline'>7 = 7 = (6,7]</span></li><li> <span class='latexinline'>8 = 2^3 = (0,8]</span></li><li> <span class='latexinline'>9 = 9 = (8, 9]</span></li><li> <span class='latexinline'>10 = 2\times 5 = (8, 10]</span></li><li> <span class='latexinline'>11 = 11 = (10, 11]</span></li><li> <span class='latexinline'>12 = 2^2\times 3 = (8, 12]</span></li><li> <span class='latexinline'>13 = 13 = (12, 13]</span></li><li> <span class='latexinline'>14 = 2\times 7 = (12, 14]</span></li><li> <span class='latexinline'>15 = 15 = (14, 15]</span></li><li> <span class='latexinline'>16 = 2^4 = (0, 16]</span></li></ul>
<img width=300 src="./static/fenwick-structure.gif">
<h4><a id=query href='#query'> § </a> query</h4>
To perform a cumulative sum, we need to read from the correct overlap regions
that cover the full array. For example, to read from <span class='latexinline'>15</span>, we would want
to read:
<ul><li> <span class='latexinline'>a[15] = (14, 15], a[14] = (12, 14], a[12] = (8, 12], a[8] = (0, 8]</span>.</li></ul>
So we need to read the indices:
<ul><li> <span class='latexinline'>15=2^0 \cdot 15 \xrightarrow{-2^0} 14=2^1 \cdot 7 \xrightarrow{-2^1} 12=2^2\cdot3 \xrightarrow{-2^2} 8=2^3\cdot1 \xrightarrow{-2^3} 0</span></li></ul>
At each  location, we strip off the value <span class='latexinline'>2^r</span>. We can discover this value
with bit-fiddling: We claim that <span class='latexinline'>a \& (-a) = 2^r</span>.
Let <span class='latexinline'>a = x 1 0^r</span>. Now, <span class='latexinline'>-a = \lnot a + 1 = x01^r + 1 = \overline{x}10^r</span>.
Hence, <span class='latexinline'>a \& (-a) = a \& (\lnot a + 1) = (x 10^r) \& (\overline{\x}10^r) = 0^{|\alpha|}10^r = 2^r</span>
So the full implementation of query is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=update href='#update'> § </a> update</h4>
To perform an update at <span class='latexinline'>i</span>, we need to update all locations which on querying
overlap with <span class='latexinline'>i</span>. For example, to update the location <span class='latexinline'>9</span>, we would want to
update:
<ul><li> <span class='latexinline'>a[9] = (8, 9], a[10] = (8, 10], a[12] = (8, 12], a[16] = (0, 16]</span>.</li></ul>
So we need to update the indices:
<ul><li> <span class='latexinline'>9=2^0 \cdot 9 \xrightarrow{+2^0} 10=2^1 \cdot 5 \xrightarrow{+2^1} 12=2^2\cdot3 \xrightarrow{+2^2} 16=2^4\cdot1 \xrightarrow{+2^4} \dots</span></li></ul>
We use the same bit-fiddling technique as above to strip off the value <span class='latexinline'>2^r</span>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=correctness href='#correctness'> § </a> correctness</h4>
We wish to analyze the operations <span class='latexinline'>Query(q) \equiv \sum_{i=1}^q a[i]</span>, and
<span class='latexinline'>Update(i, val) \equiv a[i]~\texttt{+=}~val</span>. To do this, we are allowed to maintain
an auxiliary array <span class='latexinline'>d</span> which we will manipuate. We will stipulate the
conditions of operations on <span class='latexinline'>d</span> such that they will reflect the values of
<span class='latexinline'>Query</span> and <span class='latexinline'>Update</span>, albeit much faster.
We will analyze the algorithm in terms of orbits. We have two operators, one
for update called <span class='latexinline'>U</span>, and one for query called <span class='latexinline'>Q</span>. Given an index <span class='latexinline'>i</span>,
repeatedly applying the query operator gives us the indeces we need to read and
accumulate from the underlying array <span class='latexinline'>a</span> to get the total sum <span class='latexinline'>a[0..i]</span>:
<ul><li> <span class='latexinline'>Query(i) = \sum_i d[Q^i(q)]</span></li></ul>
Given an index <span class='latexinline'>u</span>, repeatedly applying the update operator <span class='latexinline'>U</span> gives us all
the indeces we need to add the change to update:
<ul><li> <span class='latexinline'>Update(i, val) = \forall j~, d[U^j(i)]~\texttt{+=}~ val</span></li></ul>
For query and update to work, we need the condition that:
<ul><li> <span class='latexinline'>q \geq u \iff \left\vert \{ Q^i(q)~:~ i \in \mathbb{N} \} \cap \{ U^i(u)~:~i \in \mathbb{N} \} \right\vert = 1</span></li></ul>
That is, if and only if the query index <span class='latexinline'>q</span> includes the update location <span class='latexinline'>u</span>,
will the orbits intersect.
The intuition is that we want updates at an index <span class='latexinline'>u</span> to only affect queries
that occur at indeces <span class='latexinline'>q \geq u</span>. Hence, we axiomatise that for an update
to be legal, it must the orbits of queries that are at indeces greater than it.
We will show that our operators:
<ul><li> <span class='latexinline'>Q(i=2^r\cdot a) = i - 2^r = 2^r(a-1)</span></li><li> <span class='latexinline'>U(j=2^s\cdot b) = j + 2^{s} = 2^{s}(b+1)</span></li></ul>
do satisfy the conditions above.
For a quick numerical check, we can use the code blow to ensure
that the orbits are indeed disjoint:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=case-1-q-u href='#case-1-q-u'> § </a> Case 1: <span class='latexinline'>q = u</span></h4>
We note that <span class='latexinline'>Q</span> always decreases the value of <span class='latexinline'>q</span>, and <span class='latexinline'>u</span> always increases
it. Hence, if <span class='latexinline'>q = u</span>, they meet at this point, and
<span class='latexinline'>\forall i, j \geq 1, \quad Q^i (q) \neq U^j(u)</span>.
Hence, they meet exactly once as required.
<h4><a id=case-2-q-u href='#case-2-q-u'> § </a> Case 2: <span class='latexinline'>q < u</span></h4>
As noted above, <span class='latexinline'>q</span> always decreases and <span class='latexinline'>u</span> always increases, hence in this
case they will never meet as required.
<h4><a id=case-3-q-u href='#case-3-q-u'> § </a> Case 3: <span class='latexinline'>q > u</span></h4>
Let the entire array have size <span class='latexinline'>2^N</span>.
Let <span class='latexinline'>q = \texttt{e1 f_q}, u = \texttt{e0 f_u}</span>, where
<span class='latexinline'>\texttt{e, f_q, f_u}</span> may be empty strings.
Notice that <span class='latexinline'>Q</span> will always strip away rightmost ones in <span class='latexinline'>f_q</span>,
leading to <span class='latexinline'>q = \texttt{e10...0}</span> at some point.
Similarly, <span class='latexinline'>U</span> will keep on adding new rightmost ones, causing the
state to be <span class='latexinline'>u = \texttt{e01...10...0} \xrightarrow{U} \texttt{e100...}</span>.
Hence, at some point <span class='latexinline'>q = u</span>.
<h4><a id=references href='#references'> § </a> References</h4>
<ul><li> <a href=http://michaelnielsen.org/polymath1/index.php?title=Updating_partial_sums_with_Fenwick_tree>Fenwick trees on PolyMath</a></li><li> <a href=https://doc.lagout.org/security/Hackers%20Delight.pdf>Hacker's delight</a></li></ul>
<h3><a id=dirichlet-inversion href='#dirichlet-inversion'> § </a> <a href=#Dirichlet-inversion>Dirichlet inversion</a></h3>
We call all functions <span class='latexinline'>f: \mathbb Z \rightarrow \mathbb R</span> as
<i>arithmetic functions</i>, since they operate on the integers.
We introduce an operator <span class='latexinline'>f \star g: \mathbb Z \rightarrow \mathbb R</span>.
It is defined by:
<ul><li> <span class='latexinline'>(f \star g)(n) \equiv \sum_{d \vert n} f(d) g(n/d)</span></li></ul>
We will show that the set of arithmetic functions forms a group
under the operator <span class='latexinline'>\star</span>, with identity:
<div class='latexblock'>id_\star(n) \equiv \floor{1/n} = \begin{cases} 1 & n = 1 \\ 0 & \text{otherwise} \end{cases}</div>
The reason all of this is interesting is that the inverse of the constant function <span class='latexinline'>1(n) \equiv 1</span>
is going to be this function called as the mobius function <span class='latexinline'>\mu</span>:
<div class='latexblock'>
\mu(n=p_1^\alpha_1 p_2^\alpha_2 \dots p_r^\alpha_r) \equiv
\begin{cases}
  0 & \text{if any $\alpha_i > 1$} \\
  (-1)^{\alpha_1 + \alpha_2 + \dots + \alpha_r} & \text{if all $\alpha_i \in \{ 0, 1 \}$}
\end{cases}
</div>
The mobius function will allow us to perform <i>mobius inversion</i>:
<div class='latexblock'>
\begin{align*}
  f(n) &\equiv \sum_{d \vert n} g(d) = \sum_{d \vert n} g(d) 1(n/d) = g \star 1 \\
  f \star 1^{-1} &=  g \star 1 \star 1^{-1} \\
  f \star \mu &= g
\end{align*}
</div>
That is, we originally had <span class='latexinline'>f</span> defined in terms of <span class='latexinline'>g</span>. We can
recover an expression for <span class='latexinline'>g</span> in terms of <span class='latexinline'>f</span>.
<h4><a id=the-algebra-of-multiplicative-functions href='#the-algebra-of-multiplicative-functions'> § </a> The algebra of multiplicative functions</h4>
We claim that the set of functions <span class='latexinline'>\{ \mathbb Z \rightarrow \mathbb C \}</span>
is a commutative group, with the group operation <span class='latexinline'>\star</span> such that:
<ul><li> <span class='latexinline'>(f \star g)(n) \equiv \sum_{d \vert n} f(d) g(n/d)</span>.</li></ul>
with the identity element being <span class='latexinline'>id_\star(n) \equiv \lfloor 1 / n \rfloor</span>. The idea
is that if <span class='latexinline'>(n = 1)</span>, then <span class='latexinline'>\lfloor 1/1 \rfloor = 1</span>, and for any other
number <span class='latexinline'>n > 0</span>, <span class='latexinline'>1/n < 1</span>, hence <span class='latexinline'>\lfloor 1/n \rfloor = 0</span>.
<h4><a id=verification-of-istar-being-the-identity href='#verification-of-istar-being-the-identity'> § </a> verification of <span class='latexinline'>istar</span> being the identity</h4>
<div class='latexblock'>
\begin{align*}
&(f \star id_\star)(n) \equiv \sum_{d \vert n} f(d) id_\star(n/d) \\
&= f(n) id_\star(1) + \sum_{d \vert n, d > 1} f(n) id_\star(d) \\
&= f(n) \cdot 1 + \sum_{d \vert n, d > 1} f(n) \cdot 0 \\
&= f(n) \\
\end{align*}
</div>
<h4><a id=associativity-commutativity-of-star href='#associativity-commutativity-of-star'> § </a> associativity, commutativity of <span class='latexinline'>\star</span></h4>
To prove associativity, it's better to write the formula as:
<div class='latexblock'>
(f \star g)(n) \equiv \sum_{d \vert n} f(n) g(n/d) = \sum_{xy = n} f(x) g(y)
</div>
From this rewrite, it's clear that <span class='latexinline'>(f \star g \star h)(n)</span> will unambiguously
sum over tripes <span class='latexinline'>(x, y, z)</span> such that <span class='latexinline'>xyz = n</span>. I leave the working-this-out
to you. This should also make the commutativity immediate. Summing over pairs
of the form <span class='latexinline'>f(x) g(y) : xy = n</span> is the same as summing over <span class='latexinline'>f(y) g(x) : yx = n</span>.
<h4><a id=existence-of-inverse href='#existence-of-inverse'> § </a> Existence of inverse</h4>
We can show that an inverse exists by showing that a formula for it exists; The 
idea is to construct one by induction. 
Clearly, for a given function <span class='latexinline'>f</span>, we need the inverse <span class='latexinline'>f^{-1}</span> to be such that
<span class='latexinline'>(f \star f^{-1})(n) = id_\star</span>. Hence:
<div class='latexblock'>
\begin{align*}
&(f \star f^{-1})(1) = id_\star(1) = 1 \\
&f(1) f^{-1}(1) = 1 \\
& f^{-1}(1) \equiv 1 / f(1)\\
\end{align*}
</div>
Great, we have a base case; We can now compute <span class='latexinline'>f^{-1}(n)</span> inductively, assuming
we know the value of <span class='latexinline'>f^{-1}(d)</span> for all <span class='latexinline'>d \vert n</span>.
<div class='latexblock'>
\begin{align*}
&(f \star f^{-1})(n) = id_\star(1) = 0 \\
&\sum_{d \vert n} f(d) f^{-1}(n/d) = 0 \\
&f(1) f^{-1}(n) + \sum_{d \vert n, d < n} f(d) f^{-1}(n/d) = 0 \\
&f^{-1}(n) = -\frac{\sum_{d \vert n, d < n} f(d) f^{-1}(n/d)}{f(1)} 
\end{align*}
</div>
<h4><a id=mu-as-the-inverse-of-the-one-function href='#mu-as-the-inverse-of-the-one-function'> § </a> <span class='latexinline'>\mu</span> as the inverse of the <span class='latexinline'>one</span> function</h4>
<h4><a id=mobius-inversion href='#mobius-inversion'> § </a> Mobius inversion</h4>
Now that we know that <span class='latexinline'>\mu = \texttt{const 1}^{-1}</span>, we can use this fact
to perform <i>mobius inversion</i>:
<div class='latexblock'>
f(n) \equiv \sum_{d \vert n} g(n/d) = \texttt{const 1} \star g
</div>
We have <span class='latexinline'>f</span> written in terms of <span class='latexinline'>g</span>. We can now write <span class='latexinline'>g</span> in terms of <span class='latexinline'>f</span>:
<div class='latexblock'>
\begin{align*}
&f(n)  = \texttt{const 1} \star g \\
&f \star \texttt{const 1}^{-1} = g \\
&g = f \star \mu \\
&g = \sum_{d \vert n} f(d) \mu(n/d)
\end{align*}
</div>
<h4><a id=n-sumd-vert-n-phid href='#n-sumd-vert-n-phid'> § </a> <span class='latexinline'>n = \sum_{d \vert n} \phi(d)</span></h4>
<div class='latexblock'>
\begin{array}{|c|c|c|c|}
d & \{ 1 \leq x \leq 12 : (x, 12) = d \} & \{ 1 \leq x \leq 12: (x/d, 12/d) = 1\} & \text{size of set} \\
1 & \{ 1, 5, 7, 11 \} & (x, 12) = 1 & 4 \\
2 & \{2, 10 \} & (x/2, 6) = 1& 2 \\
3 & \{3, 9 \} & (x/3, 4) = 1 & 2 \\
4 & \{4, 8 \} & (x/4, 3) = 1 & 2 \\
6 & \{ 6 \} & (x/6, 2) = 1 & 1 \\
12 & \{ 12 \} (x/12, 1) = 1 & 1
\end{array}
</div>
Notice that the sizes of sets that we are calculating, for example,
<span class='latexinline'>|\{ 1 \leq x \leq 12 : (x/2, 6) = 1 \}| =  \phi(6)</span>. Summing over all of
what we have, we've counted the numbers in <span class='latexinline'>[1, 2, \dots, 12]</span> in two ways ---
one directly, and the other by partitioning into equivalence classes:
<div class='latexblock'> 12 = \phi(1) + \phi(2) + \phi(3) + \phi(4) + \phi(6) + \phi(12) = \sum_{d \vert 12} \phi(12/d) </div>
In general, the same argument allows us to prove that:
<div class='latexblock'> n = \sum_{d \vert n} n/d </div>
<h4><a id=using-mobius-inversion-on-the-euler-totient-function href='#using-mobius-inversion-on-the-euler-totient-function'> § </a> Using mobius inversion on the euler totient function</h4>
<h4><a id=other-arithmetical-functions-and-their-relations href='#other-arithmetical-functions-and-their-relations'> § </a> Other arithmetical functions and their relations</h4>
<h3><a id=incunabulum-for-the-21st-century-making-the-j-interpreter-compile-in-2020 href='#incunabulum-for-the-21st-century-making-the-j-interpreter-compile-in-2020'> § </a> <a href=#incunabulum-for-the-21st-century-making-the-j-interpreter-compile-in-2020>Incunabulum for the 21st century: Making the J interpreter compile in 2020</a></h3>
This is me trying to understand the fabled interpreter of the <code>J</code> language
working, so I could absorb Arthur Whitney's style of writing C: it's
cryptic, short, and fits in a page. <a href=https://code.jsoftware.com/wiki/Essays/Incunabulum>I learnt of this from the <code>J</code> language page</a>,
which comes with the quote:
<blockquote> One summer weekend in 1989, Arthur Whitney visited Ken Iverson at Kiln Farm and produced—on one page and in one afternoon—an interpreter fragment on the AT&T 3B1 computer. I studied this interpreter for about a week for its organization and programming style; and on Sunday, August 27, 1989, at about four o'clock in the afternoon, wrote the first line of code that became the implementation described in this document. Arthur's one-page interpreter fragment is as follows:</blockquote>
<h4><a id=the-original-source href='#the-original-source'> § </a> The original source</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
It's a lot to take in --- it's quite breathtaking really, the way it all
hangs together in one page.
<h4><a id=the-attempt-to-get-it-run href='#the-attempt-to-get-it-run'> § </a> The attempt to get it run</h4>
Unfortunately, this does not work if we try to get it to run in 2020. I decided
to read the code and understand what would happen. I managed to read enough
to understand that the code <code>a=~3</code> ought to create an array with values <code>[0 1 2]</code>.
On attempting to <i>run</i> this however, we get:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
oops! The code uses a lot of punning between <code>int</code>
and <code>int*</code>. These assumptions break now that we're in 64-bit. The patch to
get this to work is:
<h4><a id=the-patch href='#the-patch'> § </a> the patch</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=it-runs href='#it-runs'> § </a> It runs!</h4>
After applying the patch, we manage to get the interpreter to run!
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=the-lock-screen href='#the-lock-screen'> § </a> The lock screen</h4>
I liked it so much that I took a screenshot and made it my lock screen.
<img width=500 src="./static/screenshot-j-incunabulum.png">
<h4><a id=thoughts href='#thoughts'> § </a> Thoughts</h4>
I'm really fascinated by the code. I loved I could simply stare the screenshot
to absorb the code. There was no scrolling involved.  The variables are
well-named (to the extent I understand the code), and it's clearly extremely
well thought out. If there's someone who understands some of the thorny
aspects of the code:
<ul><li> What is the <code>t</code> variable really tracking?</li><li> How does one create a multi-dimensional array easily?</li><li> What are some interesting programs one can run with this mini-interpreter?</li></ul>
I'd be really glad to know the details. Please leave 
<a href=https://github.com/bollu/bollu.github.io/issues/new>an issue or a pull request against the repo</a>.
I'm going write a dissection of the code once I fully understand it, since I
couldn't find explanaing the code on the internet.
Until then, enjoy the monolith of code!
<h3><a id=an-example-of-a-sequence-whose-successive-terms-get-closer-together-but-isnt-cauchy-does-not-converge href='#an-example-of-a-sequence-whose-successive-terms-get-closer-together-but-isnt-cauchy-does-not-converge'> § </a> <a href=#an-example-of-a-sequence-whose-successive-terms-get-closer-together-but-isnt-cauchy-does-not-converge>An example of a sequence whose successive terms get closer together but isn't Cauchy (does not converge)</a></h3>
<h4><a id=the-problem href='#the-problem'> § </a> The problem</h4>
Provide an example of a sequence <span class='latexinline'>a_n: \mathbb N \rightarrow \mathbb R</span>
such that <span class='latexinline'>\lim_{n \rightarrow \infty} \vert a_{n+1} - a_n \vert \rightarrow 0</span>,
but <span class='latexinline'>\lim_{n, m \rightarrow \infty, m > n} |a_n - a_m| \neq 0</span>. That is,
proide a series where the distances between successive terms converges to zero,
but where distances between terms that are "farther apart than 1" does
not converge to 0. That is, the sequence is not <i>Cauchy</i>.
<h4><a id=regular-solution-harmonic-numbers href='#regular-solution-harmonic-numbers'> § </a> Regular solution: Harmonic numbers</h4>
The usual solution is to take the harmonic numbers,
<span class='latexinline'>H_n \equiv \sum_{i=1}^n 1/i</span>. Then, we show that:
<div class='latexblock'>
\begin{align*}
\lim_{n \rightarrow \infty} \left| H_{n+1} - H_n \right|
&= \left| \frac{1}{n+1} - \frac{1}{n} \right| \\
&= \frac{1}{(n+1)n} \rightarrow 0
\end{align*}
</div>
<div class='latexblock'>
\begin{align*}
&\lim_{n \rightarrow \infty} \left| H_{2n} - H_n \right| \\
&= \left|\frac{1}{2n} - \frac{1}{n} \right| \\
&= \sum_{i=n+1}^{2n} \frac{1}{n+1} + \frac{1}{n+2} + \dots + \frac{1}{2n} \\
&\geq \sum_{i=n+1}^{2n} \frac{1}{2n} + \frac{1}{2n} + \dots + \frac{1}{2n} \\
&\geq \frac{n}{2n} = \frac{1}{2} \neq 0 \text{on} x \rightarrow \infty
\end{align*}
</div>
<h4><a id=memorable-solution-logarithm href='#memorable-solution-logarithm'> § </a> Memorable solution: logarithm</h4>
We can much more simply choose <span class='latexinline'>a_n = \log(n)</span>. This yields the simple
calculation:
<div class='latexblock'>
\begin{align*}
&\lim_{n \rightarrow \infty} a_{n+1} - a_n = \log(n+1) - \log(n) \\
&= \log((n+1)/n)) \\
&= \log(1 + 1/n) \xrightarrow{n \rightarrow \infty} \log(1) = 0
\end{align*}
</div>
while on the other hand,
<div class='latexblock'>
\begin{align*}
\lim_{n \rightarrow \infty} a_{2n} - a_n 
= \log(2n) - \log(n)
= \log(2) + \log(n) - \log(n)
= \log 2 \neq 0
\end{align*}
</div>
I find this far cleaner conceptually, since it's "obvious" to everyone
that <span class='latexinline'>a_n = \log(n)</span> diverges, while the corresponding fact for <span class='latexinline'>H_n</span>
is hardly convincing. We also get straightforward equalities everywhere,
instead of inequalities.
I still feel that I don't grok what precisely fails here, in that, my intuition
still feels that the local condition <i>ought</i> to imply the Cauchy condition:
if <span class='latexinline'>a_n</span> tells <span class='latexinline'>a_{n+1}</span> to not be too far, and <span class='latexinline'>a_{n+1}</span> tells <span class='latexinline'>a_{n+2}</span>,
surely this <i>must</i> be transitive?
I have taught my instincts to not trust my instincts on analysis, which is a
shitty solution :) I hope to internalize this someday.
<b>EDIT:</b> I feel I now understand what's precisely happening
after ruminating a bit.
The Cauchy convergence criterion allows us to drop a finite number
of terms, and then capture <i>everything after that point</i> in a ball
of radius <span class='latexinline'>\epsilon</span>. As <span class='latexinline'>\epsilon</span> shrinks, <i>all</i> the terms in the
sequence are "squeezed togeher".
In the <span class='latexinline'>a_{n+1} - a_n</span> case, only successive terms must maintain
an <span class='latexinline'>\epsilon</span> distance. But as the <span class='latexinline'>\log</span> example shows, you can steadily
plod along, keeping <span class='latexinline'>\epsilon</span> ball next to <span class='latexinline'>\epsilon</span> ball, to reach:
<div class='latexblock'>
\lim_{n \rightarrow \infty} \lim_{\epsilon \rightarrow 0} f(n) \cdot \epsilon
</div>
whose behaviour can do unexpected things depending on the choice of <span class='latexinline'>\n</span>.
<h3><a id=krylov-subspace-method href='#krylov-subspace-method'> § </a> <a href=#krylov-subspace-method>Krylov subspace method</a></h3>

This is a class of methods used to solve <span class='latexinline'>Ax = b</span>, where <span class='latexinline'>A</span> is sparse.
Krylov subspace methods are a class of methods which use the idea of a
Krylov subspace. There is conjugate gradient (CG), GMRES (Generalized minimal
residual method).
<div class='latexblock'>
K_m(A, v) \equiv span \{ v, Av, A^2v, \dots, A^m v\}
</div>
Clearly, <span class='latexinline'>K_m \subseteq K_{m+1}</span>, and there is a maximum <span class='latexinline'>K_N</span> that we can span
(the full vector space). We are interested in the smallest index <span class='latexinline'>M</span>
such that <span class='latexinline'>K_M = K_{M+1}</span>.
We notice that <span class='latexinline'>K_M</span> is invariant under the action of <span class='latexinline'>A</span>.
Now, let's consider:
<div class='latexblock'>
\begin{align*}
K_m(A, x) &\equiv span \{x, Ax, A^2x, \dots A^m x \} \\
        &= span \{ A^{-1} b, b, Ab, \dots A^{m-1} x \} \qquad \text{(substitute $x = A^{-1}b$)} \\
        &= A span \{ A^{-1} b, b, Ab, \dots A^{m-1} b\} \qquad \text{(Invariance of Krylov subspace)} \\
        &= span \{b, Ab, \dots A^m b\}  \\
        &= K_m(A, b)
\end{align*}
</div>
We learnt that <span class='latexinline'>Ax = b</span> has a solution in <span class='latexinline'>K_m(A, b)</span>. Using this, we can build
solvers that exploit the Krylov subspace. We will describe GMRES and CG.
<h3><a id=generalized-minimal-residual--gmres href='#generalized-minimal-residual--gmres'> § </a> Generalized minimal residual --- GMRES</h3>
We wish to solve <span class='latexinline'>Ax = b</span> where <span class='latexinline'>A</span> is sparse and <span class='latexinline'>b</span> is normalized. The <span class='latexinline'>n</span>th
Krylov subspace is <span class='latexinline'>K_n(A, b) \equiv span~\{b, Ab, A^2b, \dots, A^nb  \}</span>.
We approximate the actual solution with a vector <span class='latexinline'>x_n \in K_n(A, b)</span>. We
define the <i>residual</i> as <span class='latexinline'>r_n \equiv A x_n - b</span>.
<h3><a id=conjugate-gradient-descent href='#conjugate-gradient-descent'> § </a> Conjugate gradient descent</h3>
<h3><a id=good-reference-to-the-rete-pattern-matching-algorithm href='#good-reference-to-the-rete-pattern-matching-algorithm'> § </a> <a href=#good-reference-to-the-rete-pattern-matching-algorithm>Good reference to the Rete pattern matching algorithm</a></h3>
The <a href=https://en.wikipedia.org/wiki/Rete_algorithm>Rete pattern matching algorithm</a>
is an algorithm that allows matching a huge number of rules with a huge database
of "facts".
MLIR ("multi-language intermediate representation") is a new technology that
hopes to centralize much of the research and development of various compilers
into a single authoritative source. The major claim-to-fame is that it allows
one to mix various "dialects" (much as Racket does). So, to a first order
approximation, MLIR is "JSON for compiler intermediate representations".
What MLIR gets right is <i>tooling</i>. They take the experience that the LLVM project
has mined for the last decade and a half, and bake many of the good stuff that
came with LLVM right in. For example, MLIR comes in-built with a pretty printer,
a notion of types, a notion of "attributes", SSA, enforced provenance
tracking of code (so one can <i>always</i> know what the original source code was
that led to some assembly). Sound engineering might see MLIR succeed where
many others failed.
I was reminded of Rete since the MLIR folks are trying to solve the pattern
matching problem in general for their <a href=https://mlir.llvm.org/docs/GenericDAGRewriter/>Generic DAG Rewriter</a>.
They currently just use a worklist based algorithm. I'm trying to understand
if Rete can be used instead. Rete is famous for being hard to understand,
so I began a quest to look for good sources to implement it. I found a great
<a href=http://reports-archive.adm.cs.cmu.edu/anon/1995/CMU-CS-95-113.pdf>PhD thesis written by Robert B. Doorenboos</a>,
which quips:
<blockquote> Since the Rete match algorithm provides the starting point for much of the work in this thesis, this chapter describes Rete. Unfortunately, most of the descriptions of Rete in the literature are not particularly lucid,1 which is perhaps why Rete has acquired \a reputation for extreme differentialculty."(Perlin, 1990b) To remedy this situation, this chapter describes Rete in a tutorial style, rather than just briey reviewing it and referring the reader to the literature for a full description. We will first give an overview of Rete, and then discuss the principle data structures and procedures commonly used to implement it. High-level pseudocode will be given for many of the structures and procedures, so that this chapter may serve as a guide to readers who want to implement rete (or some variant) in their own systems.</blockquote>
I now have a reference to an accessible description of this stuff. I might
implement Rete to understand it, so that it's part of my toolkit if I ever
need it.
<h3><a id=leapfrog-integration href='#leapfrog-integration'> § </a> <a href=#leapfrog-integration>Leapfrog Integration</a></h3>
We have a system we wish to simulate using hamilton's equations:
<div class='latexblock'>
\begin{align*}
\frac{\partial q}{\partial t} = \frac{\partial H}{\partial p}|_{(p_0, q_0)} \\
\frac{\partial p}{\partial t} = -\frac{\partial H}{\partial q}|_{(p_0, q_0)} \\
\end{align*}
</div>
We want to simulate a system using these differential equations. We will begin
with some initial position and momentum <span class='latexinline'>(q_0, p_0)</span>, evaluate
<span class='latexinline'>\frac{\partial q}{\partial t} \rvert_{(q_0, p_0)}</span>, <span class='latexinline'>\frac{\partial p}{\partial t} \rvert_{(q_0, p_0)}</span>, and use
these to find <span class='latexinline'>(q_{next}, p_{next})</span>. An integrator is a general algorithm
that produces the next position and momentum using current information:
<div class='latexblock'>
(q_{next}, p_{next}) =
  I \left(q_0,
    p_0,
    \frac{\partial q}{\partial t}\rvert_{(q_0, p_0)},
    \frac{\partial p}{\partial t}\rvert_{(q_0, p_0)} \right)
</div>
The design of <span class='latexinline'>I</span> is crucial: different choices of <span class='latexinline'>I</span> will have different
trade-offs for accuracy and performance. Another interesting property
we might want is for <span class='latexinline'>I</span> to be a <i>symplectic integrator</i> --- that is,
it preserves the total energy of the system. For example, here's a plot
of the orbits of planets using two integrators, one that's symplectic (leapfrog)
and one that isn't (Euler)
<img width=400  src="./static/leapfrog-vs-euler.png">
Notice that since leapfrog attempts to keep energy conserved, the orbits stay
as orbits! On the other hand, the euler integrator quickly spirals out, since
we lose energy during the integration. Note that this is 
<i>not an issue of numerical precision</i>: The euler integrator is ineherently 
such that over long timescales, it will lose energy. On the other hand, the
leapfrog integrator will <i>always remain stable</i>, even with very large timesteps
and low precision.
I present the equations of the leapfrog integrator, a proof sketch that it
is symplectic, and the code listing that was used to generate the above plot.
Often, code makes most ideas very clear!
<h4><a id=the-integrator href='#the-integrator'> § </a> The integrator</h4>
<h4><a id=code-listing href='#code-listing'> § </a> Code listing</h4>
<h4><a id=incantations href='#incantations'> § </a> Incantations</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
For reference, we also implement an euler integrator, that uses the derivative
to compute the position and momentum of the next timestep independently.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Finally, we implement <code>planet(integrator, n, dt)</code> which simulates gravitational
potential and usual kinetic energy, using the integrator given by <code>integrator</code>
for <code>n</code> steps, with each timestep taking <code>dt</code>.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
We plot the simulations using <code>matplotlib</code> and save them.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h3><a id=comparison-of-forward-and-reverse-mode-ad href='#comparison-of-forward-and-reverse-mode-ad'> § </a> <a href=#comparison-of-forward-and-reverse-mode-ad>Comparison of forward and reverse mode AD</a></h3>
Quite a lot of ink has been spilt on this topic. My favourite reference
is the one by <a href=https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation>Rufflewind</a>.
However, none of these examples have a good stock of examples for the diference.
So here, I catalogue the explicit computations between computing forward
mode AD and reverse mode AD.
In general, in forward mode AD, we fix how much the inputs wiggle with
respect to a parameter <span class='latexinline'>t</span>. We figure out how much the output wiggles
with respect to <span class='latexinline'>t</span>. If <span class='latexinline'>output = f(input_1, input_2, \dots input_n)</span>,
then <span class='latexinline'>\frac{\partial output}{\partial t} = \sum_i \frac{\partial f}{\partial input_i} \frac{\partial input_i}{\partial dt}</span>.
In reverse mode AD, we fix how much the parameter <span class='latexinline'>t</span> wiggles with
respect to the output. We figure out how much the parameter <span class='latexinline'>t</span>
wiggles with respect to the inputs.
If <span class='latexinline'>output_i = f_i(input, \dots)</span>, then <span class='latexinline'>\frac{\partial t}{\partial input} = \sum_i \frac{\partial t}{\partial output_i} \frac{\partial f_i}{input}</span>.
This is a much messier expression, since we need to accumulate the data
over all outputs.
Essentially, deriving output from input is easy, since how to compute an output
from an input is documented in one place. deriving input from output is
annoying, since many outputs can depent on a single output.
The upshot is that if we have few "root outputs" (like a loss function),
we need to run AD once with respect to this, and we will get the wiggles
of <i>all inputs</i> at the same time with respect to this output, since we
compute the wiggles output to input.
The first example of <code>z = max(x, y)</code> captures the essential difference
between the two approached succinctly. Study this, and everything else will make
sense.
<h4><a id=maximum-z-maxx-y href='#maximum-z-maxx-y'> § </a> Maximum: <code>z = max(x, y)</code></h4>
<ul><li> Forward mode equations:</li></ul>
<div class='latexblock'>
\begin{align*}
z &= max(x, y) \\
\frac{\partial x}{\partial t} &= ? \\
\frac{\partial y}{\partial t} &= ? \\
\frac{\partial z}{\partial t}
  &= \begin{cases}
        \frac{\partial x}{\partial t} & \text{if $x > y$} \\
        \frac{\partial y}{\partial t} & \text{otherwise} \\
    \end{cases}
\end{align*}
</div>
We can compute <span class='latexinline'>\frac{\partial z}{\partial x}</span> by setting <span class='latexinline'>t = x</span>.
That is, <span class='latexinline'>\frac{\partial x}{\partial t} = 1, \frac{\partial y}{\partial t} = 0</span>.
Similarly, can compute <span class='latexinline'>\frac{\partial z}{\partial y}</span> by setting <span class='latexinline'>t = y</span>.
That is, <span class='latexinline'>\frac{\partial x}{\partial t} = 1, \frac{\partial y}{\partial t} = 0</span>.
If we want both gradients <span class='latexinline'>\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}</span>,
we will have to <b>rerun the above equations twice</b> with the two initializations.
In our equations, we are saying that we know how sensitive
the inputs <span class='latexinline'>x, y</span> are to a given parameter <span class='latexinline'>t</span>. We are deriving how sensitive
the output <span class='latexinline'>z</span> is to the parameter <span class='latexinline'>t</span> as a composition of <span class='latexinline'>x, y</span>. If
<span class='latexinline'>x > y</span>, then we know that <span class='latexinline'>z</span> is as sensitive to <span class='latexinline'>t</span> as <span class='latexinline'>x</span> is.
<ul><li> Reverse mode equations:</li></ul>
<div class='latexblock'>
\begin{align*}
z &= max(x, y) \\
\frac{\partial t}{\partial z} &= ? \\
\frac{\partial t}{\partial x}
  &= \begin{cases}
    \frac{\partial t}{\partial z} & \text{$if x > y$} \\
    0 & \text{otherwise}
  \end{cases} \\
\frac{\partial t}{\partial y}
  &= \begin{cases}
    \frac{\partial t}{\partial z} & \text{$if y > x$} \\
    0 & \text{otherwise}
  \end{cases}
\end{align*}
</div>
We can compute <span class='latexinline'>\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}</span>
<b>in one shot</b> by setting <span class='latexinline'>t = z</span>. That is, <span class='latexinline'>\frac{\partial z}{\partial t} = 1</span>.
In our equations, we are saying that we know how sensitive
the parameter <span class='latexinline'>t</span> is to a given output <span class='latexinline'>z</span>. We are trying to see
how sensitive <span class='latexinline'>t</span> is to the inputs <span class='latexinline'>x, y</span>. If <span class='latexinline'>x</span> is active (ie, <span class='latexinline'>x > y</span>),
then <span class='latexinline'>t</span> is indeed sensitive to <span class='latexinline'>x</span> and <span class='latexinline'>\frac{\partial t}{\partial x} = 1</span>.
Otherwise, it is not sensitive, and <span class='latexinline'>\frac{\partial t}{\partial x} = 0</span>.
<h4><a id=sin-z-sinx href='#sin-z-sinx'> § </a> sin: <code>z = sin(x)</code></h4>
<ul><li> Forward mode equations:</li></ul>
<div class='latexblock'>
\begin{align*}
z &= sin(x) \\
\frac{\partial x}{\partial t} &= ? \\
\frac{\partial z}{\partial t}
  &= \frac{\partial z}{\partial x} \frac{\partial x}{\partial t} \\
  &= cos(x) \frac{\partial x}{\partial t}
\end{align*}
</div>
We can compute <span class='latexinline'>\frac{\partial z}{\partial x}</span> by setting <span class='latexinline'>t = x</span>.
That is, setting <span class='latexinline'>\frac{\partial x}{\partial t} = 1</span>.
<ul><li> Reverse mode equations:</li></ul>
<div class='latexblock'>
\begin{align*}
z &= sin(x) \\
\frac{\partial t}{\partial z} &= ? \\
\frac{\partial t}{\partial x}
  &= \frac{\partial t}{\partial z} \frac{\partial z}{\partial x} \\
  &= \frac{\partial t}{\partial z} cos(x)
\end{align*}
</div>
We can compute <span class='latexinline'>\frac{\partial z}{\partial x}</span> by setting <span class='latexinline'>t = z</span>.
That is, setting <span class='latexinline'>\frac{\partial z}{\partial t} = 1</span>.
<h4><a id=addition-z-x-y href='#addition-z-x-y'> § </a> addition: <code>z = x + y</code>:</h4>
<ul><li> Forward mode equations:</li></ul>
<div class='latexblock'>
\begin{align*}
z &= x + y \\
\frac{\partial x}{\partial t} &= ? \\
\frac{\partial y}{\partial t} &= ? \\
\frac{\partial z}{\partial t}
  &= \frac{\partial z}{\partial x} \frac{\partial x}{\partial t} +
    \frac{\partial z}{\partial y} \frac{\partial y}{\partial t} \\
  &= 1 \cdot \frac{\partial x}{\partial t} + 1 \cdot \frac{\partial y}{\partial t}
  = \frac{\partial x}{\partial t} + \frac{\partial y}{\partial t}
\end{align*}
</div>
<ul><li> Reverse mode equations:</li></ul>
<div class='latexblock'>
\begin{align*}
z &= x + y \\
\frac{\partial t}{\partial z} &= ? \\
\frac{\partial t}{\partial x}
  &= \frac{\partial t}{\partial z} \frac{\partial z}{\partial x} \\
  &= \frac{\partial t}{\partial z} \cdot 1 = \frac{\partial t}{\partial z} \\
\frac{\partial t}{\partial y}
  &= \frac{\partial t}{\partial z} \frac{\partial z}{\partial y} \\
  &= \frac{\partial t}{\partial z} \cdot 1 = \frac{\partial t}{\partial z}
\end{align*}
</div>
<h4><a id=multiplication-z-xy href='#multiplication-z-xy'> § </a> multiplication: <code>z = xy</code></h4>
<ul><li> Forward mode equations:</li></ul>
<div class='latexblock'>
\begin{align*}
z &= x y \\
\frac{\partial x}{\partial t} &= ? \\
\frac{\partial y}{\partial t} &= ? \\
\frac{\partial z}{\partial t}
  &= \frac{\partial z}{\partial x} \frac{\partial x}{\partial t} +
    \frac{\partial z}{\partial y} \frac{\partial y}{\partial t} \\
  &= y \frac{\partial x}{\partial t} + x \frac{\partial y}{\partial t}
\end{align*}
</div>
<ul><li> Reverse mode equations:</li></ul>
<div class='latexblock'>
\begin{align*}
z &= x y \\
\frac{\partial t}{\partial z} &= ? \\
\frac{\partial t}{\partial x}
  &= \frac{\partial t}{\partial z} \frac{\partial z}{\partial x}
  = \frac{\partial t}{\partial z} \cdot y \\
\frac{\partial t}{\partial y}
  &= \frac{\partial t}{\partial z} \frac{\partial z}{\partial y}
  = \frac{\partial t}{\partial z} \cdot x
\end{align*}
</div>
<h4><a id=subtraction-z-x--y href='#subtraction-z-x--y'> § </a> subtraction: <code>z = x - y</code>:</h4>
<ul><li> Forward mode equations:</li></ul>
<div class='latexblock'>
\begin{align*}
z &= x + y \\
\frac{\partial x}{\partial t} &= ? \\
\frac{\partial y}{\partial t} &= ? \\
\frac{\partial z}{\partial t}
  &= \frac{\partial z}{\partial x} \frac{\partial x}{\partial t} -
    \frac{\partial z}{\partial y} \frac{\partial y}{\partial t} \\
  &= 1 \cdot \frac{\partial x}{\partial t} - 1 \cdot \frac{\partial y}{\partial t}
  = \frac{\partial x}{\partial t} - \frac{\partial y}{\partial t}
\end{align*}
</div>
<ul><li> Reverse mode equations:</li></ul>
<div class='latexblock'>
\begin{align*}
z &= x - y \\
\frac{\partial t}{\partial z} &= ? \\
\frac{\partial t}{\partial x}
  &= \frac{\partial t}{\partial z} \frac{\partial z}{\partial x} \\
  &= \frac{\partial t}{\partial z} \cdot 1 = \frac{\partial t}{\partial z} \\
\frac{\partial t}{\partial y}
  &= \frac{\partial t}{\partial z} \frac{\partial z}{\partial y} \\
  &= \frac{\partial t}{\partial z} \cdot -1 = -\frac{\partial t}{\partial z}
\end{align*}
</div>
<h3><a id=an-invitation-to-homology-and-cohomology-part-1--homology href='#an-invitation-to-homology-and-cohomology-part-1--homology'> § </a> <a href=#an-invitation-to-homology-and-cohomology-part-1--homology>An invitation to homology and cohomology, Part 1 --- Homology</a></h3>
There are many introductions to homology on the internet, but none of them
really met my criteria for being simple, picture filled, and getting the
basic ideas across. I feel that Hatcher might come closest to what I want
(and where I originally learnt the material), but their description of homology
is surrounded by the context of Algebraic Topology, while really, simplicial
homology is accessible to anyone who has seen some linear algebra and group
theory. This is my attempt to get the ideas across.
Let's first try to understand what we're trying to do here. We want to detect
holes in a space, broadly construed. We focus in <i>simplicial complexes</i>, which
are collections of triangles and triangle-like objects in higher (and lower)
dimensions. We define what holes are for these simplicial complexes, and we then
try to find algebraic objects that allow us to "detect" these holes.
<h4><a id=simplices href='#simplices'> § </a> Simplices</h4>
<ul><li> A 0-simplex is a point</li></ul>
<img src="static/simplices/0-simplex.svg">
<ul><li> A 1-simplex is a line</li></ul>
<img src="static/simplices/1-simplex.svg">
<ul><li> A 2-simplex is a filled triangle</li></ul>
<img src="static/simplices/2-simplex.svg">
<ul><li> A 3-simplex is a solid tetrahedra</li></ul>
<img src="static/simplices/3-simplex.svg">
<ul><li> A <span class='latexinline'>k</span>-dimensional simplex is the convex hull of <span class='latexinline'>k+1</span>
  linearly independent points <span class='latexinline'>(u_i \in \mathbb R^{k+1})</span>
  in <span class='latexinline'>k+1</span> dimensional space.
  <span class='latexinline'> S_k \equiv \left \\{ \sum \theta_i u_i ~\mid~ \theta_i \geq 0, ~ \sum_i \theta_i = 1 \right\\} </span></li></ul>
<h4><a id=simplicial-complexes href='#simplicial-complexes'> § </a> Simplicial complexes</h4>
A simplicial complex <span class='latexinline'>K</span> is a collection of simplices where:
<ul><li> (1) Every boundary of a simplex from <span class='latexinline'>K</span> is in <span class='latexinline'>K</span></li><li> (2) The intersection of any two simplices in <span class='latexinline'>K</span> is also in <span class='latexinline'>K</span></li></ul>
Examples of simplicial complexes:
<ul><li> Every simplex is trivially a simplicial complex.</li></ul>
<img width=200 height=200 src="static/simplices/complex-0-simplices.svg">
<ul><li> A collection of points is a simplicial complex with all simplices of degree <span class='latexinline'>0</span>.</li></ul>
<img width=200 height=200 src="static/simplices/complex-unfilled-triangle.svg">
<ul><li> An unfilled triangle is a simplicial complex with simplices of degree <span class='latexinline'>0</span>, <span class='latexinline'>1</span>.</li></ul>
<img width=200 height=200 src="static/simplices/complex-unfilled-butterfly.svg">
<ul><li> Non-triangular shapes such  as this "butterfly" are also simplicial complexes,
  this one of degree <span class='latexinline'>0</span>, <span class='latexinline'>1</span>.</li></ul>
<img width=200 height=200 src="static/simplices/complex-half-filled-butterfly.svg">
<ul><li> This is the same shape as the unfilled butterly, except now containing a
  simplex of degree 2: the filling in of the bottom of the butterfly.</li></ul>
Non-examples of simplicial complexes are:
<img width=200 height=200 src="static/simplices/non-simplex-1.svg">
<ul><li> This does not contain the point at the lower-left corner, which should exist
  since it is a boundary of the straight line. This violates rule (1):
  Every boundary of a simplex from <span class='latexinline'>K</span> is in <span class='latexinline'>K</span></li></ul>
<img width=200 height=200 src="static/simplices/non-simplex-2.svg">
<ul><li> This does not contain the points which are at the intersection of the
  triangle and the line. This violates rule (2):
  The intersection of any two simplices in <span class='latexinline'>K</span> is also in <span class='latexinline'>K</span>.</li></ul>
<h4><a id=holes-in-a-space-homology-of-a-triangle href='#holes-in-a-space-homology-of-a-triangle'> § </a> Holes in a space: Homology of a triangle</h4>
Let's consider the simplest possible case of computing the homlogy, and we
do so, we will expand on what homology <i>is</i>, and what we're trying to do.
<img width=200 height=200 src="static/simplices/homology-triangle-edges.svg">
Look at the triangle above. We have the red, green, and blue vertices, which
I'll notate <span class='latexinline'>r, g, b</span>. We also have the edges that are orange (<span class='latexinline'>o</span>), cyan (<span class='latexinline'>c</span>), and
magenta (<span class='latexinline'>m</span>).
What we are interested in doing is to be able to detect the "hole" in the
triangle in between the edges <code>o-m-c</code>. That is, we want some algorithm which
when offered the representation of the triangle, can somehow detect the hole.
Note that the hole doesn't really depend on the length of the edges. We can
"bend and stretch" the triangle, and the hole will still exist. The only way
to destroy the hole is to either <i>cut</i> the triangle, or <i>fill in</i> the triangle.
We first need to agree on an abstract <i>representation</i> of the triangle,
which ideally does not change if we were to stretch out the edges,
before we can discuss how we can detect the existence of the hole.
<h4><a id=representation-of-the-triangle-boundary-operators href='#representation-of-the-triangle-boundary-operators'> § </a> Representation of the triangle: boundary operators</h4>
We first describe the shape of the triangle in terms
of two sets, <span class='latexinline'>E</span> and <span class='latexinline'>V</span> representing the edges and the vertices, and
a function <span class='latexinline'>\partial_{EV}</span>, called as the
boundary operator, which tells us how edges
are glued to vertices.
We first have a ground set of edges <span class='latexinline'>E \equiv \\{o, m, c\\}</span> and a set of
vertices <span class='latexinline'>V \equiv \\{r, g, b \\}</span>.
What we now need is to know how the edges are connected to the vertices,
since that's what really makes a triangle. We would like to say something
like "the boundary of the edge <span class='latexinline'>o</span> has points <span class='latexinline'>r, g</span>". In fact, we have
slightly more information than that: the <i>orientation</i>. So what we
really ought to be saying is "the edge <span class='latexinline'>o</span> points from <span class='latexinline'>g</span> to <span class='latexinline'>r</span>".
To do this, we create a map from <span class='latexinline'>o</span> to <span class='latexinline'>r - g</span>, where we think of <span class='latexinline'>o</span>
as a "vector", pointing from <span class='latexinline'>g</span> to <span class='latexinline'>r</span>. But hang on, what <i>is</i> <span class='latexinline'>r - g</span>?
we don't have a mathematical structure on <span class='latexinline'>V</span> that lets us add and subtract
vertices. So, we <i>create</i> a new set <span class='latexinline'>\mathcal V</span>, which represents linear
combinations of vertices in <span class='latexinline'>V</span>. Similarly, anticipating some future
development, we also <i>create</i> a new set <span class='latexinline'>\mathcal E</span> of linear combinations
of edges <span class='latexinline'>E</span>.
<h4><a id=formal-definition-of-the-boundary-operator href='#formal-definition-of-the-boundary-operator'> § </a> Formal definition of the boundary operator</h4>
We define <span class='latexinline'>\mathcal E \equiv \mathbb Z \times \mathbb Z \times \mathbb Z</span>
that represents linear combinations of edges. For example, <span class='latexinline'>(1, 2, 3) \in \mathcal E</span>
represents <span class='latexinline'>o + 2m + 3c</span> --- that is, take 1 copy of the orange edge, 2
copies of the magenta edge, and 3 copies of the cyan edge.
We define <span class='latexinline'>\mathcal V \equiv \mathbb Z \times \mathbb Z \times \mathbb Z</span>
which represents linear combinations of vertices. For example,
<span class='latexinline'>(1, -1, 2) \in V</span> represents <span class='latexinline'>r - g + 2b</span> --- that is, take a copy of the
red vertex, subtract the green vertex, and add two copies of the blue vertex.
The boundary operator <span class='latexinline'>\partial_{EV}: \mathcal{E} \rightarrow \mathcal V</span> is
depicted in the picture. This operator sends edges to their <i>boundary</i>, and is
therefore called the <i>boundary operator</i>.  The <i>boundary</i> of an edge describes
the edge in terms of vertices, just like we would describe a direction vector
(to borrow physics parlance) by subtracting points.
The action of the operator on a linear combination of edges is:
<div class='latexblock'>
\begin{align*}
&\partial_{EV}: \mathcal E \rightarrow \mathcal V \\
&\partial_{EV}(1, 0, 0) \equiv (1, -1, 0) \qquad o \mapsto r - g \\
&\partial_{EV}(0, 1, 0) \equiv (-1, 0, 1) \qquad m \mapsto b - r \\
&\partial_{EV}(0, 0, 1) \equiv (0, 1, -1) \qquad c \mapsto b - g \\
&\text{(Extend using linearity)} \\
&\partial_{EV}(s, t, u) \equiv
  s \partial_{EV}(1, 0, 0) +
  t \partial_{EV}(0, 1, 0) +
  u \partial_{EV}(0, 0, 1) = (s - t, u - s, t - u)
\end{align*}
</div>
Now, notice that to traverse the cycle, we should traverse the orange edge,
then the magenta edge, then the cyan edge, in that direction. That is,
the cycle can be thought of as <span class='latexinline'>o + m + c</span>. However, how do we <i>detect</i> this
cycle? The key idea is that if we look at the
<i>image of the cycle <span class='latexinline'>o + m + c</span> under the boundary operator</i> <span class='latexinline'>\partial_{EV}</span>,
we will get <span class='latexinline'>0</span>! For us to have completed a cycle, we must have both
entered and exited each vertex, so the total sum must be <span class='latexinline'>0</span>.
Formally:
<div class='latexblock'>
\begin{align*}
  &\partial_{EV}(s, t, u) \equiv (s - t, u - s, t - u) \\
  &o + m + c = (1, 1, 1) \in \mathcal E \quad
  \partial_{EV}((1, 1, 1) = (1 - 1, 1 - 1, 1 - 1) = (0, 0, 0)
\end{align*}
</div>
<h4><a id=formal-definition-of-cycles href='#formal-definition-of-cycles'> § </a> Formal definition of cycles</h4>
This is very nice, since we have converted the topological invariant
of a <i>hole in the space</i> into an algebraic invariant of "linear combination
of edges that map to 0". That is, we want to consider all thoose loops
that belong to the <i>kernel</i> of <span class='latexinline'>\partial_{EV}</span>. (Terminology:
the kernel of a linear transformation is the set of all things in the domain
which map to zero)
So, we define (tentatively) the first homology group:
<div class='latexblock'>
\begin{align*}
H_1 \equiv Kernel(\partial_{EV}) \equiv
\left \{ (a, b, c) \in \mathcal E \mid \partial_EV((a, b, c)) = (0, 0, 0) \right \}
\subset \mathcal E
\end{align*}
</div>
If we try to compute this, we will have to have:
<div class='latexblock'>
\begin{align*}
H_1 &\equiv Kernel(\partial_{EV}) \\
&= \{ (s, t, u) ~\mid~ \partial_{EV}(s, t, u) = (0, 0, 0) ~ s, t, u \in \mathbb Z \} \\
&= \{ (s, t, u) ~\mid~ (s-t, u-s, t-u) = (0, 0, 0) ~ s, t, u \in \mathbb Z  \} \\
&= \{ (s, t, u) ~\mid~ s = t = u \quad s, t, u \in \mathbb Z \} \\
&= \{ (x, x, x) ~\mid~ x \in \mathbb Z \} \simeq \mathbb Z
\end{align*}
</div>
So, we know that we have a <span class='latexinline'>\mathbb Z</span> worth of cycles in our triangle, which
makes sense: We can go clockwise (positive numbers)
and counter-clockwise (negative numbers) around the triangle,
and we can go as many times as we wish, so we have <span class='latexinline'>\mathbb Z</span> as the
number of cycles.
that is, it's the linear combination of edges that map to zero through the
boundary map. Note that this also includes combinations such as <i>two</i> loops
around the triangle, such as <span class='latexinline'>o + m + c + o + m + c</span>.
<h4><a id=no-h-triangle- href='#no-h-triangle-'> § </a> (No) Holes in a space: Homology of a <i>filled</i> triangle</h4>
<img width=200 height=200 src="static/simplices/homology-triangle-faces.svg">
In this case, notice that the triangle is <i>filled</i> with a face <span class='latexinline'>f</span>.
Therefore, the "hole" that we had previously is now filled up, and does not
count anymore. So, we now need to amend our previous definition of <span class='latexinline'>H_1</span> to
kill the hole we had detected.
The idea is that the hole we had previously is now the
<i>boundary of the new face <span class='latexinline'>f</span></i>.
Since it is the boundary of a "filled in" region, it does not count anymore,
since we can "shrink the hole" across the face to make it a non-loop.
Hence, we need to quotient our <span class='latexinline'>H_1</span> with the boundary of the face.
Formally, what we do is we create another group <span class='latexinline'>\mathcal F \equiv \mathbb Z</span>,
which counts copies of our face <span class='latexinline'>f</span>, and we define another boundary operator,
such that the boundary of the face <span class='latexinline'>f</span> is <span class='latexinline'>o + m + c</span>.
<div class='latexblock'>
\begin{align*}
&\partial_{FE} : \mathcal F \rightarrow \mathcal E \\
&\partial_{FE}(1) \equiv (1, 1, 1)  \\
&\text{(Extend using linearity)} \\
&\partial_{FE}(c) \equiv c \partial(1) = (c, c, c)
\end{align*}
</div>
Now, we should notice that the <i>image</i> of <span class='latexinline'>\partial_{FE}</span> is a loop
<span class='latexinline'>(o + m + c)</span>, which lies ie the <i>kernel</i> of <span class='latexinline'>\partial_{EV}</span>. This is a
general feature of homology, so it bears repeating:
<ul><li> <span class='latexinline'>Image(\partial_{FE}) \subset Kernel(\partial_{EV})</span></li><li> <span class='latexinline'>\partial_{FE} \circ \partial_{EV} = 0</span></li><li> The above equation is sometimes stylishly (somewhat misleadingly) written as
  <span class='latexinline'>\partial^2 = 0</span>. More faithfully, one can write <span class='latexinline'>\partial_{EV} \circ \partial_{FE} = 0</span>.</li></ul>
Now, since the image of <span class='latexinline'>\partial_{FE}</span> lies entirely in the kernel of <span class='latexinline'>\partial_{EV}</span>,
we can construct <span class='latexinline'>H_1</span> as:
<ul><li> <span class='latexinline'>H_1 \equiv Kernel(\partial_{EV}) / Image(\partial_{FE}) \subset E</span></li></ul>
<h4><a id=a-complicated-space-homology-of-a-butterfly href='#a-complicated-space-homology-of-a-butterfly'> § </a> A complicated space: Homology of a butterfly</h4>
<h3><a id=an-invitation-to-homology-and-cohomology-part-2--cohomology href='#an-invitation-to-homology-and-cohomology-part-2--cohomology'> § </a> <a href=#an-invitation-to-homology-and-cohomology-part-2--cohomology>An invitation to homology and cohomology, Part 2 --- Cohomology</a></h3>

<img width=200 height=200 src="static/simplices/cohomology-triangle-vertices.svg">
Once again, we have our humble triangle with vertices <span class='latexinline'>V = \{r, g, b\}</span>,
edges <span class='latexinline'>E = \{o, m, c \}</span>, faces <span class='latexinline'>F = \{ f \}</span> with boundary maps <span class='latexinline'>\partial_{EV}</span>,
<span class='latexinline'>\partial_{FE}</span>:
<ul><li> <span class='latexinline'>\partial_{FE}(f)= o + m + c</span></li><li> <span class='latexinline'>\partial_{EV}(o) = r - g</span></li><li> <span class='latexinline'>\partial_{EV}(m) = b - r</span></li><li> <span class='latexinline'>\partial_{EV}(c)= g - b</span></li></ul>
We define a function <span class='latexinline'>h_v: V \rightarrow \mathbb R</span> on the vertices as:
<ul><li>  <span class='latexinline'>h_v(r) = 3</span>, <span class='latexinline'>h_v(g) = 4</span>, <span class='latexinline'>h_v(b) = 10</span>.</li></ul>
We now learn how to <i>extend</i> this function to the higher dimensional objects,
the edges and the faces of the triangle.
To extend this function to the edges, we define a new function:
<ul><li> <span class='latexinline'>h_e: E \rightarrow R</span></li><li> <span class='latexinline'>h_e(e) \equiv \sum_i \alpha_i h_v(v_i)</span> where <span class='latexinline'>\partial_{EV} e = \sum_i \alpha_i v_i</span></li></ul>
Expanded out on the example, we evaluate <span class='latexinline'>h_v</span> as:
<ul><li> <span class='latexinline'>h_e(o) \equiv d h_v(o)  = h_v(r) - h_v(g) = 3 - 4 = -1</span></li><li> <span class='latexinline'>h_e(m) \equiv d h_v(m)  = h_v(b) - h_v(r) = 10 - 3 = +7</span></li><li> <span class='latexinline'>h_e(c) \equiv d h_v(c)  = h_v(g) - h_v(b) = 4 - 10 = -6</span></li></ul>
More conceptually, we have created an <i>operator</i> called <span class='latexinline'>d</span> (the <b>coboundary operator</b>)
which takes functions defined on vertices to functions defined on edges. This
uses the boundary map on the edges to "lift" a function on the vertices to a
function on the edges.  It does so by assigning the "potential difference" of
the vertices to the edges.
<ul><li> <span class='latexinline'>d: (V \rightarrow \mathbb R) \rightarrow (E \rightarrow \mathbb R)</span></li><li> <span class='latexinline'>d(h_v) \equiv h_e</span>, <span class='latexinline'>h_e(e) \equiv \sum_i \alpha_i f(v_i)</span> where <span class='latexinline'>\partial_{EV} e = \sum_i \alpha_i v_i</span></li></ul>
We can repeat the construction we performed above, to construct another operator
<span class='latexinline'>d : (E \rightarrow \mathbb R) \rightarrow (F \rightarrow \mathbb R)</span>, defined
in <i>exactly the same way</i> as we did before. For example, we can evaluate:
<ul><li> <span class='latexinline'>h_f \equiv d(h_e)</span></li><li> <span class='latexinline'>h_f(f) \equiv d h_e(f) = h_e(o) + h_e(m) + h_e(c) = -1 + 7 -6 = 0</span></li></ul>
What we have is a chain:
<ul><li> <span class='latexinline'>h_v \xrightarrow{d} h_e \xrightarrow{d} h_f</span></li></ul>
Where we notice that <span class='latexinline'>d^2 = d \circ d = 0</span>, since the function <span class='latexinline'>h_f</span> that we have gotten
evaluates to zero on the face <span class='latexinline'>f</span>. We can prove this will happen <i>in general</i>,
for any choice of <span class='latexinline'>h_v</span>.
(it's a good exercise in definition chasing).
Introducing some terminology, A differential form <span class='latexinline'>f</span> is said to be a <b>closed differential form</b>
iff <span class='latexinline'>df = 0</span>.
In our case, <span class='latexinline'>h_e</span> <b>is closed</b>, since <span class='latexinline'>d h_e = h_f = 0</span>. On the other hand
<span class='latexinline'>h_v</span> is <b>not closed</b>, since <span class='latexinline'>d h_v = h_e \neq 0</span>.
The intuition for why this is called "closed" is that its coboundary vanishes.
<h3><a id=exploring-the-structure-of-functions-defined-on-the-edges href='#exploring-the-structure-of-functions-defined-on-the-edges'> § </a> Exploring the structure of functions defined on the edges</h3>
Here, we try to understand what functions defined on the edges can look like,
and their relationship with the <span class='latexinline'>d</span> operator. We discover that there are
some functions <span class='latexinline'>g_e: E \rightarrow \mathbb R</span> which can be realised as the differential
of another function <span class='latexinline'>g_v: V \rightarrow \mathbb R</span>. The differential
forms such as <span class='latexinline'>g_e</span> which can be generated a <span class='latexinline'>g_v</span> through the <span class='latexinline'>d</span> operator
are called as <b>exact differential forms</b>. That is, <span class='latexinline'>g_e = d g_v</span> <i>exactly</i>,
such that there is no "remainder term" on applying the <span class='latexinline'>d</span> operator.
<img width=200 height=200 src="static/simplices/cohomology-triangle-edges.svg">
We take an example of a differential form that is <i>not exact</i>, which has been
defined on the edges of the triangle above. Let's call it <span class='latexinline'>h_e</span>.
It is defined on the edges as:
<ul><li> <span class='latexinline'>h_e(c) = 3</span></li><li> <span class='latexinline'>h_e(m) = 2</span></li><li> <span class='latexinline'>h_e(o) = 1</span></li></ul>
We can calcuate <span class='latexinline'>h_f = d h_e</span> the same way we had before:
<ul><li> <span class='latexinline'>h_f(f) \equiv d h_e(f) = h_e(o) + h_e(m) + h_e(c) = 3 + 1 + 2 = 6</span>.</li></ul>
Since <span class='latexinline'>d h_e \neq 0</span>, this form is not exact.
Let's also try to generate <span class='latexinline'>h_e</span> from a potential. We arbitrarily fix the
potential of <span class='latexinline'>b</span> to <span class='latexinline'>0</span>. That is, we fix <span class='latexinline'>h_v(b) = 0</span>, and we then try to
see what values we are forced to values of <span class='latexinline'>h_v</span> across the rest of the triangle.
<ul><li> <span class='latexinline'>h_v b = 0</span></li><li> <span class='latexinline'>h_e(c) = h_v(g) - h_v(b)</span>. <span class='latexinline'>h_v(g) =  h_v(b) + h_e(c) = 0 + 3 = 3</span>.</li><li> <span class='latexinline'>h_e(o) = h_v(r) - h_v(g)</span>. <span class='latexinline'>h_v(r) =  h_v(g) + h_e(o) = 3 + 1 = 4</span>.</li><li> <span class='latexinline'>h_e(m) = h_v(b) - h_v(r)</span> <span class='latexinline'>2 = 0 - 4</span>. This is a contradiction!</li><li> Ideally, we need <span class='latexinline'>h_v(b) = 6</span> for the values to work out.</li></ul>
Hence, there can exist no such <span class='latexinline'>h_v</span> such that <span class='latexinline'>h_e \equiv d h_v</span>.
The interesting thing is, when we started out by assigning <span class='latexinline'>h_v(b) = 0</span>,
we could make <i>local choices</i> of potentials that seemed like they would fit
together, but they failed to fit <i>globally</i> throughout the triangle. This
failure of <i>locally consistent choices</i> to be <i>globally consistent</i> is
the essence of cohomology.
<h3><a id=cohomology-of-half-filled-butterfly href='#cohomology-of-half-filled-butterfly'> § </a> Cohomology of half-filled butterfly</h3>
<img width=400 src="static/simplices/cohomology-half-filled-butterfly.svg">
Here, we have vertices <span class='latexinline'>V \equiv \\{ r, g, b, b, p \\}</span>, edges
<span class='latexinline'>E \equiv \\{rb, gr, bg, m, o, c \\}</span> and faces <span class='latexinline'>F \equiv \\{ f \\}</span>.
Here, we see a differential form <span class='latexinline'>h_e</span> that is defined on the edges,
and also obeys the equation <span class='latexinline'>dh_e = 0</span> (Hence is closed). However, it
<i>does not have an associated potential energy</i> to derive it from. That is,
there cannot exist a certain <span class='latexinline'>h_v</span> such that <span class='latexinline'>d h_v = h_e</span>.
So, while every exact form is closed, <i>not every</i> closed form is exact.
Hence, this <span class='latexinline'>g</span> that we have found is a non-trivial element of <span class='latexinline'>Kernel(d_{FE}) / Image(d_{EV})</span>,
since <span class='latexinline'>dh_e = 0</span>, hence <span class='latexinline'>h_e \in Kernel(d_{FE})</span>, while there does not exist
a <span class='latexinline'>h_v</span> such that <span class='latexinline'>d h_v = h_e</span>, hence it is <i>not quotiented</i> by the image of
<span class='latexinline'>d_{EV}</span>.
So the failure of the space to be fully filled in (ie, the space has a hole),
is measured by the <i>existence of a function <span class='latexinline'>h_e</span> that is closed but not exact!</i>
This reveals a deep connection between homology and cohomology, which is
made explicit by the <a href=TODO>Universal Coefficient Theorem</a>
<h3><a id=stuff-i-learnt-in-2019 href='#stuff-i-learnt-in-2019'> § </a> <a href=#stuff-i-learnt-in-2019>Stuff I learnt in 2019</a></h3>
I write these retrospective blog posts every year since 2017. I tend to post a
collection of papers, books, and ideas I've stumbled across that year.
Unfortunately, this year, the paper list will be sparser, since I lost some
data along the way to the year, and hence I don't have links to everything I
read. So this is going to be a sparser list, consisting of things that I found
<i>memorable</i>.
I also re-organised my website, letting the link die, since keeping it up was
taking far too many cycles (In particular, CertBot was far too annoying to
maintain, and the feedback of hugo was also very annoying). I now have a
<i>single</i> file, the
<a href=https://github.com/bollu/bollu.github.io><code>README.md</code>of the <code>bollu/bollu.github.io</code></a>
repo,
to which I add notes on things I find interesting. I've bound the <code>i</code> alias
(for idea) on all my shells everywhere, to open the <code>README.md</code> file, wait
for me to write to it, run a <code>git commit</code> so I can type out a commit, and
then push. This has been <i>massive</i> for what I manage to write down: I feel
like I've managed to write down a lot of one-liners / small facts that I've
picked up which I would not otherwise. I'm attempting to similarly pare down
other friction-inducing parts of my workflow. Suggestions here would be very
welcome!
If there's a theme of the year (insofar as my scattered reading has a
theme...), it's "lattices and geometry". Geometry in terms of differential
geometry, topology, and topoi. Lattices in the sense of a bunch of abstract
interpretation and semantics.
<h4><a id=course-work-optimisation-theory-quantum-computation-statistics href='#course-work-optimisation-theory-quantum-computation-statistics'> § </a> Course work: optimisation theory, quantum computation, statistics</h4>
My course work was less interesting to me this time, due to the fact that I had
chosen to study some wild stuff earlier on, and now have to take reasonable stuff
to graduate. However, there were courses that filled in a lot of gaps in my
self-taught knowledge for me, and the ones I listed were the top ones in that
regard.
I wound up reading
<a href=https://web.stanford.edu/~boyd/cvxbook/>Boyd on optimisation theory</a>,
<a href=http://mmrc.amss.cas.cn/tlb/201702/W020170224608149940643.pdf>Nielsen and Chuang</a> for quantum computation,
where I also
<a href=https://github.com/bollu/quantum-course-exercises>solved a bunch of exercises in Q#</a>
which was very fun and rewarding. I'm beginning to feel that learning quantum
computation is the right route to grokking things like entanglement and
superposition, unlike the physics which is first of all much harder due to
infinite dimensionality, and less accessible since we can't <i>program</i> it.
<h4><a id=formal-research-work-compilers-formal-verification-programming-languages href='#formal-research-work-compilers-formal-verification-programming-languages'> § </a> Formal research work: Compilers, Formal verification, Programming languages</h4>
My research work is on the above topics, so I try to stay abreast of what's
going on in the field. What I've read over the past year on these topics is:
<ul><li> <a href=https://popl19.sigplan.org/details/POPL-2019-Research-Papers/71/A-2-I-Abstract-2-Interpretation><code>A^2I</code>: meta-abstract interpretation</a>.
  This paper extends the theory of abstract interpretation to perform abstract
  interpretation on program analyses themselves. I'm not sure how <i>useful</i> this
  is going to be, as I still hold on to the belief that AI as a framework is
  too general to allow one to prove complex results. But I am still interested
  in trying to adapt this to some problems I have at hand. Perhaps it's going
  to work.</li></ul>
<ul><li> <a href=https://dl.acm.org/citation.cfm?id=3341691>Cubicial Agda</a>. This paper introduces
  cubical type theory and its implementation in Agda. It appears to solve many
  problems that I had struggled with during my formalization of loop
  optimisations: In particular, dealing with Coinductive types in Coq, and that
  of defining quotient types / setoids. Supposedly, cubical Agda makes dealing
  with Coinduction far easier. It allows allows the creation of "real" quotient
  types that respect equality, without having to deal with <code>setoid</code> style
  objects that make for large Gallina terms. I don't fully understand how the
  <i>theory</i> works: In particular, as far as I can tell, the synthetic interval
  type <code>I</code> allows one to only access the start and end points (<code>0</code> and <code>1</code>),
  but not anything in between, so I don't really see how it allows for
  interpolation.  I also don't understand how this allows us to make Univalence
  computable.  I feel I need to practice with this new technology before I'm
  well versed, but it's definitely a paper I'm going to read many, many times
  till I grok it.</li></ul>
<ul><li> <a href=https://arxiv.org/abs/1911.05844>Naive Cubical type theory</a>. This paper
  promises a way to perform informal reasoning with cubical type theory, the
  way we are able to do so with, say, a polymorphic type theory for lambda
  calculus. The section names such as "how do we think of paths",
  "what can we do with paths", inspire confidence</li></ul>
<ul><li> <a href=https://icfp19.sigplan.org/details/icfp-2019-papers/26/Call-By-Need-is-Clairvoyant-Call-By-Value>Call by need is Clairvoyant call by value</a>. This key insight is to notice that call by need
  is "just" call by value, when we evaluate only those values that are
  eventually forced, and throw away the rest. Thus, if we had an oracle that
  tells us which values are eventually forced, we can convert call by need into
  call by value, relative to this oracle. This cleans up many proofs in the
  literature, and might make it far more intuitive to teach call by need to
  people as well. Slick paper, I personally really enjoyed reading this.</li></ul>
<ul><li> <a href=https://arxiv.org/abs/1803.10228>Shift/Reset the Penultimate Backpropagator</a>
  This paper describes how to implement backprop using delimited continuations.
  Also, supposedly, using staging / building a compiler out of this paradigm
  allows one to write high performance compilers for backprop without having
  to suffer, which is always nice.</li></ul>
<ul><li> <a href=https://www.cs.princeton.edu/~zkincaid/pub/popl19a.pdf>Closed forms for numerical loops</a>
  This paper introduces a new algebra of polynomials with exponentials. It then
  studies the eigenvalues of the matrix that describes the loop, and tries to
  find closed forms in terms of polynomials and exponentials. They choose
  to only work with rationals, but not extensions of rational numbers
  (in terms of field extensions of the rationals). Supposedly, this is easier
  to implement and reason about. Once again, this is a paper I'd like to
  reimplement to understand fully, but the paper is well-done!</li></ul>
<ul><li> <a href=https://engineering.purdue.edu/Papers/Sundararajah.pdf>Composable, sound transformations of Nested recursion and loops</a>.
  This paper attempts to bring ideas from polyhedral compilation
  into working with nested recursion. They create a representation using
  multitape finite automata, using which they provide a representation for
  nested recursion. I was somewhat disappointed that it does not handle
  mutual recursion, since my current understanding is that one can always
  convert nested recursion into a "reasonable" straight line program by
  simply inlining calls and then re-using polyhedral techniques.</li></ul>
<ul><li> <a href=https://github.com/bollu/blaze/blob/master/notebooks/tutorial.ipynb>Reimplementation of <code>STOKE</code> at <code>bollu/blaze</code>.</a>
  I reimplemented the <a href=http://stoke.stanford.edu/>STOKE: stochastic superoptimisation</a>
  paper, and much to my delight, it was super-effective at regenerating common
  compiler transformations. I want to use this to generate loop optimisations
  as well, by transforming a polyhedral model of the original program.</li></ul>
<h4><a id=internsh-over-the-summer-hacking-on-asterius-haskell--webassembly-compiler href='#internsh-over-the-summer-hacking-on-asterius-haskell--webassembly-compiler'> § </a> Internship at <a href=http://tweag.io/>Tweag.io</a> over the summer: Hacking on Asterius (Haskell -> WebAssembly compiler)</h4>
<ul><li> <a href=https://www.tweag.io/posts/2019-09-12-webassembly-internship.html>Blog post on the progress made by me hacking on Austerius over at Tweag</a></li></ul>
I really enjoyed my time at Tweag! It was fun, and
<a href=https://github.com/TerrorJack>Shao Cheng</a>
was a great mentor. I must admit that I was somewhat distracted, by all the new
and shiny things I was learning thanks to all the cool people there <code>:)</code> In
particular, I wound up bugging
<a href=http://assert-false.net/arnaud/>Arnaud Spiwack</a>,
<a href=http://simeon-carstens.com/>Simeon Carstens</a>,
and <a href=https://github.com/mmesch>Matthias Meschede</a>
quite a bit, about type theory, MCMC sampling, and signal processing of storm
clouds.
I wound up reading a decent chunk of GHC source code, and while I can't link
to specifics here, I understood a lot of the RTS much better than I did before.
It was an enlightening experience, to say the least, and being paid to hack on
a GHC backend was a really fun way to spend the summer.
It also led me to fun discoveries, such as
<a href=https://github.com/ghc/ghc/blob/535a26c90f458801aeb1e941a3f541200d171e8f/compiler/cmm/Debug.hs#L458>how does one debug debug info?</a>
I also really loved Paris as a city. My AirBnb host was a charming artist who
suggest spots for me around the city, which I really appreciated. Getting
around was disorienting for the first week or so, due to the fact that I could
not (and still do not) really understand how to decide in which direction to
walk inside the subways to find a particular line 
<i>going in a particular direction</i>.
The city has some great spots for quiet work, though! In particular, the
<a href=https://www.anticafe.eu/lieux/louvre-paris-75001/>Louvre Anticafe</a>
was a really nice place to hang out and grab coffee. The model is great: you
pay for hours spent at the Anticafe, with coffee and snacks free. They also
had a discount for students which I gratefully used.
I bumped into interesting artists, programmers, and students who were open for
conversation there. I highly recommend hanging out there.
<h4><a id=probabilistic-programming-giving-a-talk-at-functionalconf href='#probabilistic-programming-giving-a-talk-at-functionalconf'> § </a> Probabilistic programming & giving a talk at FunctionalConf</h4>
This was the first talk I'd ever given, and it was on probabilistic programming
in haskell. In particular, I explained the
<a href=https://github.com/adscib/monad-bayes><code>monad-bayes</code></a> approach of
doing this, and why this was profitable.
<a href=https://github.com/bollu/functionalconf-2019-slides-probabilistic-programming/blob/master/slides.pdf>The slides are available here</a>.
It was a fun experience giving a talk, and I'd like to do more of it, since I
got a lot out of attempting to explain the ideas to people. I wish I had more
time, and had a clearer idea of who the audience was. I got quite a bit of
help from <a href=https://www.snoyman.com/>Michael Snoyman</a> to whip the talk into
shape, which I greatly appreciated.
The major ideas of probabilistic programming as I described it are
from Adam Scibior's thesis:
<ul><li> <a href=https://www.cs.ubc.ca/~ascibior/assets/pdf/thesis.pdf>Adam Scibior: Formally justified and modular Bayesian inference for probabilistic programs</a></li></ul>
Along the way, I and others at tweag read the other major papers in the space,
including:
<ul><li> <a href=https://arxiv.org/pdf/1206.3255>Church, a language for generative models</a>,
  which is nice since it describes it's semantics in terms of sampling. This is
  unlike Adam's thesis, where they define the denotational semantics in terms
  of measure theory, which is then approximated by sampling.</li><li> <a href=https://pdfs.semanticscholar.org/16c5/06c5bb253f7528ddcc80c72673fabf584f32.pdf>Riemann Manifold Langevin and Hamiltonian Monte Carlo</a>
  which describes how to perform Hamiltonian Monte Carlo on the 
  <i>information geometry</i> manifold.  So, for example, if we are trying to sample 
  from gaussians, we sample from a 2D Riemannian manifold with parameters mean 
  and varince, and metric as the <a href=https://en.wikipedia.org/wiki/Fisher_information_metric>Fisher information metric</a>.
  This is philosophically the "correct" manifold to sample from, since it
  represents the intrinsic geometry of the space we want to sample from.</li><li> <a href=https://arxiv.org/pdf/1808.08271.pdf>An elementary introduction to Information geometry by Frank Nielsen</a>
  something I stumbled onto as I continued reading about sampling from
  distributions. The above description about the "correct" manifold for
  gaussians comes from this branch of math, but generalises it quite a bit
  further. I've tried to reread it several times as I gradually gained maturity
  in differential geometry. I can't say I understand it just yet, but I hope to
  do so in a couple of months. I need more time for sure to meditate on the
  objects.</li><li> <a href=https://github.com/bollu/shakuni>Reimplementation of <code>monad-bayes</code></a>.
  This repo holds the original implementation on which the talk is based on.
  I read through the <code>monad-bayes</code> source code, and then re-implemented the
  bits I found interesting. It was a nice exercise, and you can see
  the git history tell a tale of my numerous mis-understandings of MCMC methods,
  till I finally got what the hell was going on.</li></ul>
<h4><a id=presburger-arithmetic href='#presburger-arithmetic'> § </a> Presburger Arithmetic</h4>
Since we use a bunch of <a href=https://en.wikipedia.org/wiki/Presburger_arithmetic>presburger arithmetic</a>
for <a href=http://polyhedral.info/>polyhedral compilation</a>
which is a large research interest of mine, I've been trying to build a
"complete" understanding of this space. So this time, I wanted to learn
how to build good solvers:
<ul><li> <a href=https://github.com/bollu/gutenberger><code>bollu/gutenberger</code></a> is a decision
  procedure for Presburger arithmetic that exploits their encoding as finite
  automata. One thing that I was experimenting with was that we only use
  numbers of finite bit-width, so we can explore the entire state space
  of the automata and then perform NFA reduction using
  <a href=https://en.wikipedia.org/wiki/DFA_minimization>DFA minimisation</a>. The
  reference I used for this was the excellent textbook
  <a href=https://www7.in.tum.de/~esparza/autoskript.pdf>Automata theory: An algorithmic approach, Chapter 10</a></li><li> <a href=http://www.lsv.fr/~haase/documents/ch16.pdf>The taming of the semi-linear set</a>
  This uses a different encoding of presburger sets, which allows them to bound
  a different quantity (the norm) rather than the bitwidth descriptions. This allows
  them to compute <i>exponentially</i> better bounds for some operations than
  were known before, which is quite cool. This is a paper I keep trying to
  read and failing due to density. I should really find a week away from civilization
  to just plonk down and meditate upon this.</li></ul>
<h4><a id=open-questions-for-which-i-want-answers href='#open-questions-for-which-i-want-answers'> § </a> Open questions for which I want answers</h4>
I want better references to being able to <i>regenerate</i> the inequalities
description from a given automata which accepts the presburger set automata.
This will allow one to smoothly switch between the <i>geometric</i> description
and the <i>algebraic</i> description. There are some operations that only work
well on the geometry (such as optimisation), and others that only work well on
the algebraic description (such as state-space minimisation). I have not found
any good results for this, only scattered fragments of partial results.
If nothing else, I would like some kind of intuition for <i>why this is hard</i>.
Having tried my stab at it, the general impression that I have is that the
space of automata is much larger than the things that can be encoded as
presburger sets. Indeed, it was shown that automata accept numbers which
are ultimately periodic.
<ul><li>  first order logic + "arithmetic with +" + (<i>another operation I cannot recall</i>).
   I'm going to fill this in once I re-find the reference.</li></ul>
But yes, it's known that automata accept a language that's broader than just
first order logic + "arithmetic with +", which means it's hard to dis-entangle
the presburger gits from the non-presburger bits of the automata.
<h4><a id=prolog href='#prolog'> § </a> Prolog</h4>
I wanted to get a better understading of how prolog works under the hood, so I began
re-implementing the <a href=http://wambook.sourceforge.net/>WAM: warren abstract machine</a>.
It's really weird, this is the <i>only stable reference</i> I can find to implementing
high-performance prolog interpreters. I don't really understand how to chase the
paper-trail in this space, I'd greatly appreciate references. My implementation
is at <a href=https://github.com/bollu/warren-cpp/><code>bollu/warren-cpp</code></a>. Unfortunately,
I had to give up due to a really hard-to-debug bug.
It's crazy to debug this abstract machine, since the internal representation gets
<i>super convoluted</i> and hard to track, due to the kind of optimised encoding it
uses on the heap.
If anyone has a better/cleaner design for implementing good prologs, I'd love
to know.
Another fun paper I found in this space thanks to Edward Kmett was
<a href=http://www.drdobbs.com/architecture-and-design/the-rete-matching-algorithm/184405218>the Rete matching algorithm</a>,
which allows one to declare many many pattern matches, which are then "fused"
together into an optimal matcher that tries to reuse work across failed
matchers.
<h4><a id=general-relativity href='#general-relativity'> § </a> General Relativity</h4>
This was on my "list of things I want to understand before I die", so I wound
up taking up an Independent Study in university, which basically means that
I study something on my own, and visit a professor once every couple weeks,
and am graded at the end of the term. For GR, I wound up referencing a wide
variety of sources, as well as a bunch of pure math diffgeo books. I've read
everything referenced to various levels. I feel I did take away the core
ideas of differential and Riemannian geometry. I'm much less sure I've grokked
general relativity, but I can at least read the equations and I know all the
terms, so that's something.
<ul><li> <a href=https://theoreticalminimum.com/courses/general-relativity/2012/fall>The theoretical minimum by Leonard Susskind</a>.
  The lectures are breezy in style, building up the minimal theory (and no proofs)
  for the math, and a bunch of lectures spent analysing the physics. While I wish
  it were a little more proof heavy, it was a really great reference to learn the
  basic theory! I definitely recommend following this and then reading other
  books to fill in the gaps.</li><li> <a href=https://en.wikipedia.org/wiki/Gravitation_(book>Gravitation by Misner Thorne and Wheeler</a>)
  This is an imposing book. I first read through the entire thing (Well, the parts I thought I needed),
  to be able to get a vague sense of what they're going for. They're rigorous in
  a very curious way: It has a bunch of great <i>physics</i> perspectives of looking
  at things, and that was invaluable to me. Their view of forms as "slot machines"
  is also fun. In general, I found myself repeatedly consulting this book for
  the "true physical" meaning of a thing, such as curvature, parallel transport,
  the equation of a geodesic, and whatnot.</li><li> <a href=http://www2.ing.unipi.it/griff/files/dC.pdf>Differential Geometry of Curves and Surfaces by do Carmo</a>
  This is the best book to intro differential geometry I found. It throws away
  all of the high powered definitions that "modern" treatments offer, and
  starts from the ground up, building up the theory in 2D and 3D. This is amazing,
  since it gives you small, computable examples for things like
  "the Jacobian represents how tangents on a surface are transformed locally".</li><li> <a href=https://www.youtube.com/watch?v=pXGTevGJ01o&list=PLDfPUNusx1EoVnrQcCRishydtNBYU6A0c>Symplectic geometry & classical mechanics by Tobias Osborne</a>
  This lecture series was great, since it re-did a lot of the math I'd seen
  in a more physicist style, especially around vector fields, flows, and
  Lie brackets. Unfortunately for me, I never even <i>got</i> to the classical
  mechanics part by the time the semester ended. I began
  <a href=https://github.com/bollu/notes/blob/master/diffgeo/main.pdf>taking down notes in my repo</a>,
  which I plan to complete.</li><li> <a href=https://sites.math.washington.edu/~lee/Books/ISM/>Introduction to Smooth manifolds: John Lee</a>
  This was a very well written <i>mathematical</i> introduction to differential geometry.
  So it gets to the physically important bits (metrics, covariant derivatives)
  far later, so I mostly used it as a reference for problems and more rigour.</li><li> <a href=http://hermes.ffn.ub.es/luisnavarro/nuevo_maletin/Einstein_GRelativity_1916.pdf>Einstein's original paper introducing GR, translated</a>
  finally made it click as to <i>why</i>
  he wanted to use tensor equations: tensor equations of the form <code>T = 0</code> are
  invariant in <i>any coordinate system</i>, since on change of coordinates, <code>T</code>
  changes by a multiplicative factor! It's a small thing in hindsight, but it
  was nice to see it explicitly spelled out, since as I understand, no one
  among the physicists knew tensor calculus at the time, so he had to introduce
  all of it.</li></ul>
<h4><a id=discrete-differential-geometry href='#discrete-differential-geometry'> § </a> Discrete differential geometry</h4>
I can't recall how I ran across this: I think it was because I was trying to
get a better understanding of Cohomology, which led me to Google for
"computational differential geometry", that finally led me to Discrete
differential geometry.
It's a really nice collection of theories that show us how to discretize
differential geometry in low dimensions, leading to rich intuitions and
a myriad of applications for computer graphics.
<ul><li> <a href=https://www.cs.cmu.edu/~kmcrane/Projects/DDG/paper.pdf>The textbook by Kennan Crane on the topic</a>
  which I read over the summer when I was stuck (more often than I'd like) in
  the Paris metro. The book is very accessible, and requires just some
  imagination to grok. Discretizing differential geometry leads to most things
  being linear algebra, which means one can calculate things on paper easily.
  That's such a blessing.</li><li> <a href=https://arxiv.org/pdf/1204.6216>Geodesics in Heat</a>
  explores a really nice way to discover geodesics by simulating the heat
  equation for a short time. The intuition is that we should think of the heat
  equation as describing the evolution of particles that are performing random
  walks. Now, if we simulate this system for a short while and then look at the
  distribution, particles that reach a particular location on the graph 
  <i>must have taken the shortest path</i>, since any longer path would not have 
  allowed particles to reach there. Thus, the distribution of particles at 
  time <code>dt</code> does truly represent distances from a given point.  The paper
  explores this analogy to find accurate geodesics on complex computational
  grids. This is aided by the use of differential geometry, appropriately
  discretized.</li><li> <a href=https://arxiv.org/pdf/1805.09170.pdf>The vector heat method</a>
  explores computing the parallel transport of a vector across a discrete
  manifold efficiently, borrowing techniques from the 'Geodesics in Heat'
  paper.</li><li> <a href=https://www.cs.cmu.edu/~kmcrane/Projects/LieGroupIntegrators/paper.pdf>Another paper by Kennan Crane: Lie group integrators for animation and control of vehicles</a>
  This paper describes a general recipe to tailor-make integrators for a system
  of constraints, by directly integrating over the lie group of the
  configuration space.  This leads to much more stable integrators. I have some
  misguided hope that we can perhaps adapt these techniques to build better FRP
  (functional reactive programming) systems, but I need to meditate on this a
  lot more to say anything definitively.</li></ul>
<h4><a id=synthetic-differential-geometry href='#synthetic-differential-geometry'> § </a> Synthetic differential geometry</h4>
It was <a href=http://assert-false.net/arnaud/>Arnaud Spiwack</a>
who pointed me to this. It's a nice axiomatic
system of differential geometry, where we can use physicist style proofs of
"taking stuff upto order <code>dx</code>", and having everything work upto mathematical
rigor.
The TL;DR is that we want to add a new number called <code>dx</code> into the reals,
such that <code>dx^2 = 0</code>. But if such a number did exist, then clearly <code>dx = 0</code>.
However, the punchline is that to prove that <code>dx^2 = 0 => dx = 0</code> requires
the use of contradiction!
So, if we banish the law of excluded middle (and therefore no longer use
proof by contradiction), we are able to postulate the existence of a new
element <code>dx</code>, which obeys <code>dx^2 = 0</code>. Using this, we can build up the
whole theory of differential geometry in a pleasing way, without having to
go through the horror that is real analysis. (I am being hyperbolic, but really,
real analytic proofs are not pleasant).
<a href=https://www.github.com/bollu/diffgeo>I began formalizing this in Coq and got a formalism going: <code>bollu/diffgeo</code></a>.
Once I was done with that, I realised I don't know how to exhibit <i>models</i> of
the damn thing! So, reading up on that made me realise that I need around 8
chapters worth of a grad level textbook (the aptly named
<a href=https://link.springer.com/book/10.1007/978-1-4757-4143-8>Models of Smooth Infinitesimal Analysis</a>).
I was disheartened, so I <a href=https://mathoverflow.net/questions/346385/constructing-computable-synthetic-differential-geometry>asked on <code>MathOverflow</code></a>
(also my first ever question there), where I learnt about tangent categories and
differential lambda calculus. Unfortunately, I don't have the bandwidth to read
another 150-page tome, so this has languished.
<h4><a id=optimisation-on-manifolds href='#optimisation-on-manifolds'> § </a> Optimisation on Manifolds</h4>
I began reading
<a href=http://www.eeci-institute.eu/GSC2011/Photos-EECI/EECI-GSC-2011-M5/book_AMS.pdf>Absil: Optimisation on matrix manifolds</a>
which describes how to perform optimisation / gradient descent on
<i>arbitrary Riemannian manifolds</i>, as well as closed forms for well-known
manifolds. The exposition in this book is really good, since it picks a
concrete manifold and churns out all the basic properties of it manually. The
only problem I had with the books was that there were quite a few gaps (?) in
the proofs -- perhaps I missed a bunch.
This led me to learn Lie theory to some degree, since that was the natural
setting for many of the proofs. I finally saw <i>why</i> anyone gives a shit about
the tangent space at the identity: because it's <i>easier to compute!</i> For a
flavour of this, 
<a href=https://math.stackexchange.com/questions/3389983/explicit-description-of-tangent-spaces-of-on>consider this question on <code>math.se</code> by me that asks about computing tangent spaces of <span class='latexinline'>O(n)</span></a>.
<h4><a id=aircs-workshop href='#aircs-workshop'> § </a> AIRCS workshop</h4>
I attended the
<a href=https://intelligence.org/ai-risk-for-computer-scientists/>AI risk for computer scientists</a>
workshop hosted by
<a href=https://intelligence.org/>MIRI (Machine intelligence research institute)</a> in
December. Here, a bunch of people were housed at a bed & breakfast for a
week, and we discussed AI risk, why it's potentially the most important thing
to work on, and anything our hearts desired, really. I came away with new
branches of math I wanted to read, a better appreciation of the AI risk
community and a sense of what their "risk timelines" were, and some
explanations about sheaves and number theory that I was sorely lacking. All in
all, it was a great time, and I'd love to go back.
<h4><a id=p-adic-numbers href='#p-adic-numbers'> § </a> P-adic numbers</h4>
While I was on a particularly rough flight back from the USA to India when
coming back from the AIRCS workshop, I began to read the textbook
<a href=https://www.springer.com/gp/book/9783540629115>Introduction to p-adic numbers by Fernando Gouvea</a>,
which fascinated me, so I then
<a href=http://bollu.github.io/#a-motivation-for-p-adic-analysis>wrote up the cool parts introduced in the first two chapters as a blog post</a>.
I wish to learn more about the p-adics and p-adic analysis, since they
seem to be deep objects in number theory.
In particular, a question that I thought might have a somewhat trivial answer
(<a href=https://math.stackexchange.com/questions/3482489/why-does-the-p-adic-norm-use-base-p>why do the p-adics use base p in defining norm</a>)
turned out to have answers that were quite deep, which was something
unexpected and joyful!
<h4><a id=topology-of-functional-programs href='#topology-of-functional-programs'> § </a> Topology of functional programs</h4>
<ul><li> <a href=http://cs.ioc.ee/ewscs/2012/escardo/slides.pdf>Slides by Martın Escardo</a></li><li> <a href=https://pdf.sciencedirectassets.com/272990/1-s2.0-S1571066104X00177/1-s2.0-S1571066104051357/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIGQAb828p1io4csznEej60j0PwJteuXf7OoHLSCDhkUTAiEA9ITs1JrUEOE%2Ft%2Fl5TI9ZkNLUfBIx42IZ%2FoAqQpdX4twq2AII6%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgwwNTkwMDM1NDY4NjUiDBQOxJ3s3HCbVxSheCqsAt65yorZMMtIhLF7ML5sJQ9S5wZxBayKDrRkUKOjSzXxtQWebXs70FXhhpToXKvJoDrLgsqDzdF%2FAshZY%2FkDUep6KILxKnYxCBrBINhjFqxDlRZH0s1Y991RgCyNNnwmFI%2BrH70lSrV0OxQ9Z5WdXXPDkTLXv8512Xz%2BJn5DqEqdqD49FLbOuHl6PRM0TnYyNJkLBXNVwt75kkGkaTfdgmgiqh7YpcXcHlbqI2oqNcaxFDewXwKDCC7qWD6ECclLgszoeOXOtRI91nvNac8%2BLV4bXkKLXpd99H94N2vXPUPz99p6oqfdY9ixtcfI9POFX8agUilYjXKVhAWk4FSzzzMqbtZLBfkCZT4ffDTxRgL52yD%2FmL5E0Pe4mczVlUoB5DKoB8Lkitrt0BumQzDr4ffvBTrQAjVRuzG5V0CC%2Fd1t%2BUMPkrywaYytbrXCZ%2BkDo0xDBqsljY8DaGIiFINr8BEEpT7UX42GRhcDzpnOnztdAOTea3qZ3SmXJwgEoh0aiz%2B87MmsC57s0Q%2F%2B%2FDDvHBY3zLCrz7rdewXOgk6VxI9d5mhG3Du1dwPRbgOe798S2waDCD8LQA3rw7w5wNGa9Uv3xtNVH%2BHw%2FXcQ6OiubO4GL9mK8U5g7TVPh1hLB26XBQooKJ564VGf4J9VqWxjlx3NicVhqnFlGevNJNKyVLiyRsRCyQGMV59%2BXqUwdEMQZYWLbfUwELNz1NKfWumvu9BXC5jjsJgNx%2FRERSb7hqT1svMJU91o%2FHtatGAnPvVjYaNthha9O9jm%2BG9nw1vMsdyJ0asI5w5SrlsEyb5C7Vk7aLBcHAEi3XPRhivY1Q4hZAN0xY9VfEZrF%2FoM9HCGxr5cYs%2BP9w%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20191221T113658Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6QRXTPOC%2F20191221%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8a26656e8f8c8772afdc2ae6caa9b583fd59a8ee6303d9770d25b4a3d8a6291f&hash=42518d3e5cc1e77ed961c359d0ea8f59bd582d1e5f13d69c3eeb2563a6c82abd&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1571066104051357&tid=spdf-584e8188-4806-44bb-a118-9b63a53caaff&sid=f53691052392834d3e18a62484e2909639a9gxrqb&type=client>Synthetic topology of data types and classical spaces</a></li></ul>
Both of these describe a general method to transfer all topological ideas as
statements about <i>computability</i> in a way that's far more natural (at least for
me, as a computer scientist). The notion of what a continuous function "should
be" (keeping inverse images of open sets open) arises naturally from this
computational viewpoint, and many proofs of topology amount to finding
functional programs that fit a certain type. It's a great read, and I feel gave
me a much better sense of what topology is trying to do.
<ul><li> <a href=http://bollu.github.io/#topology-is-really-about-computation--part-1>I've written some exposition on this topic</a>,
  which maybe a more accessible read than the links above, since it tries to
  distill the fundamental idea into a blog post.</li></ul>
<h4><a id=philosophy href='#philosophy'> § </a> Philosophy</h4>
I've wanted to understand philosophy as a whole for a while now, at least
enough to get a general sense of what happened in each century. The last year,
I meandered through some philosophy of science, which led me to some really
wild ideas (such as that of
<a href=https://en.wikipedia.org/wiki/Epistemological_anarchism>Paul Feyerabend's 'science as an anarchic enterprise'</a>
which I really enjoyed).
I also seem to get a lot more out of audio and video than text in general, so
I've been looking for podcasts and video lectures. I've been following:
<ul><li> <a href=https://historyofphilosophy.net/>The history of philosophy without any gaps</a>
  for a detailed exposition on, say, the greeks, or the arabic philosophers.
  Unfortunately, this podcast focuses on far too much detail for me to have been
  able to use it as a way to get a broad idea about <i>philosophy</i> in itself.</li></ul>
<ul><li> <a href=http://philosophizethis.org/>Philosophize This! by Stephen West</a>
  Is a good philosophy podcast for a <i>broad</i> overview of different areas
  of Philosophy. I got a lot out of this, since I was able to get a sense
  of the progression of ideas in (Western) Philosophy. So I now know what
  <a href=https://plato.stanford.edu/entries/phenomenology/>Phenomenology</a> is,
  or what Foucault was reacting against.</li></ul>
I also attempted to read a bunch of philosophers, but the only ones I could
make a good dent on were the ones listed below. I struggled in writing this
section, since it's much harder to sanity check my understanding of philosophy,
versus mathematics, since there seems to be a range of interpretations of the
same philosophical work, and the general imprecise nature of language doesn't
help here at all. So please take all the descriptions below with some salt
to taste.
<ul><li> <a href=https://en.wikipedia.org/wiki/Discipline_and_Punish>Discipline and Punish by Michel Foucault</a>
  Here, Foucault traces the history of the criminal justice system of society,
  and how it began as something performed 'on the body' (punishment),
  which was then expanded to a control 'of the mind' (reform). As usual,
  the perspective is fun, and I'm still going through the book.</li></ul>
<ul><li> <a href=https://en.wikipedia.org/wiki/Madness_and_Civilization>Madness and Civilization by Michel Foucault</a>
  which attempts to chronicle how our view of madness evolved as society did.
  It describes how madmen, who were on the edges of society, but still
  "respected" (for exmaple, considered as 'being touched by the gods') were
  pathologized by the Renaissance, and were seen as requiring treatment. I'm
  still reading it, but it's enjoyable so far, as a new perspective for me.</li></ul>
<ul><li> <a href=https://en.wikipedia.org/wiki/The_Value_of_Science>The value of science by Henri Poincare</a>.
  Here, he defends the importance of experimentation, as well as the value of
  intuition to mathematics, along with the importance of what we consider
  formal logic. It's a tough read sometimes, but I think I got something out of
  it, at least in terms of perspective about science and mathematics.</li></ul>
<h4><a id=information-theory href='#information-theory'> § </a> Information theory</h4>
I've been on a quest to understand information theory far better than I
currently do. In general, I feel like this might be a much better way to
internalize probability theory, since it feels like it states probabilistic
objects in terms of "couting" / "optimisation of encodings", which is a
perspective I find far more natural.
Towards this aim, I wound up reading:
<ul><li> <a href=http://www.inference.org.uk/mackay/itila/book.html>Information theory, Learning, and inference algorithms</a>
  This book attempts to provide the holistic view I was hoping for. It has
  great illustrations of the basic objects of information theory. However,
  I was hoping that the three topics would be more "unified" in the book,
  rather than being presented as three separate sections with some amount
  of back-and-forth-referencing among them. Even so, it was a really fun read.</li></ul>
<ul><li> <a href=http://www.cs-114.org/wp-content/uploads/2015/01/Elements_of_Information_Theory_Elements.pdf>Elements of information theory</a></li></ul>
<h4><a id=building-intuition-for-sheaves-topoi-logic href='#building-intuition-for-sheaves-topoi-logic'> § </a> Building intuition for Sheaves, Topoi, Logic</h4>
I don't understand the trifecta of sheaves, topoi, geometry, and logic, and
I'm trying to attack this knot from multiple sides at once.
<ul><li> <a href=https://arxiv.org/pdf/1308.4621>Understanding networks and their behaviours using sheaf theory</a></li><li> <a href=https://www.sciencedirect.com/science/article/pii/S1571066108005264>Sheaves, objects, and distributed systems</a></li><li> <a href=https://arxiv.org/pdf/1603.01446.pdf>Sheaves are the canonical data strucure for sensor integration</a></li><li> <a href=https://www.math.upenn.edu/~ghrist/EAT/EATchapter9.pdf>Elementary applied topology, Chapter 9: Sheaves</a></li><li> <a href=https://news.ycombinator.com/item?id=13677308>Sheaf theory: The mathematics of data fusion (video) (link to HackerNews)</a></li></ul>
All of these provide geometric viewpoints of what sheaves are, in low-dimensional
examples of graphs which are easy to visualize. I'm also trudging through the
tome:
<ul><li> <a href=https://www.springer.com/gp/book/9780387977102>Sheaves in geometry and logic: A first introduction to Topos theory</a></li></ul>
which appears to follow the "correct path" of the algebraic geometers, but this
requires a lot of bandwidth.
<ul><li> <a href=http://math.stanford.edu/~vakil/216blog/FOAGnov1817public.pdf>The Rising Sea: foundations of algebraic geometry by Ravi Vakil</a></li></ul>
This is a hardcore algebraic geometry textbook, and is arguably
<i>great for studying sheaves</i> because of it. Sheaves are Chapter 2, and allows
one to see them be developed in their "true setting" as it were. In that
Grothendeick first invented sheaves for algebraic geometry, so it's good to
see them in the home they were born in. Once again, this is a book I lack
bandwidth for except to breezily read it as I go to bed. I did get something
out from doing this. I'm considering taking this book up as an independent
study, say the first four chapters. I'll need someone who knows algebraic
geometry to supervise me, though, which is hard to find in an institute geared
purely for computer science. (If anyone on the internet is kind enough to
volunteer some of their time to answer questions, I'll be very glad! Please
email me at <code>rot13(fvqqh.qehvq@tznvy.pbz)</code>)
<h4><a id=the-attic href='#the-attic'> § </a> The attic</h4>
This section contains random assortments that I don't recall how I stumbled
across, but too cool to not include on the list. I usually read these in bits
and pieces, or as bedtime reading right before I go to bed to skim.  I find
that skimming such things gives me access to knowing about tools I would not
have known otherwise. I like knowing the existence of things, even if I don't
recall the exact thing, since knowing that something like <code>X</code> exists has saved me
from having to reinvent <code>X</code> from scratch.
<ul><li> <a href=http://www.cns.gatech.edu/GroupTheory/version9.0/GroupTheory.pdf>Group Theory: Birdtracks, Lie's and Exceptional Groups by Predrag Cvitanovic</a>
  is an exposition of Lie theory using some notation called as "Birdtrack notation",
  which is supposedly a very clean way of computing invariants, inspired by
  Feynmann notation. The writing style is informal and pleasant, and I decided
  to save the book purely because the first chapter begins with
  "Basic Concepts: A typical quantum theory is constructed from a few building blocks...".
  If a book considers building quantum theories as its starting point, I really
  want to see where it goes.</li></ul>
<ul><li> <a href=https://www.math.upenn.edu/~ghrist/notes.html>Elementary Applied topology by Robert M Ghirst</a>
  I wouldn't classify the book as elementary because it skims over too much to be
  useful as a reference, but it's great to gain an intuition for what, say,
  homology or cohomology is. I am currently reading the section on Sheaf theory,
  and I'm getting a lot out of it, since it describes how to write down, say,
  min-cut-max-flow or niquist-shannon in terms of sheaves. I don't grok it yet,
  but even knowing this can be done is very nice. The book is a wonderful
  walkthrough in general.</li></ul>
<ul><li> <a href=https://icerm.brown.edu/video_archive/?play=2034>On polysemous mathematical illustration by Robert M Ghirst</a>
  This is a talk on the wonderful illustrations by the above author, about
  the different types of mathematical illustrations one can have, and different
  "levels of abstraction".</li></ul>
<ul><li> <a href=http://chronologia.org/en/math_impressions/poster016.html>Mathematical Impressions: The illustrations of AT Femenko</a>
  These are <i>beautiful</i> illustrated pictures of various concepts in math, which
  tend to <i>evoke</i> the feeling of the object, without being too direct about it.
  For example, consider "gradient descent" below. I highly recommend going
  through the full gallery.</li></ul>
<ul><li> <a href=http://chronologia.org/art/math/123a176.jpg>Gradient Descent</a>
  <img width=200 height=200 src="http://chronologia.org/art/math/123a176.jpg"></li></ul>
<ul><li> <a href=http://chronologia.org/art/math/077a011.jpg>Topological Zoo</a>
  <img width=200 height=200 src="http://chronologia.org/art/math/077a011.jpg"></li></ul>
<ul><li> <a href=https://bastian.rieck.me/research/Dissertation_Rieck_2017.pdf>Persistent Homology in Multivariate Data Visualization</a>
  This is the PhD dissertation of <a href=https://bastian.rieck.me/>Bastian Rieck</a>,
  who's now a postdoc at ETH. I deeply enjoyed reading it, since it pays
  a lot of attention to the <i>design</i> of analyses, and how to interpret
  topological data. I really enjoyed getting a good sense of how one can
  use persistent homology to understand data, and the trade-offs between
  <a href=https://en.wikipedia.org/wiki/Vietoris%E2%80%93Rips_complex>Vietoris-Rips complex</a>
  and the <a href=https://en.wikipedia.org/wiki/%C4%8Cech_complex>Cech complex</a>.</li></ul>
<ul><li> <a href=https://arxiv.org/pdf/1205.5935.pdf>An introduction to Geometric algebra</a>
  I fell in love with geometric algebra, since it provides a really clean way
  to talk about <i>all possible subspaces</i> of a given vector space. This provides
  super slick solutions to many geometry and linear algebra problems. The
  way I tend to look at it is that when one does linear algebra, there's a strict
  separation between "vectors" (which are elements of the vector space), and,
  say, "hyperplanes" (which are <i>subspaces</i> of the vector space), as well as
  objects such as "rotations" (which are <i>operators</i> on the vector space).
  Geometric algebra provides a rich enough <i>instruction set</i> to throw all
  these three distinct things into a blender. This gives a really concise
  language to describe all phenomena that occurs in the vector space world ---
  which, let's be honest, is <i>most</i> tractable phenomena! I had a blast
  reading about GA and the kinds of operators it provides.</li></ul>
<ul><li> <a href=https://arxiv.org/pdf/1807.07159>Circuits via Topoi</a>. This paper attempts
  to provide an introduction to topos theory by providing a semantics for
  both combinational and sequential circuits under a unifying framework. I keep
  coming back to this article as I read more topos theory. Unfortunately, I'm
  not "there yet" in my understanding of topoi. I hope to be next year!</li></ul>
<ul><li> <a href=https://press.princeton.edu/books/paperback/9780691138718/fearless-symmetry>Fearless Symmetry</a>
  This is definitely my favourite non-fiction book that I've read in 2019, hands
  down. The book gives a great account of the mathematical objects that went
  into Wiles' book of Fermat's last theorem. It starts with things like
  "what is a permutation" and ends at questions like "what's a reciprocity law"
  or "what's the absolute galois group". While at points, I do believe the book
  goes far too rapidly, all in all, it's a solid account of number theory
  that's distilled, but not in any way diluted. I really recommend reading this
  book if you have any interest in number theory (or, like me, a passing
  distaste due to a course on elementary number theory I took, with proofs that
  looked very unmotivated). This book made me decide that I should, indeed,
  definitely learn algebraic number theory, upto at least
  <a href=https://en.wikipedia.org/wiki/Artin_reciprocity_law>Artin Reciprocity</a>.</li></ul>
<ul><li> <a href=https://en.wikipedia.org/wiki/Remembrance_of_Earth%27s_Past>Rememberance of Earth's past trilogy by Liu Cixin</a>
  While I would not classify this as "mind-blowing" (which I do classify Greg
  Egan books as), they were still a solidly fun read into how humanity would
  evolve and interact with alien races. It also poses some standard solutions
  to the Fermi Paradox, but it's done well. I felt that the fact that it was
  translated was painfully obvious in certain parts of the translation, which
  I found quite unfortunate. However, I think book 3 makes up in grandeur for
  whatever was lost in translation.</li></ul>
<ul><li> <a href=https://en.wikipedia.org/wiki/Walkaway_(Doctorow_novel>Walkaway by Cory Doctorow</a>)
  The book is set in a dystopian nightmare, where people are attempting to
  "walk away" from society and set up communes, where they espouse having
  a post-scarcity style economy based on gifting. It was a really great
  description of what such a society could look like. I took issue with some
  weird love-triangle-like-shenanigans in the second half of the book, but
  the story arc more than makes up for it. Plus, the people throw a party
  called as a "communist party" in the first page of the book, which grabbed
  my attention immediately!</li></ul>
<ul><li> <a href=http://www.cs.unipr.it/purrs/>PURRS: Parma University Recurrence Relation Solver</a>
  I wanted better tactics for solving recurrences in Coq, which led me into
  a rabbit hole of the technology of recurrence relation solving. This was the
  newest <i>stable</i> reference to a complete tool that I was able to find. Their
  references section is invaluable, since it's been vetted by them
  actually implementing this tool!</li></ul>
<ul><li> <a href=https://www21.in.tum.de/~nipkow/TRaAT/>Term rewriting and all that</a>.
  I read this book purely for its description of Groebner bases and the Bucchberger
  algorithm in a way that <i>made sense</i> for the first time.
  <a href=http://bollu.github.io/#what-the-hell-is-a-grobner-basis-ideals-as-rewrite-systems>I've written about this more extensively before</a>
  so I'm not going to repeat myself here. In general, I think it's a great book
  that's worth reading, if nothing else, for at least the chapter on Groebner
  bases.</li></ul>
<ul><li> <a href=http://worrydream.com/refs/Wadge%20-%20Lucid,%20the%20Dataflow%20Programming%20Language.pdf>Lucid: The dataflow programming language</a>
  This document is the user manual of Lucid. I didn't fully understand the
  book, but what I understood as their main argument is that full access too
  looping is un-necessary to perform most of the tasks that we do. Rather,
  one can provide a "rich enough" set of combinators to manipulate streams
  that allows one to write all programs worthy of our interest.</li></ul>
<ul><li> <a href=https://hal.inria.fr/inria-00548290/document>Bundle Adjustment — A Modern Synthesis</a>
  I learnt about Bundle Adjustment from a friend taking a course on robotics.
  The general problem is to reconstruct the 3D coordinates of a point cloud
  given 2D projections of the points and the camera parameters, as the camera
  moves in time. I found the paper interesting since it winds up invoking a
  decent amount of differential geometric and gauge theoretic language to
  describe the problem at hand. I was unable to see why this vocabulary helped
  in this use-case, but perhaps I missed the point of the paper. It was hard to
  tell.</li></ul>
<h4><a id=conclusions href='#conclusions'> § </a> Conclusions</h4>
I always feel a little wrong posting this at the end of every year, since I
feel that among the things I cover under "read", I've internalized some things
far better than others: For example, I feel I understannd Riemannian geometry
far better than I do General Relativity. I try to put up the caveats at the
beginning of each section, but I'd really like a way to communicate my
confidence without reducing readability.
The final thing that I wish for is some kind of reading group? It's hard
to maintain a group when my interests shift as rapidly as they do, which
was one of the reason I really loved the AIRCS workshop: They were people
who were working on formal methods, compilers, type theory, number theory,
embedded systems, temporal logic... It was very cool to be in a group of
people who had answers and intuitions to questions that had bugged me for
some time now. I wonder if attending courses at a larger research university
feels the same way. My uni is good, but we have quite small population, which
almost by construction means reduced diversity.
I also wish that I could openly add more references to repos I've been working
on for a while now, but I can't due to the nature of academia and publishing.
This one bums me out, since there's a long story of a huge number of commits
and trial-by-fire that I think I'll be too exhausted to write about once the
thing is done.
Sometimes, I also wish that I could spend the time I spend reading <i>disparate</i>
topics on <i>focused reading on one topic</i>. Unfortunately, I feel like I'm not
wired this way, and the joy I get from sampling many things at the same time
and making connections is somehow much deeper than the joy I get by deeply
reading one topic (in exclusion of all else). I don't know what this says
about my chances as a grad student in the future <code>:)</code>.
<h3><a id=a-motivation-for-p-adic-analysis href='#a-motivation-for-p-adic-analysis'> § </a> <a href=#a-motivation-for-p-adic-analysis>A motivation for p-adic analysis</a></h3>
I've seen the definitions of p-adic numbers scattered around on the internet,
but this analogy as motivated by the book
<a href=https://www.springer.com/gp/book/9783540629115>p-adic numbers by Fernando Gouvea</a>
really made me understand why one would study the p-adics, and why the
definitions are natural. So I'm going to recapitulate the material, with the
aim of having somoene who reads this post be left with a sense of why it's
profitable to study the p-adics, and what sorts of analogies are fruitful when
thinking about them.
We wish to draw an analogy between the ring <span class='latexinline'>\mathbb C[X]</span>, where <span class='latexinline'>(X - \alpha)</span>
are the prime ideals, and <span class='latexinline'>\mathbb Z</span> where <span class='latexinline'>(p)</span> are the prime ideals. We wish
to take all operations one can perform with polynomials, such as generating
functions (<span class='latexinline'>1/(X - \alpha) = 1 + X + X^2 + \dots</span> ),
taylor expansions (expanding aronund <span class='latexinline'>(X - \alpha)</span>),
and see what their analogous objects will look like in <span class='latexinline'>\mathbb Z</span>
relative to a prime <span class='latexinline'>p</span>.
<h4><a id=perspective-taylor-series-as-writing-in-base-p href='#perspective-taylor-series-as-writing-in-base-p'> § </a> Perspective: Taylor series as writing in base <span class='latexinline'>p</span>:</h4>
Now, for example, given a prime <span class='latexinline'>p</span>, we can write any positive integer <span class='latexinline'>m</span>
in base <span class='latexinline'>p</span>, as <span class='latexinline'>(m = \sum_{i=0}^n a_i p^i)</span> where <span class='latexinline'>(0 \leq a_i \leq p - 1)</span>.
For example, consider <span class='latexinline'>m = 72, p = 3</span>. The expansion of 72 is
<span class='latexinline'>72 = 0\times 1 + 0 \times 3 + 2 \times 3^2 + 2 \times 3^3</span>.
This shows us that 72 is divisible by <span class='latexinline'>3^2</span>.
This perspective to take is that this us the information local to prime <span class='latexinline'>p</span>,
about what order the number <span class='latexinline'>m</span> is divisible by <span class='latexinline'>p</span>,
just as the taylor expansion tells us around <span class='latexinline'>(X - \alpha)</span> of a polynomial <span class='latexinline'>P(X)</span>
tells us to what order <span class='latexinline'>P(X)</span> vanishes at a point <span class='latexinline'>\alpha</span>.
<h4><a id=perspective-rational-numbers-and-rational-functions-as-infinite-series href='#perspective-rational-numbers-and-rational-functions-as-infinite-series'> § </a> Perspective: rational numbers and rational functions as infinite series:</h4>
Now, we investigate the behaviour of expressions such as
<ul><li> <span class='latexinline'>P(X) = 1/(1+X) = 1 - X + X^2 -X^3 + \dots</span>.</li></ul>
We know that the above formula is correct formally from the theory of
generating functions.  Hence, we take inspiration to define values for
<i>rational numbers</i>.
Let's take <span class='latexinline'>p \equiv 3</span>, and we know that <span class='latexinline'>4 = 1 + 3 = 1 + p</span>.
We now calculate <span class='latexinline'>1/4</span> as:
<div class='latexblock'>
1/4 = 1/(1+p) = 1 - p + p^2 - p^3 + p^4 - p^5 + p^6 + \cdots
</div>
However, we don't really know how to interpret <span class='latexinline'>(-1 \cdot p)</span>, since we assumed
the coefficients are always non-negative. What we can do is to rewrite <span class='latexinline'>p^2 = 3p</span>,
and then use this to make the coefficient positive. Performing this transformation
for every negative coefficient, we arrive at:
<div class='latexblock'>
\begin{align*}
1/4 &= 1/(1+p) = 1 - p + p^2 - p^3 + p^4 + \cdots \\
&= 1 + (- p + 3p) + (- p^3 + 3p^3)  +  \cdots \\
&= 1 + 2p + 2p^3 + \cdots
\end{align*}
</div>
We can verify that this is indeed correct, by multiplying with <span class='latexinline'>4 = (1 + p)</span>
and checking that the result is <span class='latexinline'>1</span>:
<div class='latexblock'>
\begin{align*}
&(1 + p)(1 + 2p + 2p^3 + \cdots) \\
&= (1 + p) + (2p + 2p^2) + (2p^3 + 2p^4) + \cdots \\
&= 1 + 3p + 2p^2 + 2p^3 + 2p^4 + \cdots \\
&\text{(Rewrite $3p = p \cdot p = p^2$)} \\
&= 1 + (p^2 + 2p^2) + 2p^3 + 2p^4 + \cdots \\
&= 1 + 3p^2 + 2p^3 + 2p^4 + \cdots \\
&\text{(Rewrite $3p^2 = p^3$ and collect $p^3$)} \\
&= 1 + 3p^3 + 2p^4 + \cdots \\
&= 1 + 3p^4 + \cdots \\
&= 1 + \cdots = 1
\end{align*}
</div>
What winds up happening is that all the numbers after <span class='latexinline'>1</span> end up being cleared
due to the carrying of <span class='latexinline'>(3p^i \mapsto p^{i+1})</span>.
This little calculation indicates that we can also define take the <span class='latexinline'>p</span>-adic
expansion of <i>rational numbers</i>.
<h4><a id=perspective--1-as-a-p-adic-number href='#perspective--1-as-a-p-adic-number'> § </a> Perspective: -1 as a p-adic number</h4>
We next want to find a p-adic expansion of -1, since we can then expand
out theory to work out "in general". The core idea is to "borrow" <span class='latexinline'>p</span>, so
that we can write -1 as <span class='latexinline'>(p - 1) - p</span>, and then we fix <span class='latexinline'>-p</span>, just like we fixed
<span class='latexinline'>-1</span>. This eventually leads us to an infinite series expansion for <span class='latexinline'>-1</span>. Written
down formally, the calculation proceeds as:
<div class='latexblock'>
\begin{align*}
-1 &= -1 + p - p  \qquad \text{(borrow $p$, and subtract to keep equality)} \\
&= (p - 1) - p \qquad \text{(Now we have a problem of $-p$)} \\
&= (p - 1) - p + p^2 - p^2  \\
&= (p - 1) + p(p - 1) - p^2 \\
&= (p - 1) + p(p - 1) - p^2 + p^3 - p^3 \\
&= (p - 1) + p(p - 1) + p^2(p - 1) - p^3 \\
&\text{(Generalizing the above pattern)} \\
-1 &= (p - 1) + p(p - 1) + p^2(p - 1) + p^3(p - 1) + p^4(p - 1) + \cdots \\
\end{align*}
</div>
This now gives us access to negative numbers, since we can formally multiply
the series of two numbers, to write <span class='latexinline'>-a = -1 \cdot a</span>.
Notice that this definition of <span class='latexinline'>-1</span> also curiously matches the 2s complement
definition, where we have <span class='latexinline'>-1 = 11\dots 1</span>. In this case, the expansion is
<i>infinite</i>, while in the 2s complement case, it is finite. I would be very
interested to explore this connection more fully.
<h4><a id=what-have-we-achieved-so-far href='#what-have-we-achieved-so-far'> § </a> What have we achieved so far?</h4>
We've now managed to completely reinterpret all the numbers we care about in
the rationals as power series in base <span class='latexinline'>p</span>. This is pretty neat. We're next
going to try to <i>complete</i> this, just as we complete the rationals to get
the reals. We're going to show that we get a <i>different</i> number system on
completion, called <span class='latexinline'>\mathbb Q_p</span>.
To perform this, we first look at how the <span class='latexinline'>p</span>-adic numbers help us solve
congruences mod p, and how this gives rise to completions to equations such
as <span class='latexinline'>x^2 - 2 = 0</span>, which in the reals give us <span class='latexinline'>x = \sqrt 2</span>, and in <span class='latexinline'>\mathbb Q_p</span>
give us a different answer!
<h4><a id=solving-x2-equiv-25-mod-3n href='#solving-x2-equiv-25-mod-3n'> § </a> Solving <span class='latexinline'>X^2 \equiv 25 \mod 3^n</span></h4>
Let's start by solving an equation we already know how to solve:
<span class='latexinline'>X^2 \equiv 25 \mod 3^n</span>.
We already know the solutions to <span class='latexinline'>X^2 \equiv 25 \mod 3^n</span> in <span class='latexinline'>\mathbb Z</span> are
<span class='latexinline'>X \equiv \pm 5 \mod 3^n</span>.
Explicitly, the solutions are:
<ul><li> <span class='latexinline'>X \equiv 3 \mod 3</span></li><li> <span class='latexinline'>X \equiv 5 \mod 9</span></li><li> <span class='latexinline'>X \equiv 5 \mod 27</span></li><li> At this point, the answer remains constant.</li></ul>
This was somewhat predictable. We move to a slightly more interesting case.
<h4><a id=solving-x--5-mod-3n href='#solving-x--5-mod-3n'> § </a> Solving <span class='latexinline'>X = -5 \mod 3^n</span></h4>
The solution sets are:
<ul><li> <span class='latexinline'>X \equiv -5 \equiv 1 \mod 3</span></li><li> <span class='latexinline'>X \equiv -5 \equiv 4 = 1 + 3 \mod 9</span></li><li> <span class='latexinline'>X \equiv -5 \equiv 22 = 1 + 3 + 2 \cdot 9 \mod 27</span></li><li> <span class='latexinline'>X \equiv -5 \equiv 76 = 1 + 3 + 2 \cdot 9 + 2 \cdot 27 \mod 81</span></li></ul>
This gives us the infinite 3-adic expansion:
<ul><li> <span class='latexinline'>X = -5 = 1 + 1\cdot 3 + 2\cdot 3^2 + 2\cdot 3^3 + \dots</span></li></ul>
Note that we can't really <i>predict</i> the digits in the 3-adic sequence of -5,
but we can keep expanding and finding more digits.
Also see that the solutions are "coherent". In that, if we look at the
solution mod 9, which is <span class='latexinline'>4</span>, and then consider it mod 3, we get <span class='latexinline'>1</span>. So,
we can say that given a sequence of integers <span class='latexinline'>0 \leq \alpha_n \leq p^n - 1</span>,
<b><span class='latexinline'>\alpha_n</span> is p-adically coherent sequence</b> iff:
<ul><li> <span class='latexinline'> \alpha_{n+1} = \alpha_n \mod p^n</span>.</li></ul>
<h4><a id=viewpoint-solution-sets-of-x2-25-mod-3n href='#viewpoint-solution-sets-of-x2-25-mod-3n'> § </a> Viewpoint: Solution sets of <span class='latexinline'>X^2 = 25 \mod 3^n</span></h4>
Since our solution sets are coherent, we can view the solutions as a tree,
with the expansions of <span class='latexinline'>X = 5, X = -5 \mod 3</span> and then continuing onwards
from there. That is, the sequences are
<ul><li> <span class='latexinline'>2 \rightarrow 5 \rightarrow 5 \rightarrow 5 \rightarrow \dots</span></li><li> <span class='latexinline'>1 \rightarrow 4 \rightarrow 22 \rightarrow 76 \rightarrow \dots</span></li></ul>
<h4><a id=solving-x2-equiv-2-mod-7n href='#solving-x2-equiv-2-mod-7n'> § </a> Solving <span class='latexinline'>X^2 \equiv 2 \mod 7^n</span></h4>
We now construct a solution to the equation <span class='latexinline'>X^2 = 1</span> in the 7-adic system,
thereby showing that <span class='latexinline'>\mathbb Q_p</span> is indeed strictly <i>larger</i> than <span class='latexinline'>\mathbb Q</span>,
since this equation does not have rational roots.
For <span class='latexinline'>n=1</span>, we have the solutions as <span class='latexinline'>X \equiv 3 \mod 7</span>, <span class='latexinline'>X \equiv 4 \equiv -3 \mod 7</span>.
To find solutions for <span class='latexinline'>n = 2</span>, we recall that we need our solutions to be consistent
with those for <span class='latexinline'>n = 1</span>. So, we solve for:
<ul><li> <span class='latexinline'>(3 + 7k)^2 = 2 \mod 49</span>, <span class='latexinline'>(4 + 7k)^2 = 2 \mod 49</span>.</li></ul>
Solving the first of these:
<div class='latexblock'>
\begin{align*}
(3 + 7k)^2 &\equiv 2 \mod 49 \\
9 + 42 k + 49k^2 &\equiv 2 \mod 49 \\
9 + 42 k + 0k^2 &\equiv 2 \mod 49 \\
7 + 42 k &\equiv 0 \mod 49 \\
1 + 6 k &\equiv 0 \mod 49 \\
k &\equiv 1 \mod 7
\end{align*}
</div>
This gives the solution <span class='latexinline'>X \equiv 10 \mod 49</span>. The other branch (<span class='latexinline'>X = 4 + 7k</span>)
gives us <span class='latexinline'>X \equiv 39 \equiv -10 \mod 49</span>.
We can continue this process indefinitely (<i>exercise</i>), giving us the sequences:
<ul><li> <span class='latexinline'>3 \rightarrow 10 \rightarrow 108 \rightarrow 2166 \rightarrow \dots</span></li><li> <span class='latexinline'>4 \rightarrow 39 \rightarrow 235 \rightarrow 235 \rightarrow \dots</span></li></ul>
We can show that the sequences of solutions we get satisfy the equation
<span class='latexinline'>X^2 = 2 \mod 7</span>. This is so by construction. Hence, <span class='latexinline'>\mathbb Q_7</span> contains
a solution that <span class='latexinline'>\mathbb Q</span> does not, and is therefore strictly bigger, since
we can already represent every rational in <span class='latexinline'>\mathbb Q</span> in <span class='latexinline'>\mathbb Q_7</span>.
<h4><a id=use-case-solving-x-1-3x-as-a-recurrence href='#use-case-solving-x-1-3x-as-a-recurrence'> § </a> Use case: Solving <span class='latexinline'>X = 1 + 3X</span> as a recurrence</h4>
Let's use the tools we have built so far to solve the equation <span class='latexinline'>X = 1 + 3X</span>.
Instead of solving it using algebra, we look at it as a recurrence <span class='latexinline'>X_{n+1} = 1 + 3X_n</span>.
This gives us the terms:
<ul><li> <span class='latexinline'>X_0 = 1</span></li><li> <span class='latexinline'>X_1 = 1 + 3</span></li><li> <span class='latexinline'>X_2 = 1 + 3 + 3^2</span></li><li> <span class='latexinline'>X_n = 1 + 3 + \dots + 3^n</span></li></ul>
In <span class='latexinline'>\mathbb R</span>, this is a divergent sequence. However, we know that the
solution so <span class='latexinline'>1 + X + X^2 + \dots = 1/(1-X)</span>, at least as a generating function.
Plugging this in, we get that the answer should be:
<ul><li> <span class='latexinline'>1/(1 - 3) = -1/2</span></li></ul>
which is indeed the correct answer.
Now this required some really shady stuff in <span class='latexinline'>\mathbb R</span>. However, with a change
of viewpoint, we can explain what's going on. We can look at the above series
as being a series in <span class='latexinline'>\mathbb Q_3</span>.  Now, this series does <i>really</i> converge,
and by the same argument as above, it converges to <span class='latexinline'>-1/2</span>.
The nice thing about this is that a dubious computation becomes a legal one
by changing one's perspective on where the above series lives.
<h4><a id=viewpoint-evaluation-for-p-adics href='#viewpoint-evaluation-for-p-adics'> § </a> Viewpoint: 'Evaluation' for p-adics</h4>
The last thing that we need to import from the theory of polynomials
is the ability to <i>evaluate</i> them: Given a rational function <span class='latexinline'>F(X) = P(X)/Q(X)</span>,
where <span class='latexinline'>P(X), Q(X)</span> are polynomials, we can
evaluate it at some arbitrary point <span class='latexinline'>x_0</span>, as long as <span class='latexinline'>x_0</span> is not a zero
of the polynomial <span class='latexinline'>Q(X)</span>.
We would like a similar function, such that for a fixed prime <span class='latexinline'>p</span>, we obtain
a ring homomorphism from <span class='latexinline'>\mathbb Q \rightarrow \mathbb F_p^x</span>, which we will
denote as <span class='latexinline'>p(x_0)</span>, where we are imagining that we are "evaluating" the prime
<span class='latexinline'>p</span> against the rational <span class='latexinline'>x_0</span>.
We define the value of <span class='latexinline'>x_0 = a/b</span> at the prime <span class='latexinline'>p</span> to be equal to
<span class='latexinline'>ab^{-1} \mod p</span>, where <span class='latexinline'>b b^{-1} \equiv 1 \mod p</span>. That is, we compute the
usual <span class='latexinline'>ab^{-1}</span> to evaluate <span class='latexinline'>a/b</span>, except we do this <span class='latexinline'>(\mod p)</span>, to stay with
the analogy.
Note that if <span class='latexinline'>b \equiv 0 \mod p</span>, then we cannot evaluate
the rational <span class='latexinline'>a/b</span>, and we say that <span class='latexinline'>a/b</span> has a pole at <span class='latexinline'>p</span>. The order
of the pole is the number of times <span class='latexinline'>p</span> occurs in the prime factorization of <span class='latexinline'>b</span>.
I'm not sure how profitable this viewpoint is, so I
<a href=https://math.stackexchange.com/questions/3483369/profit-of-definition-evaluation-of-a-rational-at-a-p-adic>asked on math.se</a>,
and I'll update this post when I recieve a good answer.
<h4><a id=perspective-forcing-the-formal-sum-to-converge-by-imposing-a-new-norm href='#perspective-forcing-the-formal-sum-to-converge-by-imposing-a-new-norm'> § </a> Perspective: Forcing the formal sum to converge by imposing a new norm:</h4>
So far, we have dealt with infinite series in base <span class='latexinline'>p</span>, which have terms
<span class='latexinline'>p^i, i \geq 0</span>.
Clearly, these sums are divergent as per the usual topology on <span class='latexinline'>\mathbb Q</span>.
However, we would enjoy assigning analytic meaning to these series. Hence, we
wish to consider a new notion of the absolute value of a number, which makes it
such that <span class='latexinline'>p^i</span> with large <span class='latexinline'>i</span> are considered small.
We define the absolute value for a field <span class='latexinline'>K</span> as a function
<span class='latexinline'>|\cdot |: K \rightarrow \mathbb R</span>. It obeys the axioms:
<ol><li> <span class='latexinline'>\lvert x \rvert = 0 \iff x = 0</span></li><li> <span class='latexinline'>\lvert xy \rvert =  \lvert x \rvert  \lvert y \rvert</span> for all <span class='latexinline'>x, y \in K</span></li><li> <span class='latexinline'>\lvert x + y \rvert \leq \lvert x \rvert + \lvert y \rvert</span>, for all <span class='latexinline'>x, y \in K</span>.</li></ol>
We want the triangle inequality so it's metric-like, and the norm to be
multiplicative so it measures the size of elements.
The usual absolute value <span class='latexinline'>\lvert x \rvert \equiv \\{ x : x \geq 0; -x : ~ \text{otherwise} \\}</span> satisfies
these axioms.
Now, we create a new absolute value that measures primeness. We first introduce
a gadget known as a valuation, which measures the <span class='latexinline'>p</span>-ness of a number. We use
this to create a norm that makes number smaller as their <span class='latexinline'>p</span>-ness increases.
This will allow infinite series in <span class='latexinline'>p^i</span> to converge.
<h4><a id=p-adic-valuation-definition href='#p-adic-valuation-definition'> § </a> p-adic valuation: Definition</h4>
First, we introduce
a valuation <span class='latexinline'>v_p: \mathbb Z - \\{0\\} \rightarrow \mathbb R</span>, where <span class='latexinline'>v_p(n)</span> is
the power of the prime <span class='latexinline'>p^i</span> in the prime factorization of <span class='latexinline'>n</span>. More formally,
<span class='latexinline'>v_p(n)</span> is the unique number such that:
<ul><li> <span class='latexinline'>n = p^{v_p(n)} m</span>, where <span class='latexinline'>p \nmid m</span>.</li><li> We extend the valuation to the rationals by defining <span class='latexinline'>v_p(a/b) = v_p(a) - v_p(b)</span>.</li><li> We set <span class='latexinline'>v_p(0) = +\infty</span>. The intuition is that <span class='latexinline'>0</span> can be divided by <span class='latexinline'>p</span>
  an infinite number of times.</li></ul>
The valuation gets larger as we have larger powers of <span class='latexinline'>p</span> in the prime
factorization of a number. However, we want the norm to get <i>smaller</i>. Also,
we need the norm to be multiplicative, while <span class='latexinline'>v_p(nm) = v_p(n) + v_p(m)</span>, which
is additive.
To fix both of these, we create a norm by exponentiating <span class='latexinline'>v_p</span>.
This converts the additive property into a multiplicative property. We
exponentiate with a negative sign so that higher values of <span class='latexinline'>v_p</span> lead to
smaller values of the norm.
<h4><a id=p-adic-abosolute-value-definition href='#p-adic-abosolute-value-definition'> § </a> p-adic abosolute value: Definition</h4>
Now, we define the <b>p-adic absolute value</b> of a number <span class='latexinline'>n</span> as
<span class='latexinline'>|n|_p \equiv p^{-v_p(n)}</span>.
<ul><li> the norm of <span class='latexinline'>0</span> is <span class='latexinline'>p^{-v_p(0)} = p^{-\infty} = 0</span>.</li><li> If <span class='latexinline'>p^{-v_p(n)} = 0</span>, then <span class='latexinline'>-v_p(n) = \log_p 0 = -\infty</span>, and hence <span class='latexinline'>n = 0</span>.</li><li> The norm is multiplicative since <span class='latexinline'>v_p</span> is additive.</li><li> Since <span class='latexinline'>v_p(x + y) \geq \min (v_p(x), v_p(y)), |x + y|_p \leq max(|x|_p, |y|_p) \leq |x|_p + |y|_p</span>.
  Hence, the triangle inequality is also satisfied.</li></ul>
So <span class='latexinline'>|n|_p</span> is indeed a norm, which measures <span class='latexinline'>p</span>-ness, and is smaller as <span class='latexinline'>i</span>
gets larger in the power <span class='latexinline'>p^i</span> of the factorization of <span class='latexinline'>n</span>, causing our
infinite series to converge.
There is a question of why we chose a base <span class='latexinline'>p</span> for <span class='latexinline'>|n|_p = p^{v_p(n)}</span>. It would
appear that any choice of <span class='latexinline'>|n|_p = c^{v_p(n)}, c > 1</span> would be legal.
<a href=https://math.stackexchange.com/questions/3482489/why-does-the-p-adic-norm-use-base-p>I asked this on <code>math.se</code></a>,
and the answer is that this choosing a base <span class='latexinline'>p</span> gives us the nice formula
<div class='latexblock'>
\forall x \in \mathbb Z, \prod_{\{p : p~\text{is prime}\} \cup \{ \infty \}} |x|_p = 1
</div>
That is, the product of all <span class='latexinline'>p</span> norms and the usual norm
(denoted by <span class='latexinline'>\lvert x \rvert_\infty </span> )
give us the number 1. The reason is that the <span class='latexinline'> \lvert x\rvert_p </span> give us
multiples <span class='latexinline'>p^{-v_p(x)}</span>,
while the usual norm <span class='latexinline'>\lvert x \rvert_\infty</span> contains a multiple
<span class='latexinline'>p^{v_p(x)}</span>, thereby cancelling each other out.
<h4><a id=conclusion href='#conclusion'> § </a> Conclusion</h4>
What we've done in this whirlwind tour is to try and draw analogies between
the ring of polynomials <span class='latexinline'>\mathbb C[X]</span> and the ring <span class='latexinline'>\mathbb Z</span>, by trying
to draw analogies between their prime ideals: <span class='latexinline'>(X - \alpha)</span> and <span class='latexinline'>(p)</span>. So,
we imported the notions of generating functions, polynomial evaluation, and
completions (of <span class='latexinline'>\mathbb Q</span>) to gain a picture of what <span class='latexinline'>\mathbb Q_p</span> is like.
We also tried out the theory we've built against some toy problems, that shows
us that this point of view maybe profitable. If you found this interesting,
I highly recommend the book
<a href=https://www.springer.com/gp/book/9783540629115>p-adic numbers by Fernando Gouvea</a>.
<h3><a id=line-of-investigation-to-build-physical-intuition-for-semidirect-products href='#line-of-investigation-to-build-physical-intuition-for-semidirect-products'> § </a> <a href=#line-of-investigation-to-build-physical-intuition-for-semidirect-products>Line of investigation to build physical intuition for semidirect products</a></h3>
To quote wikipedia:
<blockquote> In crystallography, the space group of a crystal splits as the semidirect product of the point group and the translation group if and only if the space group is symmorphic</blockquote>
The if and only if is interesting: The geometry ofthe crystal lattice truly
appears to capture the structure of the semidirect product. It's a discrete
object as well, which makes it way easier to visualize. I'm going to hunt down
the definitions involved so I can finally feel like I truly understand semidirect
products from the "action" perspective.
<h3><a id=topology-is-really-about-computation--part-2 href='#topology-is-really-about-computation--part-2'> § </a> <a href=#topology-is-really-about-computation--part-2>Topology is really about computation --- part 2</a></h3>
Here, we're going to describe whatever I've picked up of sheaves in the past
couple of weeks. I'm trying to understand the relationship between sheaves,
topoi, geometry, and logic. I currently see how topoi allows us to model logic,
and how sheaves allow us to model geometry, but I see nothing about the
relationship! I'm hoping that writing this down will allow me to gain some
perspective on this.
<h3><a id=what-is-a-sheaf href='#what-is-a-sheaf'> § </a> What is a sheaf?</h3>
Let's consider two sets <span class='latexinline'>P, A</span>, <span class='latexinline'>P \subseteq A</span>. Now, given a function
<span class='latexinline'>f: A \rightarrow X</span>, we can restrict this function to <span class='latexinline'> A_P: P \rightarrow X </span>.
So, we get to <i>invert the direction</i>:
<div class='latexblock'>
(P \subseteq A) \iff (f: A \rightarrow X) \rightarrow (f_P: P \rightarrow X)
</div>
We should now try to discover some sort of structure to this "reversal"
business. Perhaps we will discover a contravariant functor! (Spoiler: we will).
<h3><a id=topology-is-really-about-computation--part-1 href='#topology-is-really-about-computation--part-1'> § </a> <a href=#topology-is-really-about-computation--part-1>Topology is really about computation --- part 1</a></h3>
Most people believe that topology is about some notion of "nearness" or
"closeness", which has been abstracted out from our usual notion of
continuity that we have from a metric space. Here, I make the claim that
topology is really about <i>computation</i>, and more specifically, <i>decidability</i>.
These are not new ideas. I learnt of this from a monograph 
<a href=https://www.cs.bham.ac.uk/~mhe/papers/entcs87.pdf>Synthetic topology of data types and classical spaces. Martın Escardo</a>.
This does not seem very well known, so I decided to write about it.
The idea is this: We have turing machines which can compute things. We then
also have a set <span class='latexinline'>S</span>. Now, a topology <span class='latexinline'>\tau \subset 2^S</span> precisely encodes
which of the subsets of <span class='latexinline'>S</span> can be separated from the rest of the space by a turing machine.
Thus, a discrete space is a very nice space, where every point can be separated
from every other point. An indescrete space is one where no point can be separated.
Something like the reals is somewhere in between, where we can separate
stuff inside an open interval from stuff clearly outside, but there's some
funny behaviour that goes on at the boundary due to things like <code>(0.999... = 1)</code>,
which we'll see in detail in a moment.
<h4><a id=semidecidability href='#semidecidability'> § </a> Semidecidability</h4>
A subset <span class='latexinline'>Q\subseteq S</span> is <i>semidecidable</i>, if there exists a turing machine
<span class='latexinline'>\hat Q: Q \rightarrow \{ \bot, \top \}</span>, such that:
<div class='latexblock'>
\begin{align*}
\hat Q(q) = \top \iff q \in Q \\
\hat Q(q) = \bot \iff q \notin Q \\
\end{align*}
</div>
Where <span class='latexinline'>\top</span> signifies stopping at a state and returning <code>TRUE</code>, and
<span class='latexinline'>\bot</span> signifies <i>never halting at all</i>!. So, the subset <span class='latexinline'>Q</span> is
<i>semidedicable</i>, in that, we will halt and say <code>TRUE</code> if the element
belongs in the set. But if an element does not belong in the set, we are
supposed to never terminate.
<h4><a id=deep-dive-semidecidability-of-the-interval-1-2 href='#deep-dive-semidecidability-of-the-interval-1-2'> § </a> Deep dive: semidecidability of the interval <span class='latexinline'>(1, 2)</span></h4>
Let's start with an example. We consider the interval <span class='latexinline'>I = (1, 2)</span>, as a
subset of <span class='latexinline'>\mathbb{R}</span>.Let the turing machine recieve the real number
as a function <span class='latexinline'>f: \mathbb N \rightarrow \{0, 1, \dots 9\}</span>, such that
given a real number <span class='latexinline'>{(a_0 \cdot a_1 \cdot a_2 \dots)}</span>, this is encoded as a
function <span class='latexinline'>{f_a(i) = a_i}</span>.
We now build a turing machine <span class='latexinline'>\hat I</span> which when given the input the function <span class='latexinline'>f_a</span>,
semi-decides whether <span class='latexinline'>{a \in I}</span>.
Let's consider the numbers in <span class='latexinline'>I</span>:
<div class='latexblock'>
\begin{align*}
&0 \rightarrow \texttt{NO} \\
&0.\overline{9} \rightarrow \texttt{NO} \\
&1.00\dots \rightarrow \texttt{NO} \\
&1.a_1 a_2 \dots \rightarrow \texttt{YES} \\
&1.\overline{9} \rightarrow \texttt{NO} \\
&2.0 \rightarrow \texttt{NO} \\
&2.a_1 a_2 \rightarrow \texttt{NO}
\end{align*}
</div>
So, we can write a turing machine (ie, some code) that tries to decide whether
a real number <span class='latexinline'>a</span>'s encoding <span class='latexinline'>f_a</span> belongs to the interval <span class='latexinline'>I = (1, 2)</span>
as follows:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Hence, we say that the interval <span class='latexinline'>I = (1, 2)</span> is <i>semi-decidable</i>, since we
have a function
<span class='latexinline'>\hat I \equiv \texttt{decide-number-in-open-1-2}</span>
such that
<span class='latexinline'>\hat I (f_a) \text{ terminates } \iff a \in I</span>.
We don't make <i>any claim</i> about
what happens if <span class='latexinline'>a \notin I</span>. This is the essence of semidecidability: We
can precisely state when elements in the set belong to the set, but not
when they don't.
<h4><a id=semi-decidability-in-general href='#semi-decidability-in-general'> § </a> Semi decidability in general</h4>
To put this on more solid ground, we define a topology on a set <span class='latexinline'>S</span> by considering
programs which recieve as input elements of <span class='latexinline'>S</span>, suitably encoded. For example,
the way in which we encoded real numbers as functions from the index to the
digit. Similarly, we encode other mathematical objects in some suitable way.
Now, we define:
<ul><li> For every program <span class='latexinline'>P</span> which takes as inputs elements in <span class='latexinline'>S</span>, the set
  <span class='latexinline'>{halts(P) \equiv \\{ s \in S \vert P(s) \text{halts} \\}}</span> is called as a
  <i>semidecidable set</i>.</li></ul>
<ul><li> Alternatively, we can say for a subset <span class='latexinline'>{T \subset S}</span>, if there
  exists a program <span class='latexinline'>{\hat T}</span>, such that
  <span class='latexinline'>{s \in T \iff \hat T(s) \text{ halts}}</span>, then <span class='latexinline'>T</span> is semi-dedecidable.</li></ul>
These are just two viewpoints on the same object. In one, we define the
set based on the program. In the other, we define the program based on the
set.
<h4><a id=semi-decidability-of-the-empty-set-and-the-universe-set href='#semi-decidability-of-the-empty-set-and-the-universe-set'> § </a> Semi decidability of the empty set and the universe set.</h4>
The empty set is semi-decidable, due to the existence of the program:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
The universe set is semi-decidable, due to the existence of the program:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=semi-decidability-of-the-union-of-sets href='#semi-decidability-of-the-union-of-sets'> § </a> Semi decidability of the union of sets</h4>
infinite unions of sets are semi decidable, since we can "diagonalize" on
the steps of all programs. That way, if any program halts, we will reach
the state where it halts in our diagonalized enumeration.
Let <code>A00, A01... A0n</code> be the initial states of the machines. We are trying to
semidecide whether any of them halt. We lay out the steps of the machines
in an imaginary grid:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
For example, machine <code>A0</code> has states:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
We can walk through the combined state-space of the machines as:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Where on the <code>k</code>'th line, we collect all states <span class='latexinline'>A_{ij}</span> such that <span class='latexinline'>(i + j = k)</span>.
Now, if any of the machines have a state that is <code>HALT</code>, we will reach the
state as we enumerate the diagonals, and the machine that explores the
combined state space can also return <code>HALT</code>.
<h4><a id=semi-decidability-of-the-intersection-of-sets href='#semi-decidability-of-the-intersection-of-sets'> § </a> Semi decidability of the intersection of sets</h4>
infinite intersections of sets are <i>not</i> semi decidable, since by running
these programs in parallel, we cannot know if an infinite number of programs
halt in finite time. We <i>can</i> tell if <i>one</i> of them halts, but of if <i>all</i>
of them halt.
For example, consider the sequence of machines produced by <code>machine_creator</code>:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
We wish to check if the intersection of all <code>machine_creator(n)</code> halt, for all
<span class='latexinline'>n \geq 0, n \in \mathbb N</span>. Clearly, the answer is an infinite number of steps,
even though every single machine created by <code>machine_creator</code> halts in a
finite number of steps.
<h4><a id=an-even-deeper-dive-re-examining-the-topology-of-the-reals href='#an-even-deeper-dive-re-examining-the-topology-of-the-reals'> § </a> An even deeper dive: re-examining the topology of the reals</h4>
We generally think of the topology of reals as being generated from the
base of intervals <span class='latexinline'>(a, b)</span>. But really, this is a perverse perspective
from the point of view of computation.
Structurally speaking, the only comparison operator we have on the reals
is a <span class='latexinline'><</span> operator. So we should ideally start by taking as a base
the sets <span class='latexinline'>(-\infty, r)</span>. This is elegant, because:
<ol><li> it relates to the order theoretic notion of <a href=https://en.wikipedia.org/wiki/Upper_set>downward closed set</a></li><li> <a href=https://math.stackexchange.com/a/37209/261373>By yoneda</a>, the downward closed set <span class='latexinline'>(-\infty, r)</span> contains the exact same order theoretic information as <span class='latexinline'>r</span> itself.</li><li> It makes our "code" easier to write:</li></ol>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=references href='#references'> § </a> References</h4>
<ul><li> <a href=https://www.cs.bham.ac.uk/~mhe/papers/entcs87.pdf>Synthetic topology of data types and classical spaces. Martın Escardo</a></li></ul>
<h3><a id=pslq-algorithm-finding-integer-relations-between-reals href='#pslq-algorithm-finding-integer-relations-between-reals'> § </a> <a href=#pslq-algorithm-finding-integer-relations-between-reals>PSLQ algorithm: finding integer relations between reals</a></h3>
An algorithm to find <i>integer</i> relations between <i>real</i> numbers. It was
apparently named "algorithms of the century" by Computing in science and
engineering.
<ul><li> <a href=http://mathworld.wolfram.com/PSLQAlgorithm.html>Wolfram link</a></li></ul>
<h3><a id=geometric-characterization-of-normal-subgroups href='#geometric-characterization-of-normal-subgroups'> § </a> <a href=#geometric-characterization-of-normal-subgroups>Geometric characterization of normal subgroups</a></h3>
<blockquote> <span class='latexinline'>Stab(Orb(x)) = Stab(x) \iff Stab(x) \text{ is normal}</span></blockquote>
<blockquote> <span class='latexinline'>\forall x' \in Orb(x), Stab(x') = Stab(x) \iff Stab(x) \text{ is normal}</span></blockquote>
<h4><a id=forward-if-the-stabilizer-is-normal-then-all-elements-in-the-orbit-have-the-same-stabilizer href='#forward-if-the-stabilizer-is-normal-then-all-elements-in-the-orbit-have-the-same-stabilizer'> § </a> Forward: if the stabilizer is normal, then all elements in the orbit have the same stabilizer</h4>
Let a group <span class='latexinline'>G</span> act on a set <span class='latexinline'>X</span> with action <span class='latexinline'>(~\dot~) : G \times X \rightarrow X</span>.
Let <span class='latexinline'>H \subseteq G</span> be the stabilizer of a point <span class='latexinline'>x \in X</span>. Now, let
<span class='latexinline'>K = kHk^{-1}</span>, a conjugacy class of <span class='latexinline'>H</span>. Clearly, the element <span class='latexinline'>(k \cdot x)</span>
in the orbit of <span class='latexinline'>x</span> is stabilized by <span class='latexinline'>K</span>.
If the group <span class='latexinline'>H</span> is normal, then <span class='latexinline'>K = H</span>. So every element in the orbit of <span class='latexinline'>x</span>
is stabilized by <span class='latexinline'>H</span>.
<h4><a id=interaction-of-stablizer-and-the-orbit href='#interaction-of-stablizer-and-the-orbit'> § </a> Interaction of stablizer and the orbit:</h4>
<blockquote> <span class='latexinline'>Stab(g \cdot x) = g Stab(x) g^{-1}</span></blockquote>
<blockquote> <span class='latexinline'>g^{-1} Stab(g \cdot x) g = Stab(x)</span></blockquote>
<ul><li>  Proof of <span class='latexinline'>s \in Stab(x) \implies gsg^{-1} \in Stab(g \cdot x)</span>:
   The action of <span class='latexinline'>gsg^{-1}</span> on <span class='latexinline'>g \cdot x</span> is:
   <span class='latexinline'>(g \cdot x \rightarrow_{g^-1} x \rightarrow_s x \rightarrow_g g \cdot x)</span>.</li></ul>
<ul><li> Proof of <span class='latexinline'>s' \in Stab(g \cdot x) \implies g^{-1}s'g \in Stab(x)</span>:
  The action of <span class='latexinline'>g^{-1}s'g</span> on <span class='latexinline'>x</span> is:
  <span class='latexinline'>(x \rightarrow_{g} g \cdot x \rightarrow_{s'} g \cdot x \rightarrow_{g^{-1}} x)</span>.</li></ul>
Hence, both containments are proved.
<h4><a id=backward-if-all-elements-in-the-orbit-have-the-same-orbit-then-the-stabilizer-is-normal href='#backward-if-all-elements-in-the-orbit-have-the-same-orbit-then-the-stabilizer-is-normal'> § </a> Backward: if all elements in the orbit have the same orbit, then the stabilizer is normal.</h4>
From the above equation <span class='latexinline'>Stab(g \cdot x) = g Stab(x) g^{-1}</span>. If the
entire orbit has the same stabilizer, <span class='latexinline'>Stab (g \cdot x) = Stab(x)</span>. Hence,
we get <span class='latexinline'>Stab(x) = g Stab(x) g^{-1}</span>, proving that it's normal.
<h3><a id=handy-characterization-of-adding-an-element-into-an-ideal-proof-that-maximal-ideal-is-prime href='#handy-characterization-of-adding-an-element-into-an-ideal-proof-that-maximal-ideal-is-prime'> § </a> <a href=#handy-characterization-of-adding-an-element-into-an-ideal-proof-that-maximal-ideal-is-prime>Handy characterization of adding an element into an ideal, proof that maximal ideal is prime</a></h3>
<h4><a id=the-characterization href='#the-characterization'> § </a> The characterization</h4>
Let <span class='latexinline'>I</span> be an ideal. The ideal generated by adding <span class='latexinline'>(a \in R)</span> to <span class='latexinline'>I</span> is
defined as <span class='latexinline'>A \equiv (I \cup \{ a\})</span>. We prove that <span class='latexinline'>A = I + aR</span>.
<div class='latexblock'>
\begin{align*}
&(I \cup \{a \})  \\
&= \quad \{ \alpha i + \beta a | i \in I, \alpha, \beta \in R \} \\
&= \quad \{ i' + \beta a | i' \in I, \alpha, \beta \in R \} \qquad \text{($I$ is closed under multiplication by $R$)} \\
&= I + aR
\end{align*}
</div>
<h4><a id=quotient-based-proof-that-maximal-ideal-is-prime href='#quotient-based-proof-that-maximal-ideal-is-prime'> § </a> Quotient based proof that maximal ideal is prime</h4>
An ideal <span class='latexinline'>P</span> is prime iff the quotient ring <span class='latexinline'>R/P</span> is an integral domain. An
ideal <span class='latexinline'>M</span> is maximal <span class='latexinline'>R/M</span> is a field. Every field is an integral domain,
hence:
<span class='latexinline'>M \text{ is maximal } \implies R/M \text{ is a field } \implies R/M \text {is an integral domain} \implies M \text{ is prime}</span>.
I was dissatisfied with this proof, since it is not ideal theoretic: It argues
about the behaviour of the quotients. I then found this proof that argues
purly using ideals:
<h4><a id=ideal-theoretic-proof-that-maximal-ideal-is-prime href='#ideal-theoretic-proof-that-maximal-ideal-is-prime'> § </a> Ideal theoretic proof that maximal ideal is prime</h4>
<h4><a id=sketch href='#sketch'> § </a> Sketch</h4>
Let <span class='latexinline'>I</span> be a maximal ideal. Let <span class='latexinline'>a, b \in R</span> such that <span class='latexinline'>ab \in I</span>. We need
to prove that <span class='latexinline'>a \in I \lor b \in I</span>. If <span class='latexinline'>a \in I</span>, the problem is done.
So, let <span class='latexinline'>a \notin I</span>. Build ideal <span class='latexinline'>A = (I \cup {a})</span>. <span class='latexinline'>I \subsetneq A</span>. Since
<span class='latexinline'>I</span> is maximal, <span class='latexinline'>A = R</span>. Hence, there are solutions for
<span class='latexinline'>1_R \in A \implies 1_r \in I + aR \implies \exists i \in I, r \in R, 1_R = i  + ar</span>.
Now, <span class='latexinline'>b = b \cdot 1_R = b(i + ar) = bi + (ba)r \in I + IR = I</span>. (<span class='latexinline'>ba \in I</span> by assumption).
Hence, <span class='latexinline'>b \in I</span>.
<h4><a id=details href='#details'> § </a> Details</h4>
let <span class='latexinline'>i</span> be a maximal ideal. let <span class='latexinline'>a, b \in r</span> such that <span class='latexinline'>ab \in i</span>. we need
to prove that <span class='latexinline'>a \in i \lor b \in i</span>.
if <span class='latexinline'>a \in i</span>, then the problem is done. so, let <span class='latexinline'>a \notin i</span>. consider
the ideal <span class='latexinline'>A</span> generated by adding <span class='latexinline'>a</span> into <span class='latexinline'>I</span>. <span class='latexinline'>A \equiv (I \cup \{a\})</span>.
We have shown that <span class='latexinline'>A = I + aR</span>. Hence, <span class='latexinline'>I + a0 = I \subset A</span>.
Also, <span class='latexinline'>0 + ac \dot 1 = a \in A</span>, <span class='latexinline'>a \neq I</span> \implies <span class='latexinline'>A \neq I</span>. Therefore,
<span class='latexinline'>I \subsetneq A</span>. Since <span class='latexinline'>I</span> is maximal, this means that <span class='latexinline'>A = R</span>
Therefore, <span class='latexinline'>I + aR = R</span>. Hence, there exists some <span class='latexinline'>i \in I, r \in R</span> such
that <span class='latexinline'>i + ar = 1_R</span>.
Now, <span class='latexinline'>b = b \cdot 1_R = b \cdot (i + ar) = bi + (ba) r \in I + IR = I</span> Hence,
<span class='latexinline'>b \in I</span>.
<h3><a id=radical-ideals-nilpotents-and-reduced-rings href='#radical-ideals-nilpotents-and-reduced-rings'> § </a> <a href=#radical-ideals-nilpotents-and-reduced-rings>Radical ideals, nilpotents, and reduced rings</a></h3>
<h4><a id=radical-ideals href='#radical-ideals'> § </a> Radical Ideals</h4>
A radical ideal of a ring <span class='latexinline'>R</span> is an ideal such that
<span class='latexinline'>\forall r \in R, r^n \in I \implies r \in I</span>.
That is, if any power of <span class='latexinline'>r</span> is in <span class='latexinline'>I</span>, then the element
<span class='latexinline'>r</span> also gets "sucked into" <span class='latexinline'>I</span>.
<h4><a id=nilpotent-elements href='#nilpotent-elements'> § </a> Nilpotent elements</h4>
A nilpotent element of a ring <span class='latexinline'>R</span> is any element <span class='latexinline'>r</span> such that there exists
some power <span class='latexinline'>n</span> such that <span class='latexinline'>r^n = 0</span>.
Note that every ideal of the ring contains <span class='latexinline'>0</span>. Hence, if an ideal <span class='latexinline'>I</span>
of a ring is known to be a radical ideal, then for any nilpotent <span class='latexinline'>r</span>,
since <span class='latexinline'>\exists n, r^n = 0 \in I</span>, since <span class='latexinline'>I</span> is radical, <span class='latexinline'>r \in I</span>.
That is, <i>a radical ideal with always contain all nilpotents!</i> It will
contain other elements as well, but it will contain nilpotents for sure.
<h4><a id=radicalization-of-an-ideal href='#radicalization-of-an-ideal'> § </a> Radicalization of an ideal</h4>
Given a ideal <span class='latexinline'>I</span>, it's radical idea <span class='latexinline'>\sqrt I \equiv \{ r \in R, r^n \in I \}</span>.
That is, we add all the elements <span class='latexinline'>I</span> needs to have for it to become a radical.
Notice that the radicalization of the zero ideal <span class='latexinline'>I</span> will precisely contain
all nilpotents. that is, <span class='latexinline'>\sqrt{(0)} \equiv \{ r \in R, r^n = 0\}</span>.
<h4><a id=reduced-rings href='#reduced-rings'> § </a> Reduced rings</h4>
A ring <span class='latexinline'>R</span> is a reduced ring if the only nilpotent in the ring is <span class='latexinline'>0</span>.
<h4><a id=creating-reduced-rings-removing-nilpotents-by-quotienting-radical-ideals href='#creating-reduced-rings-removing-nilpotents-by-quotienting-radical-ideals'> § </a> creating reduced rings (removing nilpotents) by quotienting radical ideals</h4>
Tto remove nilpotents of the ring <span class='latexinline'>R</span>, we can create <span class='latexinline'>R' \equiv R / \sqrt{(0}</span>. Since
<span class='latexinline'>\sqrt{(0)}</span> is the ideal which contains all nilpotents, the quotient ring <span class='latexinline'>R'</span> will contain
no nilpotents other than <span class='latexinline'>0</span>.
Similarly, quotienting by any larger radical ideal <span class='latexinline'>I</span> will remove all nilpotents
(and then some), leaving a reduced ring.
<blockquote> A ring modulo a radical ideal is reduced</blockquote>
<h4><a id=integral-domains href='#integral-domains'> § </a> Integral domains</h4>
a Ring <span class='latexinline'>R</span> is an integral domain if <span class='latexinline'>ab = 0 \implies a = 0 \lor b = 0</span>. That is,
the ring <span class='latexinline'>R</span> has no zero divisors.
<h4><a id=prime-ideals href='#prime-ideals'> § </a> Prime ideals</h4>
An ideal <span class='latexinline'>I</span> of a ring <span class='latexinline'>R</span> is a prime ideal if
<span class='latexinline'>\forall xy \in R, xy \in I \implies x \in I \lor y \in I</span>. This generalizes
the notion of a prime number diving a composite: <span class='latexinline'>p | xy \implies p | x \lor p | y</span>.
<h4><a id=creating-integral-domains-by-quotenting-prime-ideals href='#creating-integral-domains-by-quotenting-prime-ideals'> § </a> creating integral domains by quotenting prime ideals</h4>
Recall that every ideal contains a <span class='latexinline'>0</span>. Now, if an ideal <span class='latexinline'>I</span> is prime, and if
<span class='latexinline'>ab = 0 \in I</span>, then either <span class='latexinline'>a \in I</span> or <span class='latexinline'>b \in I</span> (by the definition of prime).
We create <span class='latexinline'>R' = R / I</span>. We denote <span class='latexinline'>\overline{r} \in R'</span> as the image of <span class='latexinline'>r \in R</span>
in the quotient ring <span class='latexinline'>R'</span>.
The intuition is that quotienting by a  <span class='latexinline'>I</span>, since if <span class='latexinline'>ab = 0 \implies a \in I \lor b \in I</span>,
we are "forcing" that in the quotient ring <span class='latexinline'>R'</span>, if <span class='latexinline'>\overline{a} \overline{b} = 0</span>, then either
<span class='latexinline'>\overline{a} = 0</span> or <span class='latexinline'>\overline{b} = 0</span>, since <span class='latexinline'>(a \in I \implies \overline a = 0)</span>,
and <span class='latexinline'>b \in I \implies \overline b = 0)</span>.
<blockquote> A ring modulo a prime ideal is an integral domain.</blockquote>
I learnt of this explanation from this
<a href=http://quickmathintuitions.org/relationship-between-reduced-rings-radical-ideals-and-nilpotent-elements/>excellent blog post by Stefano Ottolenghi</a>.
<h3><a id=my-disenchantment-with-abstract-interpretation href='#my-disenchantment-with-abstract-interpretation'> § </a> <a href=#my-disenchantment-with-abstract-interpretation>My disenchantment with abstract interpretation</a></h3>
When I first ran across the theory of abstract interpretation, it seemed magical:
Define two functions, check that they're monotone maps, and boom, we have
on our hands an analysis.
However, the problem appears to be that in reality, it's not as simple. Here is
the list of issues I've run across when trying to use abstract interpretation
for a "real world" use-case:
First of all, all interesting lattices are infinte height, requiring some
choice of widening.  Defining a good widening is a black art.  Secondly, while
there is a lot of theory on combining abstract domains (reduced products and
the like), it seems hard to deploy the theory in the real world.
I read a fair bit into the theory of abstract acceleration, where the idea is
that instead of widening indiscriminately, if our theory is powerful enough to
compute an exact closed form, we choose to do so. However, the problem is that
this regime does not "fit well" into abstract interpretation: We have the
abstract interpreter on the one hand, and then the acceleration regime on the
other, which is a separate algorithm. So the full analysis looks something
like:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
That is, what used to be a nice theory of just "do things in any order and
it will converge", now becomes a new algorithm, that uses abstract interpretation
as a subroutine. This was not the hope I had! I wanted to <i>get away</i> from having
to do proofs by analyzing an algorithm, this was the entire promise: Define
a lattice well enough and the proof is guaranteed. Rather, what I had
imagined was:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Now this <code>acceleration_domain</code> maybe frightfully complicated, but I'm willing
to pay that price, as long as it's an honest-to-god abstract interpretation.
This was a huge bummer for me to find out that this is not the case.
<h3><a id=computing-equivalent-gate-sets-using-grobner-bases href='#computing-equivalent-gate-sets-using-grobner-bases'> § </a> <a href=#computing-equivalent-gate-sets-using-grobner-bases>Computing equivalent gate sets using grobner bases</a></h3>
Here's a fun little problem, whose only solution I know involves a fair
bit of math and computer algebra:
We are given the grammar for a language <code>L</code>:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
where <code>+_mod8</code> is addition modulo 8, <code>-_mod8</code> is subtraction modulo 8,
and <code>^</code> is XOR.
This language is equipped with the obvious
evaluation rules, corresponding to those of arithmetic. We are guaranteed
that during evaluation, the variables <code>a_i</code> will only have values <code>0</code> and <code>1</code>.
Since we have addition, we can perform multiplication by a constant
by repeated addition. So we can perform <code>3*a</code> as <code>a+a+a</code>.
We are then given the input expression <code>(a0 ^ a1 ^ a2 ^ a3)</code>. We wish
to find an equivalent expression in terms of the above language <code>L</code>.
We think of <code>E</code> as some set of logic gates we are allowed to use, and we are
trying to express the operation <code>(a0 ^ a1 ^ a2 ^ a3)</code> in terms of these gates.
The first idea that I thought was that of employing a grobner basis,
since they essentially embody rewrite rules modulo polynomial equalities, which
is precisely our setting here.
In this blog post, I'm going to describe what a grobner basis is and why it's
natural to reach for them to solve this problem, the code, and the eventual
solution.
As a spolier, the solution is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Clearly, this contains only additions/subtractions and multiplications by
a constant.
If there's some principled way to derive this (beyond throwing symbolic
algebra machinery), I'd really love to know ---
<a href=https://github.com/bollu/bollu.github.io/issues>Please raise an issue with the explanation!</a>
<h4><a id=what-the-hell-is-grobner-basis href='#what-the-hell-is-grobner-basis'> § </a> What the hell is Grobner Basis?</h4>
The nutshell is that a grobner basis is a way to construct rewrite rules which
also understand arithmetic (I learnt this viewpoint from the book "Term
rewriting and all that". Fantastic book in general). Expanding on the
nutshell, assume we have a term rewriting system:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
over an alphabet <code>{A, B, C}</code>.
Now, given the string <code>C + AB</code>, we wish to find out if it can be rewritten to
<code>0</code> or not. Let's try to substitute and see what happens:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
At this point, we're stuck! we don't have rewrite rules to allow us to
rewrite <code>(-1*B)B</code> into <code>-B^2</code>. Indeed, creating such a list would be
infinitely long. But if we are willing to accept that we somehow have
the rewrite rules that correspond to polynomial arithmetic, where we view
<code>A, B, C</code> as variables, then we <i>can</i> rewrite the above string to 0:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
A Grobner basis is the algorithmic / mathematical machine that allows us
to perform this kind of substitution.
In this example, this might appear stupid: what is so special? We simply
substituted variables and arrived at <code>0</code> by using arithmetic. What's
so complicated about that? To understand why this is not always so easy,
let's consider a pathological, specially constructed example
<h4><a id=a-complicated-example-that-shatters-dreams href='#a-complicated-example-that-shatters-dreams'> § </a> A complicated example that shatters dreams</h4>
Here's the pathological example:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
And we consider the string <code>S = AB + B^2</code>.  If we blindly apply the
first rule, we arrive at:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
However, if we apply <code>(2)</code> and then <code>(1)</code>:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
This tells us that we <i>can't just apply the rewrite rules willy-nilly</i>.
It's sensitive to the <i>order</i> of the rewrites! That is, the rewrite system
is not <a href=https://en.wikipedia.org/wiki/Confluence_(abstract_rewriting>confluent</a>).
The grobner basis is a function from rewrite systems to rewrite systems.
When given a rewrite system <code>R</code>, it produces a <i>new</i> rewrite system <code>R'</code>
that <i>is confluent</i>. So, we can apply the rewrite rules of <code>R'</code> in any order,
and we guaranteed that we will only get a 0 from <code>R'</code> if and only if
we could have gotten a <code>0</code> from <code>R</code> for all strings.
We can then go on to phrase this whole rewriting setup in the language of
ideals from ring theory, and that is the language in which it is most
often described. I've gone into more depth on that perspective here: 
<a href=#what-the-hell-is-a-grobner-basis-ideals-as-rewrite-systems>"What is a grobner basis? polynomial factorization as rewrite systems"</a>.
Now that we have a handle on what a grobner basis is, let's go on to solve
the original problem:
<h4><a id=an-explanation-through-a-slightly-simpler-problem href='#an-explanation-through-a-slightly-simpler-problem'> § </a> An explanation through a slightly simpler problem</h4>
I'll first demonstrate the idea of how to solve the original problem
by solving a slightly simpler problem:
<blockquote> Rewrite <code>a^b^c</code> in terms of <code>a^b</code>, <code>b^c</code>, <code>c^a</code> and the same <code>+_mod8</code> instruction set as the original problem. The only difference this time is that we do <i>not</i> have <code>T -> V ^ V ^ V</code>.</blockquote>
The idea is to construct the polynomial ring over <code>Z/8Z</code> (integers modulo 8) with
variables as <code>a, b, c, axorb, bxorc, axorc</code>. Now, we know that <code>a^b = a + b - 2ab</code>. So,
we setup rewrite rules such that <code>a + b - 2ab -> axorb</code>, <code>b + c - 2bc -> bxorb</code>,
<code>c + a - 2ca -> cxora</code>.
We construct the <i>polynomial</i> <code>f(a, b, c) = a^b^c</code>, which
has been written in terms of addition and multiplication, defined
as <code>f_orig(a, b, c) = 4*a*b*c - 2*a*b - 2*a*c - 2*b*c + a + b + c</code>. We then
rewrite <code>f_orig</code> with respect to our rewrite rules. Hopefully, the rewrite
rules should give us a clean expression in terms of one variable and
two-variable <code>xor</code>s. There is the danger that we may have some term
such as <code>a * bxorc</code>, and we do get such a term (<code>2*b*axorc</code>) in this case,
but it does not appear in the <i>original</i> problem.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Running the code gives us the reduced polynomial <code>-2*b*axorc + b + axorc</code>
which unfortunately contains a term that is <code>b * axorc</code>. So, this approach
does not work, and I was informed by my friend that she is unaware
of a solution to this problem (writing <code>a^b^c</code> in terms of smaller xors and
sums).
The full code output is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
That is, both the original polynomial and the reduced polynomial match
the expected results. But the reduced polynomial is not in our language <code>L</code>,
since it has a term that is a <i>product</i> of <code>b</code> with <code>axorc</code>.
<h4><a id=tacking-the-original-problem href='#tacking-the-original-problem'> § </a> Tacking the original problem.</h4>
We try the exact same approach to the original problem of expressing
<code>a ^ b ^ c ^ d</code>. We find that the reduced polynomial is
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
which happily has no products between terms! It also passes our sanity
check, so we've now found the answer.
The full output is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=code-for-abcd-reduction href='#code-for-abcd-reduction'> § </a> code for <code>a^b^c^d</code> reduction:</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=closing-thoughts href='#closing-thoughts'> § </a> Closing thoughts</h4>
This was a really fun exercise: Around a hundred lines of code illuminates
the use of machinery such as grobner basis for solving real-world problems!
I really enjoyed hacking this up and getting nerd sniped.
<h3><a id=the-janus-programming-language--time-reversible-computation href='#the-janus-programming-language--time-reversible-computation'> § </a> <a href=#the-janus-programming-language--time-reversible-computation>The janus programming language --- Time reversible computation</a></h3>
<ul><li> <a href=https://en.wikipedia.org/wiki/Janus_(time-reversible_computing_programming_language>Wiki link</a></li><li> <a href=http://tetsuo.jp/ref/janus.pdf>Original letter to Landlauer</a></li></ul>
I found out it's called Janus, since Janus is the god of doorways in greek
mythology. Hence, he is also the god of duality and transitions --- he
<i>literally</i> looks both into the future and into the past.
<blockquote> He is usually depicted as having two faces, since he looks to the future and to the past.</blockquote>
An apt name for the language!
<h3><a id=a-b--a-book-about-proofs-of-combinatorial-closed-forms href='#a-b--a-book-about-proofs-of-combinatorial-closed-forms'> § </a> <a href=#a--b--a-book-about-proofs-of-combinatorial-closed-forms><code>A = B</code> --- A book about proofs of combinatorial closed forms</a></h3>
The book explains algorithms on solving closed forms for combinatorial
recurrences, by means of <a href=http://mathworld.wolfram.com/ZeilbergersAlgorithm.html>Zeilberger's algorithm</a>.
The book is written by Zeilberger himself, and supposedy also teaches one Maple.
I'd like to learn the algorithm, since it might be useful eventually for
Groebner basis / loop analysis shenanigans I like to play as part of
my work on compilers.
<ul><li> <a href=https://www.math.upenn.edu/~wilf/AeqB.pdf>Download link here</a></li></ul>
<h3><a id=generating-k-bitsets-of-a-given-length-n href='#generating-k-bitsets-of-a-given-length-n'> § </a> <a href=#generating-k-bitsets-of-a-given-length-n>Generating <code>k</code> bitsets of a given length <code>n</code></a>:</h3>
The problem is to generate all bitvectors of length <code>n</code> that have <code>k</code> bits
set. For example, generate all bitvectors of length <code>5</code> that have <code>3</code> bits
set.
I know that an algorithm exists in Hacker's delight, but I've been too sick
to crack open a book, so I decided to discover the algorithm myself. The one
I came up with relies on looking at the numbers moving at a certain velocity,
and them colliding with each other. For example, let us try to generate all
<code>5C3</code> combinations of bits.
We start wih:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Where the <code><</code> represents that the <code>1</code> at position <code>a</code> is moving leftwards.
Our arena is <i>circular</i>, so the leftmost <code>1</code> can wrap around to the right.
This leads to the next state
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
We continue moving left peacefully.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
whoops, we have now collided with a block of <code>1</code>s. Not to worry, we simply
transfer our velocity by way of collision, from the <code>1</code> at <code>d</code> to the <code>1</code> at <code>b</code>.
I denote the transfer as follows:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
The <code>1</code> at <code>b</code> proceeds along its merry way with the given velocity
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Once again, it wraps around, and suffers a collision
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
This continues:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
I don't have a proof of correctness, but I have an intuition that this
should generate all states. Does anyone have a proof?
<i>EDIT:</i> <a href=https://math.stackexchange.com/questions/3398241/correctness-proof-for-algorithm-to-generate-k-bitsets-of-n-bits-nck>this algorithm does not work</a>,
since it will keep clusters of <span class='latexinline'>k-1</span> bits next to each other, when a
bit hits a cluster of <span class='latexinline'>k - 1</span> bits.  For completeness, I'm going to draft out
the usual algorithm in full:
<h4><a id=usual-algorithm href='#usual-algorithm'> § </a> Usual Algorithm</h4>
Let's consider the same example of <code>5C3</code>:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
We start with all bits at their lowest position. Now, we try to go to
the next smallest number, which still has 3 bits toggled. Clearly, we need
the bit at position <code>b</code> to be 1, since that's the next number. Then,
we can keep the lower 2 bits <code>d, e</code> set to 1, so that it's still as small a
number as possible.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Once again, we now move the digit at <code>d</code> to the digit at <code>c</code>, while keeping
the final digit at <code>e</code> to make sure it's still the smallest possible.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Now, we can move the <code>1</code> at <code>e</code> to <code>d</code>, since that will lead to the smallest
increase:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
At this point, we are forced to move to location <code>a</code>, since we have exhausted
all smaller locations. so we move the <code>1</code> at <code>b</code> to <code>a</code>, and then reset all
the other bits to be as close to the LSB as possible:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Continuing this process gives us the rest of the sequence:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h3><a id=bondi-k-calculus href='#bondi-k-calculus'> § </a> <a href=#bondi-k-calculus>Bondi k-calculus</a></h3>
<ul><li> <a href=https://en.wikipedia.org/wiki/Bondi_k-calculus>Link here</a></li></ul>
An alternative formalism to derive special relativity geometrically,
resting purely on hypotehses about the way light travels.
However, I've not been able to prove the correctness of the assumptions made,
by using coordinate geometry. I suspect this is because I will need to use
hyperbolic geometry for the "side lengths" to work out.
Indeed, I found another source, called as <a href=http://bearsoft.co.uk/Kcalc.html>The k-calculus fiddle</a>
which attempts to discredit k-calculus. The author of the above blog writes at
the end:
<blockquote> In asking Ray D'Inverno's permission to use his book as the example of k-calculus, he was kind enough to point out that the arguments I have given are invalid. Chapter 2 of his book should be read through to the end and then reread in the light of the fact that the geometry of space and time is Minkowskian. Euclidean geometry should not be used in interpreting the diagrams because their geometry is Minkowskian.</blockquote>
which seems to imply that we need to use hyperbolic geometry for this.
<h3><a id=topology-as-an-object-telling-us-what-zero-locus-is-closed href='#topology-as-an-object-telling-us-what-zero-locus-is-closed'> § </a> Topology as an object telling us what zero-locus is closed:</h3>
<ul><li> <a href=https://math.stackexchange.com/questions/53852/is-there-a-way-of-working-with-the-zariski-topology-in-terms-of-convergence-limi>Idea from this amazing post on <code>math.se</code></a></li></ul>
<h3><a id=vivado-toolchain-craziness- href='#vivado-toolchain-craziness-'> § </a> <a href=#vivado-toolchain-craziness>Vivado toolchain craziness </a></h3>
I found this file as I was cleaning up some old code, for a project to implement
a <a href=https://github.com/AakashKT/CuckooHashingHLS>fast K/V store on an FPGA</a>,
so I thought I should put this up for anyone else who stumbles on the
same frustrations / errors. I'm not touching this particular toolchain again
with a 10-foot pole till the tools stabilize by <i>a lot</i>.
<h4><a id=vivado-hls-issues href='#vivado-hls-issues'> § </a> Vivado HLS issues</h4>
<ul><li> Unable to create BRAM for fields such as <code>bool</code>, <code>int16</code>. The data buses
  will be <code>8/16</code> bits long, with error:</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> I had an array of structs:</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
This gets generated as 3 ports for memory, of widths <code>1</code>, <code>16</code>, <code>16</code>. Ideally,
I wanted <i>one</i> port, of width <code>16+16+1=33</code>, for each struct value.
However, what was generated were three ports of widths <code>1</code>, <code>16</code>, and <code>16</code>
which I cannot connect to BRAM.
<ul><li> <code>data_pack</code> allows us to create one port of width <code>16+16+1=33</code></li></ul>
<ul><li> Shared function names allocated on BRAM causes errors in synthesis:</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> Enums causes compile failure in RTL generation  (commit <code>3c0d619039cff7a7abb61268e6c8bc6d250d8730</code>)</li><li> <code>ap_int</code> causes compile failurre in RTL generation (commit <code>3c0d619039cff7a7abb61268e6c8bc6d250d8730</code>)</li><li> <code>x % m</code> where <code>m != 2^k</code> is very expensive -- there must be faster encodings of modulus?</li><li> How to share code between HLS and vivado SDK? I often wanted to share constant values between
  my HLS code and my Zynq code.</li><li> Can't understand why array of structs that were packed does not get deserialized correctly. I had to manually
  pack a struct into a <code>uint32</code>. For whatever reason, having a <code>#pragma pack</code> did something to the representation of the struct
  as far as I can tell, and I couldn't treat the memory as just a raw <code>struct *</code> on the other side:</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> If I change my IP, there is no way to preserve the current connections in the
  GUI why just updating the "changed connections". I'm forced to remove the IP
  and add it again (no, the Refresh IP button does not work).</li><li> On generating a new bitstream from Vivado, Vivado SDK tries to reload the config,
  fails at the reloading (thinks <code>xil_print.h</code> doesn't exist), and then fails to compile code.
  Options are to either restart Vivado SDK, or refresh <code>xil_print.h</code>.</li></ul>
<ul><li> It is entirely unclear what to version control in a vivado project, unless one
  has an omniscient view of the <i>entire toolchain</i>. I resorted to <code>git add</code> ing
  everything, but this is a terrible strategy in so many ways.</li></ul>
<h4><a id=sdaccel-bugs href='#sdaccel-bugs'> § </a> SDAccel bugs</h4>
<b><a href=https://www.xilinx.com/support/documentation/sw_manuals/xilinx2017_1/ug1028-sdsoc-intro-tutorial.pdf>link to tutorial we were following</a></b>
<ul><li> The executable is named <code>.exe</code> while it's actually an ELF executable (The SDAccel tutorials say it is called as <code>.elf</code>)</li><li> the board is supposed to automatically boot into linux, which it does not. One is expected to call <code>bootd</code> manually (for "boot default") so it boots ito linux. (The SDAccel tutorials say it automatically boots into it)</li><li> At this point, the SD card is unreadable. It took a bunch of time to figure out that the SD card needs to be mounted by us, and has the mount name <code>/dev/mmcblk0p1</code>. (The SDAccel tutorials say that it should be automatically mounted)</li><li> At this point, we are unable to run <code>hashing.elf</code>. It dies with a truly bizarre error: <code>hashing.elf: command not found</code>. This is almost un-googleable, due to the fact that the same problem occurs when people don't have the correct file name.</li><li> I rewrote <code>ls</code> with <code>hashing.elf</code> to see what would happen, because I conjectured that the shell was able to run <code>coreutils</code>.</li><li> This dies with a different error <code>ls: core not found</code>. I'd luckily seen this during my android days, and knew this was from busybox.</li><li> This led me to google "busybox unable to execute executable", which led me to this <a href=https://stackoverflow.com/questions/1562071/how-can-i-find-which-elf-dependency-is-not-fulfilled>StackOverflow link</a> that clued me into the fact that the ELF interpreter is missing.</li><li> When I discovered this, I wound up trying to understand how to get the right ELF interpreter. <code>readelf -l <exe name></code> dumps out <code>[Requesting program interpreter: /lib/ld-linux-armhf.so.3]</code>. So, I bravely copied: <code>cp /lib/ld-linux.so.3 /lib/ld-linux-armhf.so.3</code>.</li><li> Stuff is <i>still</i> broken, but I now get <i>useful</i> error messages:</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
At this point, clearly we have some linker issues (why does <code>xocc</code> not correctly statically link? What's up with it? Why does it expect it to be able to load a shared library? <b>WTF is happening</b>). do note that this is <i>not</i> the way the process
is supposed to go according to the tutorial!
<ul><li> Of course, there's no static library version of <code>libxilinxopencl.so</code>, so that's a dead end. I'm completely unsure if the tutorial even makes sense.</li><li> This entire chain of debugging is full of luck.</li></ul>
<ul><li> <a href=https://www.xilinx.com/html_docs/xilinx2018_2/sdsoc_doc/compiling-and-running-applications-on-arm-processor-hjy1504034381720.html>Link talking about generating <code>BOOT</code> file</a></li></ul>
At some point, I gave up on the entire enterprise.
<h3><a id=wha-a-grobner-basis-ideals-as-rewrite-systems href='#wha-a-grobner-basis-ideals-as-rewrite-systems'> § </a> <a href=#what-the-hell-is-a-grobner-basis-ideals-as-rewrite-systems>What the hell <i>is</i> a Grobner basis? Ideals as rewrite systems</a></h3>
<h4><a id=a-motivating-example href='#a-motivating-example'> § </a> A motivating example</h4>
The question a Grobner basis allows us to answer is this: can the polynomial
<span class='latexinline'>p(x, y) = xy^2 + y</span> be factorized in terms of <span class='latexinline'>a(x, y) = xy + 1, b(x, y) = y^2 - 1</span>,
such that <span class='latexinline'>p(x, y) = f(x, y) a(x, y) + g(x, y) b(x, y)</span> for some <i>arbitrary</i> polynomials
<span class='latexinline'>f(x, y), g(x, y) \in R[x, y]</span>.
One might imagine, "well, I'll divide and see what happens!" Now, there are two
routes to go down:
<ul><li> <span class='latexinline'>xy^2 + y = y(xy + 1) = y a(x, y) + 0 b(x, y)</span>. Well, problem solved?</li><li> <span class='latexinline'>xy^2 + y = xy^2 - x + x + y = x (y^2 - 1) + x + y = x b(x, y) + (x + y)</span>. Now what? we're stuck, and we can't apply <code>a(x, y)</code>!</li></ul>
So, clearly, the <i>order</i> in which we perform of factorization / division starts
to matter! Ideally, we want an algorithm which is <i>not sensitive</i> to the order
in which we choose to apply these changes. <span class='latexinline'>x^2 + 1</span>.
<h4><a id=the-rewrite-rule-perspective href='#the-rewrite-rule-perspective'> § </a> The rewrite rule perspective</h4>
An alternative viewpoint of asking "can this be factorized", is to ask
"can we look at the factorization as a rewrite rule"? For this perspective,
notice that "factorizing" in terms of <span class='latexinline'>xy + 1</span> is the same as being
able to set <span class='latexinline'>xy = -1</span>, and then have the polynomial collapse to zero.
(For the more algebraic minded, this relates to the fact that <span class='latexinline'>R[x] / p(x) \sim R(\text{roots of p})</span>).
The intuition behind this is that when we "divide by <span class='latexinline'>xy + 1</span>", really what
we are doing is we are setting <span class='latexinline'>xy + 1 = 0</span>, and then seeing what remains. But
<span class='latexinline'>xy + 1 = 0 \iff xy = -1</span>. Thus, we can look at the original question as:
How can we apply the rewrite rules <span class='latexinline'>xy \rightarrow -1</span>, <span class='latexinline'>y^2 \rightarrow 1</span>,
along with the regular rewrite rules of polynomial arithmetic to the polynomial
<span class='latexinline'>p(x, y) = xy^2 + y</span>, such that we end with the value <span class='latexinline'>0</span>?
Our two derivations above correspond to the application of the rules:
<ul><li> <span class='latexinline'>xy^2 + y \xrightarrow{xy = -1} -y + y = 0</span></li><li> <span class='latexinline'>xy^2 + y \xrightarrow{y^2 = 1} x + y \nrightarrow \text{stuck!}</span></li></ul>
That is, our <a href=https://en.wikipedia.org/wiki/Confluence_(abstract_rewriting>rewrite rules are not confluent</a>)
The grobner basis is a mathematical object, which is a  <i>a confluent set of rewrite rules</i>
for the above problem. That is, it's a set of polynomials which manage to find
the rewrite <span class='latexinline'>p(x, y) \xrightarrow{\star} 0</span>, regardless of the order in which
we apply them. It's also <i>correct</i>, in that it only rewrites to <span class='latexinline'>0</span> if the
original system had <i>some way</i> to rewrite to <span class='latexinline'>0</span>.
<h4><a id=the-buchbergers-algorithm href='#the-buchbergers-algorithm'> § </a> The buchberger's algorithm</h4>
We need to identify
<a href=https://en.wikipedia.org/wiki/Critical_pair_(logic>critical pairs</a>),
which in this setting are called as S-polynomials.
Let <span class='latexinline'>f_i = H(f_i) + R(f_i)</span> and <span class='latexinline'>f_j = H(f_j) + R(f_j)</span>. Let <span class='latexinline'>m = lcm(H(f_i), H(f_j))</span>,
and let <span class='latexinline'>m_i, m_j</span> be monomials such that <span class='latexinline'>m_i \cdot H(f_i) = m = m_j \cdot H(f_j)</span>.
The S-polynomial induced by <span class='latexinline'>f_i, f_j</span> is defined as <span class='latexinline'>S(f_i, f_j) = m_i f_i - m_i f_j</span>.
<h4><a id=references href='#references'> § </a> References</h4>
<ul><li> <a href=https://www21.in.tum.de/~nipkow/TRaAT/>The term rewriting perspective is from the book "term rewriting and all that"</a></li><li> <a href=https://mattpap.github.io/masters-thesis/html/src/groebner.html>Sympy has excellent reading material on grobner basis</a></li></ul>
<h3><a id=lie-bracket-versus-torsion href='#lie-bracket-versus-torsion'> § </a> <a href=lie-bracket-versus-torsion>Lie bracket versus torsion</a></h3>
<img width=200 height=200 src="static/lie-bracket-versus-torsion.png">
This picture <i>finally</i> made the difference between these two things clear.
The lie bracket moves along the <i>flow</i>, while the torsion moves along
<i>parallel transport</i>.
This is why the sides of the parallelogram that measure torsion form,
well, a parallelogram: we set them up using parallel transport.
On the other hand, the lie bracket measures the actual failure of the parallelogram
from being formed.
<h3><a id=blog-post-weekend-paper-replication-of-stoke-the-stochastic-superoptimizer href='#blog-post-weekend-paper-replication-of-stoke-the-stochastic-superoptimizer'> § </a> <a href=https://github.com/bollu/blaze/blob/master/notebooks/tutorial.ipynb>Blog post: Weekend paper replication of STOKE, the stochastic superoptimizer</a></h3>
Click the title to go to the post. We replicate the <code>STOKE</code> paper in haskell,
to implement a superoptimiser based on MCMC methods.
<h3><a id=collapsing-blockid-label-unique href='#collapsing-blockid-label-unique'> § </a> Collapsing <code>BlockId</code>, <code>Label</code>, <code>Unique</code>:</h3>
We have this hiearchy of <code>BlockId</code>, <code>Label</code>, and <code>Unique</code> that can be
collapsed.
<h3><a id=spatial-partitioning-data-structures-in-molecular-dynamics href='#spatial-partitioning-data-structures-in-molecular-dynamics'> § </a> <a href=#spatial-partitioning-data-structures-in-molecular-dynamics>Spatial partitioning data structures in molecular dynamics</a></h3>
<ul><li> <a href=https://en.wikipedia.org/wiki/Cell_lists>Cell lists</a></li><li> <a href=https://en.wikipedia.org/wiki/Verlet_list>Verlet lists</a></li></ul>
appear to be version of spatial hierarchical data structures for fast
interaction computation. Apparently, multipole expansions are not useful
in this case since multipole expansions are useful to take into account
long range effects, but not short range effects.
<h3><a id=vector-arthur-whitney-and-text-editors href='#vector-arthur-whitney-and-text-editors'> § </a> <a href=#vector-arthur-whitney-and-text-editors>Vector: Arthur Whitney and text editors</a></h3>
<ul><li> http://archive.vector.org.uk/art10501320</li></ul>
<h3><a id=representing-cps-in-llvm-using-the-coro-intrinsics href='#representing-cps-in-llvm-using-the-coro-intrinsics'> § </a> Representing CPS in LLVM using the <code>@coro.*</code> intrinsics</h3>
This is part of a larger thread --- <a href=http://lists.llvm.org/pipermail/llvm-dev/2017-April/112212.html>Adding CPS call support to LLVM</a> where there is a large discussion on the correct design of how to teach LLVM about CPS.
Gor Nishanov proided the above example of encoding CPS using the llvm <code>coro</code> instructions.
<ul><li> https://gist.github.com/bollu/e0573dbc145028fb42f89e64c6dd6742</li></ul>
<h3><a id=bug-in-the-llvm-code-generator-lowering-of-moadd2-and-moaddwordc href='#bug-in-the-llvm-code-generator-lowering-of-moadd2-and-moaddwordc'> § </a> Bug in the LLVM code generator: Lowering of <code>MO_Add2</code> and <code>MO_AddWordC</code></h3>
<a href=https://github.com/ghc/ghc/blob/bf73419518ca550e85188616f860961c7e2a336b/compiler/llvmGen/LlvmCodeGen/CodeGen.hs#L817>Both of these are lowered the same way</a>,
but they should be different.
In particular, <code>GHC.Prim</code> explains:
<ul><li> <a href=http://hackage.haskell.org/package/ghc-prim-0.5.3/docs/GHC-Prim.html#v:addWordC-35-><code>AddWordC#</code></a> returns <code>(result, carry)</code></li><li> <a href=http://hackage.haskell.org/package/ghc-prim-0.5.3/docs/GHC-Prim.html#v:plusWord-35-><code>PlusWordC#</code></a> returns <code>(carry, result)</code></li></ul>
Honestly, this is confusing, but I guess there's some story to having two separate primops for this?
<h3><a id=discrete-random-distributions-with-conditioning-in-20-lines-of-haskell href='#discrete-random-distributions-with-conditioning-in-20-lines-of-haskell'> § </a> <a href=#discrete-random-distributions-with-conditioning-in-20-lines-of-haskell>Discrete random distributions with conditioning in 20 lines of haskell</a></h3>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
This gives the output:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Notice that <code>D a ~= WriterT (Product Float) []</code>!
<h3><a id=everything-you-know-about-word2vec-is-wrong href='#everything-you-know-about-word2vec-is-wrong'> § </a> <a href=#everything-you-know-about-word2vec-is-wrong>Everything you know about word2vec is wrong</a></h3>
The classic explanation of <code>word2vec</code>, in skip-gram, with negative sampling,
in the paper and countless blog posts on the internet is as follows:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Indeed, if I google "word2vec skipgram", the results I get are:
<ul><li> <a href=https://en.wikipedia.org/wiki/Word2vec#Training_algorithm>The wikipedia page which describes the algorithm on a high level</a></li><li> <a href=https://www.tensorflow.org/tutorials/representation/word2vec>The tensorflow page with the same explanation</a></li><li> <a href=https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b>The towards data science blog which describes the same algorithm</a></li></ul>
the list goes on. However, <b>every single one of these implementations is wrong</b>.
The original word2vec <code>C</code> implementation does <i>not</i> do what's explained above,
and is <i>drastically different</i>. Most serious users of word embeddings, who use
embeddings generated from <code>word2vec</code> do one of the following things:
<ol><li> They invoke the original C implementation directly.</li><li> They invoke the <code>gensim</code> implementation, which is <i>transliterated</i> from the
   C source to the extent that the variables names are the same.</li></ol>
Indeed, the <code>gensim</code> implementation is the 
<b>only one that I know of which is faithful to the C implementation</b>.
<h4><a id=the-c-implementation href='#the-c-implementation'> § </a> The C implementation</h4>
The C implementation in fact maintains <i>two vectors for each word</i>, one where
it appears as a focus word, and one where it appears as a context word.
(Is this sounding familiar? Indeed, it appears that GloVe actually took this
idea from <code>word2vec</code>, which has never mentioned this fact!)
The setup is incredibly well done in the C code:
<ul><li> An array called <code>syn0</code> holds the vector embedding of a word when it occurs
  as a <i>focus word</i>. This is <b>random initialized</b>.</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> Another array called <code>syn1neg</code> holds the vector of a word when it occurs
  as a <i>context word</i>. This is <b>zero initialized</b>.</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> During training (skip-gram, negative sampling, though other cases are
  also similar), we first pick a focus word. This is held constant throughout
  the positive and negative sample training. The gradients of the focus vector
  are accumulated in a buffer, and are applied to the focus word
  <i>after it has been affected by both positive and negative samples</i>.</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=why-random-and-zero-initialization href='#why-random-and-zero-initialization'> § </a> Why random and zero initialization?</h4>
Once again, since none of this actually explained in the original papers
<i>or on the web</i>, I can only hypothesize.
My hypothesis is that since the negative samples come from all over the text
and are not really weighed by frequency, you can wind up picking <i>any word</i>,
and more often than not, <i>a word whose vector has not been trained much at all</i>.
If this vector actually had a value, then it could move the actually important
focus word randomly.
The solution is to set all negative samples to zero, so that 
<i>only vectors that have occurred somewhat frequently</i> will affect the representation 
of another vector.
It's quite ingenious, really, and until this, I'd never really thought of
how important initialization strategies really are.
<h4><a id=why-im-writing-this href='#why-im-writing-this'> § </a> Why I'm writing this</h4>
I spent two months of my life trying to reproduce <code>word2vec</code>, following
the paper exactly, reading countless articles, and simply not succeeding.
I was unable to reach the same scores that <code>word2vec</code> did, and it was not
for lack of trying.
I could not have imagined that the paper would have literally fabricated an
algorithm that doesn't work, while the implementation does something completely
different.
Eventually, I decided to read the sources, and spent three whole days convinced
I was reading the code wrong since literally everything on the internet told me
otherwise.
I don't understand why the original paper and the internet contain zero
explanations of the <i>actual</i> mechanism behind <code>word2vec</code>, so I decided to put
it up myself.
This also explains GloVe's radical choice of having a separate vector
for the negative context --- they were just doing what <code>word2vec</code> does, but
they told people about it <code>:)</code>.
Is this academic dishonesty? I don't know the answer, and that's a heavy
question. But I'm frankly incredibly pissed, and this is probably the last
time I take a machine learning paper's explanation of the algorithm
seriously again --- from next time, I read the source <i>first</i>.
<h3><a id=hamiltonian-monte-carlo-leapfrog-integrators-and-sympletic-geometry href='#hamiltonian-monte-carlo-leapfrog-integrators-and-sympletic-geometry'> § </a> Hamiltonian monte carlo, leapfrog integrators, and sympletic geometry</h3>
This is a section that I'll update as I learn more about the space, since I'm studying
differential geometry over the summer, I hope to know enough about "sympletic manifolds".
I'll make this an append-only log to add to the section as I understand more.
<h4><a id=31st-may href='#31st-may'> § </a> 31st May</h4>
<ul><li> To perform hamiltonian monte carlo, we use the hamiltonian and its
  derivatives to provide a momentum to our proposal distribution --- That is,
  when we choose a new point from the current point, our probability
  distribution for the new point is influenced by our current momentum.</li></ul>
<ul><li> For some integral necessary within this scheme, Euler integration doesn't cut it
  since the error diverges to infinity</li></ul>
<ul><li> Hence, we need an integrator that guarantees that the energy of out system is
  conserved.  Enter the leapfrog integrator. This integrator is also 
  <i>time reversible</i> -- We can run it forward for <code>n</code> steps, and then run it
  backward for <code>n</code> steps to arrive at the same state.  Now I finally know how
  Braid was implemented, something that bugged the hell out of 9th grade me
  when I tried to implement Braid-like physics in my engine!</li></ul>
<ul><li> The actual derivation of the integrator uses Lie algebras, Sympletic
  geometry, and other diffgeo ideas, which is great, because it gives me
  motivation to study differential geometry <code>:)</code></li></ul>
<ul><li> Original paper: <a href=https://www.sciencedirect.com/science/article/abs/pii/0375960190900923>Construction of higher order sympletic integrators</a></li></ul>
<h4><a id=simulating-orbits-with-large-timesteps href='#simulating-orbits-with-large-timesteps'> § </a> Simulating orbits with large timesteps</h4>
<img width=200 height=200 src="./static/leapfrog-vs-euler.png">
Clearly, the leapfrog integrator preserves energy and continues to move
in an orbit, while the euler integrator goes batshit and causes orbits
to spiral outwards. Full code is available below. More of the code is
spent coaxing matplotlib to look nice, than doing the actual
computation.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h3><a id=small-haskell-mcmc-implementation href='#small-haskell-mcmc-implementation'> § </a> <a href=#small-haskell-mcmc-implementation>Small Haskell MCMC implementation</a></h3>
We create a simple monad called <code>PL</code> which allows for a single operation: sampling
from a uniform distribution. We then exploit this to implement MCMC using metropolis hastings,
which is used to sample from arbitrary distributions. Bonus is a small library to render sparklines
in the CLI.
For next time:
<ul><li> Using applicative to speed up computations by exploiting parallelism</li><li> Conditioning of a distribution wrt a variable</li></ul>
<h4><a id=source-code href='#source-code'> § </a> Source code</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=output href='#output'> § </a> Output</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h3><a id=the-smallest-implementation-of-reverse-mode-ad-autograd-ever href='#the-smallest-implementation-of-reverse-mode-ad-autograd-ever'> § </a> The smallest implementation of reverse mode AD (autograd) ever:</h3>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Yeah, in ~80 lines of code, you can basically build an autograd engine. Isn't
haskell so rad?
<h3><a id=timings-of-passes-in-ghc-and-low-hanging-fruit-in-the-backend href='#timings-of-passes-in-ghc-and-low-hanging-fruit-in-the-backend'> § </a> Timings of passes in GHC, and low hanging fruit in the backend:</h3>
<ul><li> One can use <code>-v3</code> to get pass timings.</li><li> Apparently, GHC spends a lot of time in the simplifier, and time
  spend in the backend is peanuts in comparison to this.</li></ul>
To quote <code>AndreasK</code>:
<blockquote> - Register allocation, common block elimination, block layout and pretty printing are the "slow" things in the backend as far as I remember. - There are also a handful of TODO's in the x86 codegen which still apply. So you can try to grep for these. - Strength reduction for division by a constant</blockquote>
<ul><li> <a href=https://gitlab.haskell.org/ghc/ghc/issues/9041>NCG generates slow loop code</a></li></ul>
<h3><a id=varargs-in-ghc-ghctestsuitetestsrtst7160hs href='#varargs-in-ghc-ghctestsuitetestsrtst7160hs'> § </a> Varargs in GHC: <code>ghc/testsuite/tests/rts/T7160.hs</code></h3>
A comment from this test case tells us why the function <code>debugBelch2</code> exists:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
The implementation is:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h3><a id=debugging-debug-info-in-ghc href='#debugging-debug-info-in-ghc'> § </a> <a href=#debugging-debug-info-in-GHC>Debugging debug info in GHC</a></h3>
I wanted to use debug info to help build a better debugging experience
within <a href=http://github.com/tweag/asterius><code>tweag/asterius</code></a>. So, I was
reading through the sources of <code>cmm/Debug.hs</code>.
I'd never considered how to debug debug-info, and I found the information
tucked inside a cute note in GHC (<code>Note [Debugging DWARF unwinding info]</code>):
<blockquote> This makes GDB produce a trace of its internal workings. Having gone this far, it's just a tiny step to run GDB in GDB. Make sure you install debugging symbols for gdb if you obtain it through a package manager.</blockquote>
<ul><li> <a href=https://github.com/ghc/ghc/blob/535a26c90f458801aeb1e941a3f541200d171e8f/compiler/cmm/Debug.hs#L458>Link to GHC sources</a></li></ul>
<h3><a id=ghc-llvm-code-generator-switch-to-unreachable href='#ghc-llvm-code-generator-switch-to-unreachable'> § </a> <a href=#ghc-llvm-code-generator-switch-to-unreachable>GHC LLVM code generator: Switch to unreachable</a></h3>
The <a href=https://github.com/ghc/ghc/blob/master/compiler/llvmGen/LlvmCodeGen/CodeGen.hs#L1102>switch to out of range</a>
code generator switches to the first label. It should be more profitable
to switch to a <code>unreachable</code> block. That way, LLVM can take advantage of UB.
<h3><a id=concurrency-in-haskell href='#concurrency-in-haskell'> § </a> <a href=#concurrency-in-haskell>Concurrency in Haskell</a></h3>
Great link to the GHC wiki that describes the concurrency primitives
"bottom up": https://gitlab.haskell.org/ghc/ghc/wikis/lightweight-concurrency
<h3><a id=handy-list-of-differential-geometry-definitions href='#handy-list-of-differential-geometry-definitions'> § </a> <a href=#handy-list-of-differential-geometry-definitions>Handy list of differential geometry definitions</a></h3>
There are way too many objects in diffgeo, all of them subtly connected.
Here I catalogue all of the ones I have run across:
<h4><a id=manifold href='#manifold'> § </a> Manifold</h4>
A manifold <span class='latexinline'>M</span> of dimension <span class='latexinline'>n</span> is a topological space. So, there is a
topological structure <span class='latexinline'>T</span> on <span class='latexinline'>M</span>. There is also an <i>Atlas</i>, which is a family
of <i>Chart</i>s that satisfy some properties.
<h4><a id=chart href='#chart'> § </a> Chart</h4>
A chart is a pair <span class='latexinline'>(O \in  T , cm: O -> \mathbb R^n</span>. The <span class='latexinline'>O</span> is an open set of the
manifold, and <span class='latexinline'>cm</span> ("chart for "m") is a continuous mapping from <span class='latexinline'>O</span> to <span class='latexinline'>\mathbb R^n</span>
under the subspace topology for <span class='latexinline'>U</span> and the standard topology for <span class='latexinline'>\mathbb R^n</span>.
<h4><a id=atlas href='#atlas'> § </a>  Atlas</h4>
An <i>Atlas</i> is a collection of <i>Chart</i>s such that the charts cover the manifold,
and the charts are pairwise compatible. That is, <span class='latexinline'>A = \{ (U_i, \phi_i) \}</span>, such
that <span class='latexinline'>\cup{i} U_i = M</span>, and <span class='latexinline'>\phi_j \circ phi_i^{-1}</span> is smooth.
<h4><a id=differentiable-map href='#differentiable-map'> § </a> Differentiable map</h4>
<span class='latexinline'>f: M \to N</span> be a mapping from an <span class='latexinline'>m</span> dimensional manifold to an <span class='latexinline'>n</span> dimensional
manifold. Let <span class='latexinline'>frep = cn \circ f \circ cm^{-1}: \mathbb R^m -> \mathbb R^n</span>
where <span class='latexinline'>cm: M \to \mathbb R^m</span> is a chart for <span class='latexinline'>M</span>, <span class='latexinline'>cn: N \to \mathbb R^n</span>
is a chart for <span class='latexinline'>N</span>. <span class='latexinline'>frep</span> is <span class='latexinline'>f</span> represented
in local coordinates. If <span class='latexinline'>frep</span> is smooth for all choices of <span class='latexinline'>cm, cn</span>,
then <span class='latexinline'>f</span> is a differentiable map from <span class='latexinline'>M</span> to <span class='latexinline'>N</span>.
<h4><a id=curve href='#curve'> § </a> Curve:</h4>
Let <span class='latexinline'>I</span> be an open interval of <span class='latexinline'>\mathbb R</span> which includes the point <code>0</code>.  A Curve is a
differentiable map <span class='latexinline'>C: (a, b) \to M</span> where <span class='latexinline'>a < 0 < b</span>.
<h4><a id=function-i-hate-this-term-i-prefer-something-like-valuation href='#function-i-hate-this-term-i-prefer-something-like-valuation'> § </a> Function: (I hate this term, I prefer something like Valuation):</h4>
A differentiable mapping from <span class='latexinline'>M</span> to <span class='latexinline'>R</span>.
<h4><a id=directional-derivative-of-a-function-fm-m--r-with-respect-to-a-curve-ct-i--m-denoted-as-cf href='#directional-derivative-of-a-function-fm-m--r-with-respect-to-a-curve-ct-i--m-denoted-as-cf'> § </a> Directional derivative of a function <code>f(m): M -> R</code> with respect to a curve <code>c(t): I -> M</code>, denoted as <code>c[f]</code>.</h4>
Let <code>g(t) = (f . c)(t) :: I -c-> M -f-> R = I -> R</code>.
This this is the value <code>dg/dt(t0) = (d (f . c) / dt) (0)</code>.
<h4><a id=tangent-vector-at-a-point-p href='#tangent-vector-at-a-point-p'> § </a> Tangent vector at a point <code>p</code>:</h4>
On a <code>m</code> dimensional manifold <code>M</code>, a tangent vector at a point <code>p</code> is an
equivalence class of curves that have <code>c(0) = p</code>, such that <code>c1(t) ~ c2(t)</code> iff
:
<ul><li> For a (all) charts <code>(O, ch)</code> such that <code>c1(0) ∈  O</code>, <code>d/dt (ch . c1: R -> R^m) = d/dt (ch . c2: R -> R^m)</code>.</li></ul> That is, they have equal derivatives.
<h4><a id=tangent-spacetpm href='#tangent-spacetpm'> § </a> Tangent space(<code>TpM</code>):</h4>
The set of all tangent vectors at a point <code>p</code> forms a vector space <code>TpM</code>.
We prove this by creating a bijection from every curve to a vector <code>R^n</code>.
Let <code>(U, ch: U -> R)</code> be a chart around the point <code>p</code>, where <code>p ∈ U ⊆ M</code>. Now,
the bijection is defined as:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=cotangent-spacetpm-dual-space-of-the-tangent-space-space-of-all-linear-functions-from-tpm-to-r href='#cotangent-spacetpm-dual-space-of-the-tangent-space-space-of-all-linear-functions-from-tpm-to-r'> § </a> Cotangent space(<code>TpM*</code>): dual space of the tangent space / Space of all linear functions from <code>TpM</code> to <code>R</code>.</h4>
<ul><li> Associated to every function <code>f</code>, there is a cotangent vector, colorfully
  called <code>df</code>. The definition is <code>df: TpM -> R</code>, <code>df(c: I -> M) = c[f]</code>. That is,
  given a curve <code>c</code>, we take the directional derivative of the function <code>f</code>
  along the curve <code>c</code>. We need to prove that this is constant for all vectors
  in the equivalence class and blah.</li></ul>
<h4><a id=pushforward-pushf-tpm--tpn href='#pushforward-pushf-tpm--tpn'> § </a>  Pushforward <code>push(f): TpM -> TpN</code></h4>
Given a curve <code>c: I -> M</code>, the pushforward
is the curve <code>f . c : I -> N</code>. This extends to the equivalence classes
and provides us a way to move curves in <code>M</code> to curves in <code>N</code>, and thus
gives us a mapping from the tangent spaces.
This satisfies the identity:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=pullback-pullf-tpn--tpm href='#pullback-pullf-tpn--tpm'> § </a> Pullback <code>pull(f): TpN* -> TpM*</code></h4>
Given a linear functional <code>wn : TpN -> R</code>, the pullback is defined as
<code> wn . push(f) : TpM -> R</code>.
This satisfies the identity:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=vector-field-as-derivation href='#vector-field-as-derivation'> § </a> Vector field as derivation</h4>
TODO
<h4><a id=lie-derivation href='#lie-derivation'> § </a> Lie derivation</h4>
<h4><a id=lie-derivation-as-lie-bracket href='#lie-derivation-as-lie-bracket'> § </a> Lie derivation as lie bracket</h4>
<h3><a id=lazy-programs-have-space-leaks-strict-programs-have-time-leaks href='#lazy-programs-have-space-leaks-strict-programs-have-time-leaks'> § </a> <a href=#lazy-programs-have-space-leaks-strict-programs-have-time-leaks>Lazy programs have space leaks, Strict programs have time leaks</a></h3>
Stumbled across this idea while reading some posts on a private discourse.
<ul><li> Continually adding new thunks without forcing them can lead to a space leak,
  aka the dreaded monadic parsing backtracking problem.</li></ul>
<ul><li> Continually <i>running</i> new thunks can lead to a "time leak", where we spend
  far too much time running things that should not be run in the first place!</li></ul>
This is an interesting perspective that I've never seen articulated before, and
somehow helps make space leaks feel more... palatable? Before, I had no
analogue to a space leak in the strict world, so I saw them as a pathology. But
with this new perspective, I can see that the strict world's version of a space
leak is a time leak.
<h3><a id=presburger-arithmetic-can-represent-the-collatz-conjecture href='#presburger-arithmetic-can-represent-the-collatz-conjecture'> § </a> <a href=#presburger-arithmetic-can-represent-the-collatz-conjecture>Presburger arithmetic can represent the Collatz Conjecture</a></h3>
An observation I had: the function
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
is a Presburger function, so by building better approximations to the
transitive closure of a presburger function, one could get better answers
to the Collatz conjecture. Unfortunately, ISL (the integer set library) of today
is not great against the formidable foe.
The code:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Produces the somewhat disappointing, and yet expected output:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
I find it odd that it is unable to prove <i>anything</i> about the image, even that
it is non-negative, for example. This is an interesting direction in which
to improve the functions <code>isl_map_power</code> and <code>isl_map_transitive_closure</code>
though.
<h3><a id=using-compactness-to-argue-about-covers href='#using-compactness-to-argue-about-covers'> § </a> <a href=#using-compactness-to-argue-about-covers>Using compactness to argue about covers</a></h3>
I've always seen compactness be used by <i>starting</i> with a possibly infinite
coverm and then <i>filtering it</i> into a finite subcover. This finite
subcover is then used for finiteness properties (like summing, min, max, etc.).
I recently ran across a use of compactness when one <i>starts</i> with the set
of <i>all possible subcovers</i>, and then argues about why a cover cannot be built
from these subcovers if the set is compact. I found it to be a very cool
use of compactness, which I'll record below:
<h4><a id=theorem href='#theorem'> § </a> Theorem:</h4>
If a family of compact, countably infinite sets <code>S_a</code> have all
<i>finite intersections</i> non-empty, then the intersection of the family <code>S_a</code>
is non-empty.
<h4><a id=proof href='#proof'> § </a> Proof:</h4>
Let <code>S = intersection of S_a</code>. We know that <code>S</code> must be compact since
all the <code>S_a</code> are compact, and the intersection of a countably infinite
number of compact sets is compact.
Now, let <code>S</code> be empty. Therefore, this means there must be a point <code>p ∈ P</code>
such that <code>p !∈ S_i</code> for some arbitrary <code>i</code>.
<h4><a id=cool-use-of-theorem href='#cool-use-of-theorem'> § </a> Cool use of theorem:</h4>
We can see that the cantor set is non-empty, since it contains a family
of closed and bounded sets <code>S1, S2, S3, ...</code> such that  <code>S1 ⊇ S2 ⊇ S3 ...</code>
where each <code>S_i</code> is one step of the cantor-ification. We can now see
that the cantor set is non-empty, since:
<ol><li> Each finite intersection is non-empty, and will be equal to the set that
   has the highest index in the finite intersection.</li><li> Each of the sets <code>Si</code> are compact since they are closed and bounded subsets of <code>R</code></li><li> Invoke theorem.</li></ol>
<h3><a id=japanese-financial-counting-system href='#japanese-financial-counting-system'> § </a> <a href=#japanese-financial-counting-system>Japanese Financial Counting system</a></h3>
<ul><li> <a href=https://en.wikipedia.org/wiki/Japanese_numerals#Formal_numbers>Wikipedia</a></li></ul>
Japanese contains a separate kanji set called <code>daiji</code>, to prevent people
from adding strokes to stuff previously written.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h3><a id=stephen-wolframs-live-stream href='#stephen-wolframs-live-stream'> § </a> <a href=#stephen-wolframs-live-stream>Stephen wolfram's live stream</a></h3>
<ul><li> <a href=https://www.twitch.tv/videos/408653972>Twitch.tv link</a></li></ul>
I've taken to watching the live stream when I have some downtime and want
some interesting content.
The discussions of Wolfram with his group are great, and they bring up
<i>really</i> interesting ideas (like that of cleave being very irregular).
<h3><a id=cleave-as-a-word-has-some-of-the-most-irregular-inflections href='#cleave-as-a-word-has-some-of-the-most-irregular-inflections'> § </a> <a href=#cleave-as-a-word-has-some-of-the-most-irregular-inflections><code>Cleave</code> as a word has some of the most irregular inflections</a></h3>
<ul><li> cleave</li><li> clove</li><li> cleaved</li><li> clave</li><li> cleft</li></ul>
<h3><a id=mccunes-single-axiom-for-group-theory href='#mccunes-single-axiom-for-group-theory'> § </a> <a href=#mccunes-single-axiom-for-group-theory>McCune's single axiom for group theory</a></h3>
<a href=http://ftp.mcs.anl.gov/pub/tech_reports/reports/P270.pdf>Single Axioms for Groups and Abelian Groups with Various Operations</a>
provides a single axiom for groups. This can be useful for some ideas I have
for training groups, where we can use this axiom as the loss function!
<h3><a id=word2vec-c-code-implements-gradient-descent-really-weirdly href='#word2vec-c-code-implements-gradient-descent-really-weirdly'> § </a> <code>Word2Vec</code> C code implements gradient descent really weirdly</h3>
I'll be posting snippets of the original source code, along with a
link to the Github sources. We are interested in exploring the skip-gram
implementation of Word2Vec, with negative sampling, without hierarchical
softmax. I assume basic familiarity with word embeddings and the skip-gram
model.
<h4><a id=construction-of-the-sigmoid-lookup-table href='#construction-of-the-sigmoid-lookup-table'> § </a> Construction of the sigmoid lookup table</h4>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
Here, the code constructs a lookup table which maps <code>[0...EXP_TABLE_SIZE-1]</code>
to <code>[sigmoid(-MAX_EXP)...sigmoid(MAX_EXP)]</code>. The index <code>i</code> first gets mapped
to <code>(i / EXP_TABLE_SIZE) * 2 - 1</code>, which sends <code>0</code> to <code>-1</code> and <code>EXP_TABLE_SIZE</code>
to <code>1</code>. This is then rescaled by <code>MAX_EXP</code>.
<h4><a id=layer-initialization href='#layer-initialization'> § </a> Layer initialization</h4>
<ul><li> <code>syn0</code> is a global variable, initialized with random weights in the range of
  <code>[-0.5...0.5]</code>. It has dimensions <code>VOCAB x HIDDEN</code>.  This layer holds the
   hidden representations of word vectors.</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> <code>syn1neg</code> is a global variable that is zero-initialized. It has dimensions
  <code>VOCAB x HIDDEN</code>. This layer also holds hidden representations of word vectors,
  <i>when they are used as a negative sample</i>.</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> <code>neu1e</code> is a temporary per-thread buffer (Remember that the <code>word2vec</code> C code
  use CPU threads for parallelism) which is zero initialized. It has dimensions
  <code>1 x HIDDEN</code>.</li></ul>
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<h4><a id=backpropogation href='#backpropogation'> § </a> Backpropogation</h4>
Throughout <code>word2vec</code>, no 2D arrays are used. Indexing of the form
<code>arr[word][ix]</code> is manually written as <code>arr[word * layer1_size + ix]</code>. So, I
will call <code>word * layer1_size</code> as the "base address", and <code>ix</code> as the "offset
of the array index expression henceforth.
Here, <code>l1</code> is the base address of the word at the center of window (the focus
word).  <code>l2</code> is the base address of either the word that is negative sampled
from the corpus, or the word that is a positive sample from within the context
window.
<code>label</code> tells us whether the sample is a positive or a negative sample.
<code>label = 1</code> for positive samples, and <code>label = 0</code> for negative samples.
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
<ul><li> We have <i>two</i> vectors for each word, one called <code>syn0[l1 + _]</code> and
  the other <code>syn1neg[l2 + _]</code>. The <code>syn1neg</code> word embedding is used whenever
  a word is used a negative sample, and is not used anywhere else. Also,
  the <code>syn1neg</code> vector is zero initialized, while the <code>syn0</code> vectors are
  randomly initialized.</li></ul>
<ul><li> The values we backprop with <code>g * syn1neg[l2 + _]</code>, <code>g * syn0[l1 + _]</code> are
  <i>not</i> the correct gradients of the error term! The derivative of a sigmoid
  is <code>dsigmoid(x)/dx = sigmoid(x) [1 - sigmoid(x)]</code>. The <code>[1 - sigmoid(x)]</code>
  is nowhere to be seen, let alone the fact that we are using
  <code>sigmoid(2x - 1)</code> and not regular sigmoid. Very weird.</li></ul>
<ul><li> We hold the value of <code>syn0</code> constant throughout all the negative samples,
  which was not mentioned in any tutorial I've read.</li></ul>
The paper does not mentioned these implementation details, and neither
does <i>any blog post that I've read</i>. I don't understand what's going on,
and I plan on updating this section when I understand this better.
<h3><a id=arthur-whitney-dense-code href='#arthur-whitney-dense-code'> § </a> <a href=#arthur-whitney-dense-code>Arthur Whitney: dense code</a></h3>
<ul><li> Guy who wrote a bunch of APL dialects, write code in an eclectic style
  that has very little whitespace and single letter variable names.</li><li> Believes that this allows him to hold the entire program in his head.</li><li> Seems legit from my limited experience with APL, haskell one-liners.</li><li> <a href=http://kparc.com/b/readme.txt>The b programming language</a>. It's quite
  awesome to read the sources. For example, <a href=http://kparc.com/b/a.c><code>a.c</code></a></li></ul>
<ul><li> <a href=https://www.jsoftware.com/papers/50/>A history of APL in 50 functions</a> ---
  A great list of APL snippets that solve classical problems.</li></ul>
<h3><a id=how-does-one-work-with-arrays-in-a-linear-language href='#how-does-one-work-with-arrays-in-a-linear-language'> § </a> <a href=#how-does-one-work-with-arrays-in-a-linear-language>How does one work with arrays in a linear language?</a></h3>
Given an array of qubits <code>xs: Qubit[]</code>, I want to switch to little endian.
Due to no-cloning, I can't copy them! I suppose I can use recursion to build
up a new "list". But this is not the efficient array version we know and love
and want.
The code that I want to work but does not:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
On the other hand, what <i>does work</i> is to setup a quantum circuit that
performs this flipping, since it's a permutation matrix at the end of
the day. But this is very slow, since it needs to simulate the "quantumness"
of the solution, since it takes <code>2^n</code> basis vectors for <code>n</code> qubits.
However, the usual recursion based solution works:
<div class='code'><!-- Generator: GNU source-highlight 3.1.8
by Lorenzo Bettini
http://www.lorenzobettini.it
http://www.gnu.org/software/src-highlite -->
<pre><tt>          ∃!id(p)
          +---+
          |   |
          |   v
+---------ppppp--------+
| πa      |   ^     πb |  
v      ∃!p2q  |        v
a         |   |        b
^         |  ∃!q2p     ^
| π'b     v   |    π'b |
+---------qqqqq--------+
</tt></pre>
</div>
This is of course, suboptimal.
I find it interesting that in the linear types world, often the "pure" solution
is <i>forced</i> since mutation very often involves temporaries / copying!
(I'm solving assignments in <a href=https://docs.microsoft.com/en-us/quantum/>qsharp</a>
for my course in college)
<h3><a id=linear-optimisation-is-the-same-as-linear-feasibility-checking href='#linear-optimisation-is-the-same-as-linear-feasibility-checking'> § </a> <a href=#linear-optimisation-is-the-same-as-linear-feasibility-checking>Linear optimisation is the same as linear feasibility checking</a></h3>
Core building block of effectively using the ellipsoid algorithm.
<ul><li> If we posess a way to check if a point <span class='latexinline'>p \in P</span> where <span class='latexinline'>P</span> is a polytope, we
  can use this to solve optimisation problems.</li><li> Given the optimisation problem maximise <span class='latexinline'>c^Tx</span> subject to <span class='latexinline'>Ax = b</span>, we can
  construct a new <i>non-emptiness</i> problem. This allows us to convert optimisation
  into <i>feasibility</i>.</li><li> The new problem is <span class='latexinline'>Ax = b, A^Ty = c, c^Tx = b^T y</span>. Note that by duality,
  a point in this new polyhedra will <i>be an optimal solution to the above linear program</i>.
  We are forcing <span class='latexinline'>c^Tx = b^Ty</span>, which will be the optimal solution, since the
  solution where the primal and dual agree is the optimal solution by strong
  duality.</li><li> This way, we have converted a <i>linear programming</i> problem into a
  <i>check if this polytope is empty</i> problem!</li></ul>
<h3><a id=quantum-computation-without-complex-numbers href='#quantum-computation-without-complex-numbers'> § </a> <a href=#quantum-computation-without-complex-numbers>Quantum computation without complex numbers</a></h3>
I recently learnt that the Toeffili and Hadamard gates are universal for
quantum computation. The description of these gates involve no complex numbers.
So, we can write any quantum circuit in a "complex number free" form. The caveat
is that we may very well have <i>input qubits</i> that require complex numbers.
Even so, a large number (all?) of the basic algorithms shown in Nielsen and
Chaung can be encoded in an entirely complex-number free fashion.
I don't really understand the ramifications of this, since I had the intuition
that the power of quantum computation comes from the ability to express
complex phases along with superposition (tensoring). However, I now have
to remove the power from going from R to C in many cases. This is definitely
something to ponder.
<ul><li> <a href=https://arxiv.org/pdf/quant-ph/0301040>Dorit Aharonov: A Simple Proof that Toffoli and Hadamard are Quantum Universal</a></li></ul>
<h3><a id=linguistic-fun-fact-comparative-illusion href='#linguistic-fun-fact-comparative-illusion'> § </a> <a href=#linguistic-fun-fact-comparative-illusion>Linguistic fun fact: Comparative Illusion</a></h3>
I steal from wikipedia:
<blockquote> Comparative Illusion, which is a grammatical illusion where certain sentences seem grammatically correct when you read them, but upon further reflection actually make no sense.</blockquote>
For example: "More people have been to Berlin than I have."
<h3><a id=long-form-posts href='#long-form-posts'> § </a> Long-form posts:</h3>
<h3><a id=reading href='#reading'> § </a> Reading</h3>
<ul><li> <a href=content/blog/stuff-i-learnt-this-year-2018.md>2018 reading</a></li><li> <a href=content/blog/papers-I-read-and-loved-in-2017.md>2017 reading</a></li></ul>
<h3><a id=haskell href='#haskell'> § </a> Haskell</h3>
<ul><li> <a href=content/blog/reading-kmett-structs.md>Reading the <code>structs</code> library</a></li><li> <a href=content/blog/machines/reading-kmett-machines.md>Reading the <code>machines</code> library (WIP)</a></li><li> <a href=content/blog/laziness-for-c-programmers.md>Explaining laziness (WIP)</a></li><li> <a href=stg-explained.md>Explaining STG(WIP)</a></li></ul>
<h3><a id=simplexhc-stg--llvm-compiler-progress href='#simplexhc-stg--llvm-compiler-progress'> § </a> Simplexhc (STG -> LLVM compiler) progress</h3>
<ul><li> <a href=content/blog/ghc-micro-optimisations-or-why-proc-points-suck.md>proc points suck / making GHC an order of magnitude faster</a></li><li> <a href=this-month-in-simplexhc-dec-2017.md>dec 2017</a></li><li> <a href=this-week-in-simpexhc-oct-29-2017.md>oct 29 2017</a></li><li> <a href=this-week-in-simplexhc-07-2017.md>july 2017</a></li><li> <a href=this-week-in-simplexhc-2017-07-06.md>july 6th 2017</a></li><li> <a href=content/blog/announcing-simplexhc.md>announcement</a></li></ul>
<h3><a id=gsoc-2015 href='#gsoc-2015'> § </a> GSoC (2015)</h3>
<ul><li> <a href=content/blog/gsoc-vispy.md>proposal</a></li><li> <a href=content/blog/gsoc-vispy-week-1-and-2.md>week 1</a></li><li> <a href=content/blog/gsoc-vispy-week-3-and-4.md>week 3 and 4</a></li><li> <a href=content/blog/gsoc-vispy-week-5.md>week 5</a></li><li> <a href=content/blog/gsoc-vispy-week-6.md>week 6</a></li><li> <a href=content/blog/gsoc-vispy-week-7.md>week 7</a></li><li> <a href=content/blog/gsoc-vispy-report-6.md>final report</a></li></ul>
</container></body></html>